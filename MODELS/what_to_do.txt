Num classes  = 1
criterion = nn.BCEWithLogitsLoss()  # Use this instead of BCELoss()
def forward(self, x_resnet, x_inception):
    resnet_features = self.resnet(x_resnet)
    inception_features = self.inception(x_inception)
    if isinstance(inception_features, tuple):  # InceptionV3 returns a tuple
        inception_features = inception_features[0]  # Take only the main output
    fused_features = torch.cat((resnet_features, inception_features), dim=1)
    fused_features = self.batch_norm(fused_features)
    fused_features = self.relu(fused_features)
    fused_features = self.dropout(fused_features)
    output = self.fc(fused_features)  # No sigmoid here
    return output  # Raw logits



  preds = (outputs > 0).float()  # ✅ Logits threshold at 0 (not 0.5 since no sigmoid in forward)


TRY 0.0001, batch 64 or around it






FINAL TO DO

DO the 3 way split, train again for baseline and proposed - DONE
RESNET ON baseline is not learning since only transfer learning not finetuning - DONE
FIX DATASET, YOLO SHOULD EXTRACT CLOSE TO SQUARE SIZES - DONE 
FIX TRANSFORMATIONS - done
FIX Dataset generation, make sure that no persons is included since its still explicit - DONE
FIX IMAGE SIZES - DONE




The augmentation on Proposed is not on transformation but on the dataset itself - done
the aumentation of baseline is on transformation - done
ADD INTERPOLATION ON BASELINE - done



upscaling for the other dataset FOR MMU NOT necessarily FOR ECD SINCE big resolution
DATASET IMBALANCE on MMU dataset 



build a inception-svm model
build a inception only model
build a resnet only model


Reminders: 
adjust b1 for momentum like behaviour if loss oscillates up and down
If your model is already converging well with default Adam (β₁ = 0.9, β₂ = 0.999), you likely don’t need to change anything.
If your model oscillates too much, decreasing β₁ slightly (e.g., 0.85) can reduce aggressive updates.
If your model converges too slowly, increasing β₁ (e.g., 0.95) might help accelerate it.



If Validation Loss continues increasing while Train Loss drops, 
this could indicate overfitting—meaning the model memorizes the training data rather than generalizing well.

1. Training Accuracy
Use Case: Measures how well the model fits the training data.
Pros: It’s useful for detecting overfitting. If your training accuracy is much higher than validation accuracy, the model may be overfitting.
Cons: It doesn’t provide any insight into how the model generalizes to unseen data.
2. Validation Accuracy
Use Case: Measures how well the model performs on the validation set, which is a proxy for unseen data.
Pros: This is often the primary metric used for hyperparameter tuning, as it shows how well the model generalizes. Validation accuracy can help detect underfitting or overfitting.
Cons: It doesn't necessarily tell you how confident the model is in its predictions. It’s possible for a model to have high validation accuracy but still make uncertain or poor predictions in some cases.

Hyperparameter Tuning - Adjusting external settings that affect training but are not learned by the model itself.
	
example - Learning rate, batch size, optimizer type (SGD, Adam), dropout rate, weight decay, etc.

Fine-Tuning - Taking a pretrained model and updating some or all of its weights for a new task.	

example - Unfreezing layers, training only specific layers, changing the classifier head.

transfer learning is using pretrained for another but related task. can be done by fine tuning or changing classifier head


 Check Validation Loss

If training loss drops fast but validation loss stagnates or increases, it's overfitting.
Lower dropout might help only if validation loss improves.


Yes! If both training and validation loss are decreasing gradually, and validation accuracy is consistently higher than training accuracy, this is generally a good sign because it means:

✅ Your model is learning effectively (since both losses are decreasing).
✅ Your model isn’t overfitting yet (since training loss isn’t lower than validation loss).
✅ The validation set might be slightly easier or less augmented, leading to better accuracy.