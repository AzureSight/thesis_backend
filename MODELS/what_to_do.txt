Num classes  = 1
criterion = nn.BCEWithLogitsLoss()  # Use this instead of BCELoss()
def forward(self, x_resnet, x_inception):
    resnet_features = self.resnet(x_resnet)
    inception_features = self.inception(x_inception)
    if isinstance(inception_features, tuple):  # InceptionV3 returns a tuple
        inception_features = inception_features[0]  # Take only the main output
    fused_features = torch.cat((resnet_features, inception_features), dim=1)
    fused_features = self.batch_norm(fused_features)
    fused_features = self.relu(fused_features)
    fused_features = self.dropout(fused_features)
    output = self.fc(fused_features)  # No sigmoid here
    return output  # Raw logits



  preds = (outputs > 0).float()  # ‚úÖ Logits threshold at 0 (not 0.5 since no sigmoid in forward)


TRY 0.0001, batch 64 or around it






FINAL TO DO

DO the 3 way split, train again for baseline and proposed - DONE
RESNET ON baseline is not learning since only transfer learning not finetuning - DONE
FIX DATASET, YOLO SHOULD EXTRACT CLOSE TO SQUARE SIZES - DONE 
FIX TRANSFORMATIONS - done
FIX Dataset generation, make sure that no persons is included since its still explicit - DONE
FIX IMAGE SIZES - DONE




The augmentation on Proposed is not on transformation but on the dataset itself - done
the aumentation of baseline is on transformation - done
ADD INTERPOLATION ON BASELINE - done



upscaling for the other dataset FOR MMU NOT necessarily FOR ECD SINCE big resolution
DATASET IMBALANCE on MMU dataset 



build a inception-svm model
build a inception only model

build a resnet only model


Reminders: 
adjust b1 for momentum like behaviour if loss oscillates up and down
If your model is already converging well with default Adam (Œ≤‚ÇÅ = 0.9, Œ≤‚ÇÇ = 0.999), you likely don‚Äôt need to change anything.
If your model oscillates too much, decreasing Œ≤‚ÇÅ slightly (e.g., 0.85) can reduce aggressive updates.
If your model converges too slowly, increasing Œ≤‚ÇÅ (e.g., 0.95) might help accelerate it.



If Validation Loss continues increasing while Train Loss drops, 
this could indicate overfitting‚Äîmeaning the model memorizes the training data rather than generalizing well.

1. Training Accuracy
Use Case: Measures how well the model fits the training data.
Pros: It‚Äôs useful for detecting overfitting. If your training accuracy is much higher than validation accuracy, the model may be overfitting.
Cons: It doesn‚Äôt provide any insight into how the model generalizes to unseen data.
2. Validation Accuracy
Use Case: Measures how well the model performs on the validation set, which is a proxy for unseen data.
Pros: This is often the primary metric used for hyperparameter tuning, as it shows how well the model generalizes. Validation accuracy can help detect underfitting or overfitting.
Cons: It doesn't necessarily tell you how confident the model is in its predictions. It‚Äôs possible for a model to have high validation accuracy but still make uncertain or poor predictions in some cases.

Hyperparameter Tuning - Adjusting external settings that affect training but are not learned by the model itself.
	
example - Learning rate, batch size, optimizer type (SGD, Adam), dropout rate, weight decay, etc.

Fine-Tuning - Taking a pretrained model and updating some or all of its weights for a new task.	

example - Unfreezing layers, training only specific layers, changing the classifier head.

transfer learning is using pretrained for another but related task. can be done by fine tuning or changing classifier head


 Check Validation Loss

If training loss drops fast but validation loss stagnates or increases, it's overfitting.
Lower dropout might help only if validation loss improves.


Yes! If both training and validation loss are decreasing gradually, and validation accuracy is consistently higher than training accuracy, this is generally a good sign because it means:

‚úÖ Your model is learning effectively (since both losses are decreasing).
‚úÖ Your model isn‚Äôt overfitting yet (since training loss isn‚Äôt lower than validation loss).
‚úÖ The validation set might be slightly easier or less augmented, leading to better accuracy.





GOOD PARAMS
-top val acc
epochs 10
batch = 32
dropout 0.3
lr = 0.0001 

-good
epochs 10
lr=0.0001
dropout 0.4
batch = 32


-good
epochs 10
lr=0.0001
dropout 0.3
batch = 64


GOOD PARAMS - ECD FULL DATASET
balanced results - but slow val and train acc = 89
epochs=10
lr=0.0001
batch = 64
dropout 0.3

balanced results - but slow val and train acc = 89 - NO OVERFITTING 
epochs=10
lr=0.0001
Weight decay = 0.0001
batch = 64
dropout 0.3

balanced results - but slow val and train acc = 90 - test 92 - NO OVERFITTING 
epochs=5
lr=0.0001
Weight decay = 0.0001
batch = 32
dropout 0.3

CJ
- top val acc 5 
- top val acc 10
- 
epochs 5
batch = 64
lr = 0.0001 
dropout 0.4


try lower learning rates, different each layer, then gradually unfreezings

Layer	------------------------Learning Rate (lr)	
fc (classifier)	--------------1e-3 (highest)	
layer4 (last ResNet block)	--1e-4	Unfreeze 
layer3 (mid ResNet block)	----1e-5	Unfreeze 
layer2 (early ResNet block)	--1e-6 (lowest)	
üîß

if epoch == 10:  
    for param in model.layer3.parameters():
        param.requires_grad = True
    optimizer = torch.optim.Adam([
        {'params': model.fc.parameters(), 'lr': 1e-3},  
        {'params': model.layer4.parameters(), 'lr': 1e-4},  
        {'params': model.layer3.parameters(), 'lr': 1e-5}  
    ])


    
64, 0.0001, all layers = 95.26 val acc


64, 0.0001 LR, 0.001 WD, 64 dense 5 epochs = 93
64, 0.0001 LR, 0.001 WD, 128 dense 5 epochs = 93 but ugly graph butno overfit
try batch 32



FINETUNE1 15 
0.2
unfreezed thrice at 9, 13,15

FINETUNE2 15 
0.3
unfreezed thrice at 11,13,15

FINETUNE3 15 
- unfrezed once at 11
0.7


Retrain Proposed
ECD Augmented 10 - same ra hahah wala na giusab
ECD Augmented 15 - done

ECD Non Augmented 15

MMU augmented 15
MMU Non augmented 15