{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_yolo_preds():\n",
    "    \"\"\"\n",
    "    Run YOLO predictions, save cropped persons, and save the whole image with bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        net: YOLO model loaded with cv2.dnn.readNetFromDarknet.\n",
    "        input_img_path (str): Path to the input image.\n",
    "        output_img_path (str): Path to save the output image with bounding boxes.\n",
    "        confidence_threshold (float): Minimum confidence for predictions.\n",
    "        overlapping_threshold (float): Threshold for non-maxima suppression.\n",
    "        labels (list): List of class labels.\n",
    "        show_display (bool): Whether to display the image with bounding boxes.\n",
    "    \"\"\"\n",
    "            # Load COCO labels\n",
    "    labels_path = \"./DARKNET/coco.txt\"\n",
    "    try:\n",
    "        with open(labels_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            labels = f.read().strip().split(\"\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {labels_path} not found. Please check the file path.\")\n",
    "        exit()\n",
    "\n",
    "    # Define YOLO paths and parameters\n",
    "    yolo_cfg = \"./DARKNET/model_data/yolov3.cfg\"\n",
    "    yolo_weights = \"./DARKNET/model_data/yolov3.weights\"\n",
    "    input_img_path = \"./DARKNET/test3.jpg\"\n",
    "    output_img_path = \"./DARKNET/output\"\n",
    "    confidence_threshold = 0.5\n",
    "    overlapping_threshold = 0.5\n",
    "\n",
    "    # Initialize YOLO model\n",
    "    net = cv2.dnn.readNetFromDarknet(yolo_cfg, yolo_weights)\n",
    "    # if cuda: \n",
    "    #     net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "    #     net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "    # Ensure output directory exists if the output_img_path is provided\n",
    "    if output_img_path:\n",
    "        output_dir = os.path.dirname(output_img_path)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "    # Generate random colors for each class\n",
    "    np.random.seed(123)\n",
    "    colors = np.random.randint(0, 255, size=(len(labels), 3), dtype=\"uint8\")\n",
    "\n",
    "    # Get the output layer names of the YOLO network\n",
    "    ln = net.getLayerNames()\n",
    "    ln = [ln[i - 1] for i in net.getUnconnectedOutLayers().flatten()]\n",
    "\n",
    "    # Read the input image\n",
    "    image = cv2.imread(input_img_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image {input_img_path}\")\n",
    "        return\n",
    "\n",
    "    (H, W) = image.shape[:2]\n",
    "\n",
    "    # Create a blob from the image\n",
    "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layerOutputs = net.forward(ln)\n",
    "\n",
    "    # Initialize lists for detections\n",
    "    boxes, confidences, classIDs = [], [], []\n",
    "\n",
    "    # Loop through each output layer\n",
    "    for output in layerOutputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            # Only keep detections for \"person\" (class ID 0)\n",
    "            if confidence > confidence_threshold and classID == 0:\n",
    "                # Scale bounding boxes back to image dimensions\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                classIDs.append(classID)\n",
    "\n",
    "    # Perform non-maxima suppression to reduce overlapping boxes\n",
    "    bboxes = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, overlapping_threshold)\n",
    "    image2 = image.copy()\n",
    "    # Collect cropped persons and draw bounding boxes on the image\n",
    "    cropped_persons = []\n",
    "    if len(bboxes) > 0:\n",
    "        for i in bboxes.flatten():\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "            # Draw bounding box on the original image\n",
    "            color = [int(c) for c in colors[classIDs[i]]]\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "            text = f\"{labels[classIDs[i]]}: {confidences[i]:.2f}\"\n",
    "            cv2.putText(image, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "            # Crop the person from the image (no bounding box)\n",
    "            cropped_person = image2[y:y + h, x:x + w]\n",
    "            cropped_persons.append(cropped_person)\n",
    "\n",
    "            # Save the cropped person image\n",
    "            cropped_filename = f\"./DARKNET/output/person_{i + 1}_confidence_{confidences[i]:.2f}.jpg\"\n",
    "            cv2.imwrite(cropped_filename, cropped_person)\n",
    "            print(f\"Cropped Person Saved: {cropped_filename}\")\n",
    "\n",
    "    # Save the full image with bounding boxes\n",
    "    if output_img_path:\n",
    "        if not output_img_path.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            output_img_path += '.jpg'  # Add .jpg extension if none is provided\n",
    "        if cv2.imwrite(output_img_path, image):\n",
    "            print(f\"Image with bounding boxes saved to {output_img_path}\")\n",
    "        else:\n",
    "            print(\"Failed to save the image with bounding boxes.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run YOLO predictions\n",
    "if __name__ == '__main__':\n",
    "    get_yolo_preds()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Example tensor\n",
    "tensor = torch.randn(3, 3)\n",
    "\n",
    "# Check the device\n",
    "print(\"Tensor is on:\", tensor.device)\n",
    "# Example model\n",
    "from torchvision import models\n",
    "\n",
    "model = models.inception_v3(weights=None)\n",
    "\n",
    "# Check the device of the model\n",
    "print(\"Model is on:\", next(model.parameters()).device)\n",
    "\n",
    "# Move tensor to GPU\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')  # or tensor.cuda()\n",
    "    print(\"Tensor moved to GPU:\", tensor.device)\n",
    "else:\n",
    "    tensor = tensor.to('cpu')  # or tensor.cpu()\n",
    "    print(\"Tensor moved to CPU:\", tensor.device)\n",
    "    # Move model to GPU\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "    print(\"Model moved to GPU.\")\n",
    "else:\n",
    "    model = model.to('cpu')\n",
    "    print(\"Model moved to CPU.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "import torch\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet50xInceptionv3 Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision import models, transforms, datasets\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Dataset path\n",
    "# dataset_path = r\"D:\\\\THESIS PROJECT FOLDER\\\\test\"\n",
    "dataset_path = \"./cropped_images\"\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise FileNotFoundError(f\"Directory not found: {dataset_path}\")\n",
    "\n",
    "# Image transformations for ResNet50 and InceptionV3\n",
    "transform_resnet = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_inception = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset using torchvision\n",
    "dataset = datasets.ImageFolder(dataset_path, transform=None)\n",
    "class_names = dataset.classes\n",
    "print(class_names)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Split dataset\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2, stratify=dataset.targets)\n",
    "\n",
    "# Apply transformations for each model\n",
    "train_loader_resnet = DataLoader(\n",
    "    datasets.ImageFolder(dataset_path, transform=transform_resnet), batch_size=32, shuffle=True\n",
    ")\n",
    "train_loader_inception = DataLoader(\n",
    "    datasets.ImageFolder(dataset_path, transform=transform_inception), batch_size=32, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "# For ResNet50, retain the model's structure and extract features dynamically\n",
    "class ResNet50FeatureExtractor(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ResNet50FeatureExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-2])  # Keep layers up to the last conv layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # Extract features\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize feature extractors\n",
    "base_model_resnet = ResNet50FeatureExtractor(models.resnet50(weights=ResNet50_Weights.DEFAULT))\n",
    "\n",
    "# Freeze layers\n",
    "for param in base_model_resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in list(base_model_resnet.parameters())[-94:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "def extract_features_fixed(model, dataloader, device):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    features, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = images.to(device)\n",
    "            print(\"Input to model:\", images.shape)  # Shape of input image batch\n",
    "            feature_maps = model(images)\n",
    "            print(\"Output of model:\", feature_maps.shape)  # Shape of feature maps\n",
    "            features.append(feature_maps.cpu().numpy())\n",
    "            labels.extend(targets.numpy())\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    return features, np.array(labels)\n",
    "\n",
    "\n",
    "# Extract features\n",
    "print(\"Extracting features from ResNet50...\")\n",
    "features_resnet, labels_resnet = extract_features_fixed(base_model_resnet, train_loader_resnet, torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "print(\"ResNet50 Feature Map Shape:\", features_resnet.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV3FeatureExtractor(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(InceptionV3FeatureExtractor, self).__init__()\n",
    "        # Remove the FC layer and include everything else\n",
    "        self.features = nn.Sequential(\n",
    "            *list(original_model.children())[:-3]  # Remove the FC layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "base_model = models.inception_v3(aux_logits=False)\n",
    "# Load pretrained weights while ignoring unexpected keys\n",
    "pretrained_model = models.inception_v3(weights=Inception_V3_Weights.DEFAULT)\n",
    "\n",
    "# # Compare weights of the first convolutional layer\n",
    "state_dict = pretrained_model.state_dict()\n",
    "\n",
    "base_model.load_state_dict(state_dict, strict=False)  # Ignore unexpected keys\n",
    "\n",
    "first_conv_weights = base_model.Conv2d_1a_3x3.conv.weight.data\n",
    "print(\"Mean of Conv2d_1a_3x3 weights:\", first_conv_weights.mean().item())\n",
    "print(\"Std of Conv2d_1a_3x3 weights:\", first_conv_weights.std().item())\n",
    "# is_pretrained = torch.allclose(\n",
    "#     base_model.Conv2d_1a_3x3.conv.weight.data,\n",
    "#     pretrained_model.Conv2d_1a_3x3.conv.weight.data\n",
    "# )\n",
    "# print(\"Is the model pre-trained?\", is_pretrained)\n",
    "# Pass the modified model to your custom feature extractor\n",
    "base_model_inception = InceptionV3FeatureExtractor(base_model)\n",
    "\n",
    "for param in base_model_inception.parameters():\n",
    "    param.requires_grad = False\n",
    "# Unfreeze some layers for fine-tuning\n",
    "\n",
    "for param in list(base_model_inception.parameters())[-50:]:\n",
    "    param.requires_grad = True\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "from torchvision.models import inception_v3\n",
    "\n",
    "\n",
    "\n",
    "# Define the input size (batch size, channels, height, width)\n",
    "input_size = (32, 3, 299, 299)  # Example for InceptionV3\n",
    "\n",
    "# Print the summary\n",
    "# summary(base_model_inception, input_size=input_size)\n",
    "# Print the full model without truncation\n",
    "torch.set_printoptions(threshold=10_000)  # Adjust threshold if needed\n",
    "print(base_model_inception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example input: batch size of 1, 3 color channels, 299x299 image\n",
    "example_input = torch.randn(32, 3, 299, 299)\n",
    "\n",
    "# Extract features\n",
    "base_model_inception.eval()  # Ensure evaluation mode\n",
    "with torch.no_grad():\n",
    "    features = base_model_inception(example_input)\n",
    "    print(\"Feature shape:\", features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correct feature extraction for InceptionV3\n",
    "print(\"Extracting features from InceptionV3...\")\n",
    "features_inception, _ = extract_features_fixed(\n",
    "    base_model_inception,\n",
    "    train_loader_inception,\n",
    "    torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "print(\"InceptionV3 Feature Map Shape:\", features_inception.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply Global Average Pooling\n",
    "features_resnet_gap = features_resnet.mean(axis=(2, 3))  # GAP for ResNet\n",
    "features_inception_gap = features_inception.mean(axis=(2, 3))  # GAP for Inception\n",
    "\n",
    "# Concatenate feature maps\n",
    "fused_features = np.concatenate([features_resnet_gap, features_inception_gap], axis=1)\n",
    "print(\"Fused Feature Shape:\", fused_features.shape)\n",
    "\n",
    "# Define the classifier\n",
    "class ClassifierModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ClassifierModel, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "input_dim = fused_features.shape[1]\n",
    "model = ClassifierModel(input_dim)\n",
    "\n",
    "# Compile the model\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Model defined and ready for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "fused_features_tensor = torch.tensor(fused_features, dtype=torch.float32)\n",
    "labels1_tensor = torch.tensor(labels_resnet, dtype=torch.float32)\n",
    "\n",
    "# Train-test split\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(fused_features_tensor, labels1_tensor, test_size=0.2, random_state=42)\n",
    "print(\"Training the classifier after unfreezing some layers...\")\n",
    "\n",
    "# Convert the data to DataLoader for batch processing\n",
    "train_dataset = TensorDataset(X_train1, y_train1)\n",
    "test_dataset = TensorDataset(X_test1, y_test1)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20, shuffle=False)\n",
    "\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs).squeeze()  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update the model weights\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Evaluating the model\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs).squeeze()  # Get model predictions\n",
    "        y_pred.extend((outputs > 0.5).int().numpy())  # Convert probabilities to binary predictions\n",
    "        y_true.extend(labels.int().numpy())\n",
    "\n",
    "# Print classification report and accuracy score\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_true, y_pred))\n",
    "\n",
    "# Save the model (optional)\n",
    "# model_save_path = 'saved_model.pth'\n",
    "# torch.save(model.state_dict(), model_save_path)\n",
    "# print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##WHOLE MODEL RESNETXINCEPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Lib\\site-packages\\torchvision\\models\\inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet50_Weights, Inception_V3_Weights\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# Dataset path\n",
    "dataset_path = \"./augTEST\"\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise FileNotFoundError(f\"Directory not found: {dataset_path}\")\n",
    "\n",
    "# Define transformations\n",
    "transform_resnet = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_inception = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset_resnet = ImageFolder(dataset_path, transform=transform_resnet)\n",
    "dataset_inception = ImageFolder(dataset_path, transform=transform_inception)\n",
    "\n",
    "class_names = dataset_resnet.classes\n",
    "num_classes = len(class_names)\n",
    "# Train-test split\n",
    "train_idx, val_idx = train_test_split(list(range(len(dataset_resnet))), test_size=0.2, random_state=42)\n",
    "\n",
    "train_resnet = torch.utils.data.Subset(dataset_resnet, train_idx)\n",
    "val_resnet = torch.utils.data.Subset(dataset_resnet, val_idx)\n",
    "\n",
    "train_inception = torch.utils.data.Subset(dataset_inception, train_idx)\n",
    "val_inception = torch.utils.data.Subset(dataset_inception, val_idx)\n",
    "\n",
    "train_loader_resnet = DataLoader(train_resnet, batch_size=32, shuffle=True)\n",
    "val_loader_resnet = DataLoader(val_resnet, batch_size=32, shuffle=False)\n",
    "\n",
    "train_loader_inception = DataLoader(train_inception, batch_size=32, shuffle=True)\n",
    "val_loader_inception = DataLoader(val_inception, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FusionModel, self).__init__()\n",
    "        \n",
    "        self.resnet = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "        self.inception = models.inception_v3(aux_logits=False)\n",
    "        pretrained_model = models.inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1)\n",
    "        state_dict = pretrained_model.state_dict()\n",
    "        self.inception.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        # Step 1: Freeze All Layers Initially**\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.inception.parameters():\n",
    "            param.requires_grad = False  \n",
    "        \n",
    "        # for param in self.resnet.fc.parameters():\n",
    "        #       param.requires_grad = True\n",
    "        # for param in self.inception.fc.parameters():\n",
    "        #       param.requires_grad = True    \n",
    "              \n",
    "        self.resnet.fc = nn.Identity()\n",
    "        self.inception.fc = nn.Identity()\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(4096)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.55)\n",
    "        self.fc = nn.Linear(4096, 1)\n",
    "        \n",
    "    def forward(self, x_resnet, x_inception):\n",
    "        resnet_features = self.resnet(x_resnet)\n",
    "        inception_features = self.inception(x_inception)\n",
    "        if isinstance(inception_features, tuple):  # InceptionV3 returns a tuple\n",
    "            inception_features = inception_features[0]  # Take only the main output\n",
    "        fused_features = torch.cat((resnet_features, inception_features), dim=1)\n",
    "        fused_features = self.batch_norm(fused_features)\n",
    "        fused_features = self.relu(fused_features)\n",
    "        fused_features = self.dropout(fused_features)\n",
    "        output = torch.sigmoid(self.fc(fused_features))\n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "model = FusionModel(num_classes=1)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FusionModel(\n",
      "  (resnet): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (inception): Inception3(\n",
      "    (Conv2d_1a_3x3): BasicConv2d(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (Conv2d_2a_3x3): BasicConv2d(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (Conv2d_2b_3x3): BasicConv2d(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (Conv2d_3b_1x1): BasicConv2d(\n",
      "      (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (Conv2d_4a_3x3): BasicConv2d(\n",
      "      (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (Mixed_5b): InceptionA(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch5x5_1): BasicConv2d(\n",
      "        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch5x5_2): BasicConv2d(\n",
      "        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_5c): InceptionA(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch5x5_1): BasicConv2d(\n",
      "        (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch5x5_2): BasicConv2d(\n",
      "        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_5d): InceptionA(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch5x5_1): BasicConv2d(\n",
      "        (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch5x5_2): BasicConv2d(\n",
      "        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_6a): InceptionB(\n",
      "      (branch3x3): BasicConv2d(\n",
      "        (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_6b): InceptionC(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_2): BasicConv2d(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_3): BasicConv2d(\n",
      "        (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_4): BasicConv2d(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_5): BasicConv2d(\n",
      "        (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_6c): InceptionC(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_2): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_3): BasicConv2d(\n",
      "        (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_4): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_5): BasicConv2d(\n",
      "        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_6d): InceptionC(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_2): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_3): BasicConv2d(\n",
      "        (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_4): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_5): BasicConv2d(\n",
      "        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_6e): InceptionC(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_2): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_3): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_4): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_5): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_7a): InceptionD(\n",
      "      (branch3x3_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3_2): BasicConv2d(\n",
      "        (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7x3_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7x3_2): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7x3_3): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7x3_4): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_7b): InceptionE(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3_1): BasicConv2d(\n",
      "        (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3_2a): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3_2b): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_3a): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_3b): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_7c): InceptionE(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3_1): BasicConv2d(\n",
      "        (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3_2a): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3_2b): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_3a): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_3b): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (batch_norm): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.55, inplace=False)\n",
      "  (fc): Linear(in_features=4096, out_features=1, bias=True)\n",
      ")\n",
      "resnet.conv1.weight: Frozen\n",
      "resnet.bn1.weight: Frozen\n",
      "resnet.bn1.bias: Frozen\n",
      "resnet.layer1.0.conv1.weight: Frozen\n",
      "resnet.layer1.0.bn1.weight: Frozen\n",
      "resnet.layer1.0.bn1.bias: Frozen\n",
      "resnet.layer1.0.conv2.weight: Frozen\n",
      "resnet.layer1.0.bn2.weight: Frozen\n",
      "resnet.layer1.0.bn2.bias: Frozen\n",
      "resnet.layer1.0.conv3.weight: Frozen\n",
      "resnet.layer1.0.bn3.weight: Frozen\n",
      "resnet.layer1.0.bn3.bias: Frozen\n",
      "resnet.layer1.0.downsample.0.weight: Frozen\n",
      "resnet.layer1.0.downsample.1.weight: Frozen\n",
      "resnet.layer1.0.downsample.1.bias: Frozen\n",
      "resnet.layer1.1.conv1.weight: Frozen\n",
      "resnet.layer1.1.bn1.weight: Frozen\n",
      "resnet.layer1.1.bn1.bias: Frozen\n",
      "resnet.layer1.1.conv2.weight: Frozen\n",
      "resnet.layer1.1.bn2.weight: Frozen\n",
      "resnet.layer1.1.bn2.bias: Frozen\n",
      "resnet.layer1.1.conv3.weight: Frozen\n",
      "resnet.layer1.1.bn3.weight: Frozen\n",
      "resnet.layer1.1.bn3.bias: Frozen\n",
      "resnet.layer1.2.conv1.weight: Frozen\n",
      "resnet.layer1.2.bn1.weight: Frozen\n",
      "resnet.layer1.2.bn1.bias: Frozen\n",
      "resnet.layer1.2.conv2.weight: Frozen\n",
      "resnet.layer1.2.bn2.weight: Frozen\n",
      "resnet.layer1.2.bn2.bias: Frozen\n",
      "resnet.layer1.2.conv3.weight: Frozen\n",
      "resnet.layer1.2.bn3.weight: Frozen\n",
      "resnet.layer1.2.bn3.bias: Frozen\n",
      "resnet.layer2.0.conv1.weight: Frozen\n",
      "resnet.layer2.0.bn1.weight: Frozen\n",
      "resnet.layer2.0.bn1.bias: Frozen\n",
      "resnet.layer2.0.conv2.weight: Frozen\n",
      "resnet.layer2.0.bn2.weight: Frozen\n",
      "resnet.layer2.0.bn2.bias: Frozen\n",
      "resnet.layer2.0.conv3.weight: Frozen\n",
      "resnet.layer2.0.bn3.weight: Frozen\n",
      "resnet.layer2.0.bn3.bias: Frozen\n",
      "resnet.layer2.0.downsample.0.weight: Frozen\n",
      "resnet.layer2.0.downsample.1.weight: Frozen\n",
      "resnet.layer2.0.downsample.1.bias: Frozen\n",
      "resnet.layer2.1.conv1.weight: Frozen\n",
      "resnet.layer2.1.bn1.weight: Frozen\n",
      "resnet.layer2.1.bn1.bias: Frozen\n",
      "resnet.layer2.1.conv2.weight: Frozen\n",
      "resnet.layer2.1.bn2.weight: Frozen\n",
      "resnet.layer2.1.bn2.bias: Frozen\n",
      "resnet.layer2.1.conv3.weight: Frozen\n",
      "resnet.layer2.1.bn3.weight: Frozen\n",
      "resnet.layer2.1.bn3.bias: Frozen\n",
      "resnet.layer2.2.conv1.weight: Frozen\n",
      "resnet.layer2.2.bn1.weight: Frozen\n",
      "resnet.layer2.2.bn1.bias: Frozen\n",
      "resnet.layer2.2.conv2.weight: Frozen\n",
      "resnet.layer2.2.bn2.weight: Frozen\n",
      "resnet.layer2.2.bn2.bias: Frozen\n",
      "resnet.layer2.2.conv3.weight: Frozen\n",
      "resnet.layer2.2.bn3.weight: Frozen\n",
      "resnet.layer2.2.bn3.bias: Frozen\n",
      "resnet.layer2.3.conv1.weight: Frozen\n",
      "resnet.layer2.3.bn1.weight: Frozen\n",
      "resnet.layer2.3.bn1.bias: Frozen\n",
      "resnet.layer2.3.conv2.weight: Frozen\n",
      "resnet.layer2.3.bn2.weight: Frozen\n",
      "resnet.layer2.3.bn2.bias: Frozen\n",
      "resnet.layer2.3.conv3.weight: Frozen\n",
      "resnet.layer2.3.bn3.weight: Frozen\n",
      "resnet.layer2.3.bn3.bias: Frozen\n",
      "resnet.layer3.0.conv1.weight: Frozen\n",
      "resnet.layer3.0.bn1.weight: Frozen\n",
      "resnet.layer3.0.bn1.bias: Frozen\n",
      "resnet.layer3.0.conv2.weight: Frozen\n",
      "resnet.layer3.0.bn2.weight: Frozen\n",
      "resnet.layer3.0.bn2.bias: Frozen\n",
      "resnet.layer3.0.conv3.weight: Frozen\n",
      "resnet.layer3.0.bn3.weight: Frozen\n",
      "resnet.layer3.0.bn3.bias: Frozen\n",
      "resnet.layer3.0.downsample.0.weight: Frozen\n",
      "resnet.layer3.0.downsample.1.weight: Frozen\n",
      "resnet.layer3.0.downsample.1.bias: Frozen\n",
      "resnet.layer3.1.conv1.weight: Frozen\n",
      "resnet.layer3.1.bn1.weight: Frozen\n",
      "resnet.layer3.1.bn1.bias: Frozen\n",
      "resnet.layer3.1.conv2.weight: Frozen\n",
      "resnet.layer3.1.bn2.weight: Frozen\n",
      "resnet.layer3.1.bn2.bias: Frozen\n",
      "resnet.layer3.1.conv3.weight: Frozen\n",
      "resnet.layer3.1.bn3.weight: Frozen\n",
      "resnet.layer3.1.bn3.bias: Frozen\n",
      "resnet.layer3.2.conv1.weight: Frozen\n",
      "resnet.layer3.2.bn1.weight: Frozen\n",
      "resnet.layer3.2.bn1.bias: Frozen\n",
      "resnet.layer3.2.conv2.weight: Frozen\n",
      "resnet.layer3.2.bn2.weight: Frozen\n",
      "resnet.layer3.2.bn2.bias: Frozen\n",
      "resnet.layer3.2.conv3.weight: Frozen\n",
      "resnet.layer3.2.bn3.weight: Frozen\n",
      "resnet.layer3.2.bn3.bias: Frozen\n",
      "resnet.layer3.3.conv1.weight: Frozen\n",
      "resnet.layer3.3.bn1.weight: Frozen\n",
      "resnet.layer3.3.bn1.bias: Frozen\n",
      "resnet.layer3.3.conv2.weight: Frozen\n",
      "resnet.layer3.3.bn2.weight: Frozen\n",
      "resnet.layer3.3.bn2.bias: Frozen\n",
      "resnet.layer3.3.conv3.weight: Frozen\n",
      "resnet.layer3.3.bn3.weight: Frozen\n",
      "resnet.layer3.3.bn3.bias: Frozen\n",
      "resnet.layer3.4.conv1.weight: Frozen\n",
      "resnet.layer3.4.bn1.weight: Frozen\n",
      "resnet.layer3.4.bn1.bias: Frozen\n",
      "resnet.layer3.4.conv2.weight: Frozen\n",
      "resnet.layer3.4.bn2.weight: Frozen\n",
      "resnet.layer3.4.bn2.bias: Frozen\n",
      "resnet.layer3.4.conv3.weight: Frozen\n",
      "resnet.layer3.4.bn3.weight: Frozen\n",
      "resnet.layer3.4.bn3.bias: Frozen\n",
      "resnet.layer3.5.conv1.weight: Frozen\n",
      "resnet.layer3.5.bn1.weight: Frozen\n",
      "resnet.layer3.5.bn1.bias: Frozen\n",
      "resnet.layer3.5.conv2.weight: Frozen\n",
      "resnet.layer3.5.bn2.weight: Frozen\n",
      "resnet.layer3.5.bn2.bias: Frozen\n",
      "resnet.layer3.5.conv3.weight: Frozen\n",
      "resnet.layer3.5.bn3.weight: Frozen\n",
      "resnet.layer3.5.bn3.bias: Frozen\n",
      "resnet.layer4.0.conv1.weight: Frozen\n",
      "resnet.layer4.0.bn1.weight: Frozen\n",
      "resnet.layer4.0.bn1.bias: Frozen\n",
      "resnet.layer4.0.conv2.weight: Frozen\n",
      "resnet.layer4.0.bn2.weight: Frozen\n",
      "resnet.layer4.0.bn2.bias: Frozen\n",
      "resnet.layer4.0.conv3.weight: Frozen\n",
      "resnet.layer4.0.bn3.weight: Frozen\n",
      "resnet.layer4.0.bn3.bias: Frozen\n",
      "resnet.layer4.0.downsample.0.weight: Frozen\n",
      "resnet.layer4.0.downsample.1.weight: Frozen\n",
      "resnet.layer4.0.downsample.1.bias: Frozen\n",
      "resnet.layer4.1.conv1.weight: Frozen\n",
      "resnet.layer4.1.bn1.weight: Frozen\n",
      "resnet.layer4.1.bn1.bias: Frozen\n",
      "resnet.layer4.1.conv2.weight: Frozen\n",
      "resnet.layer4.1.bn2.weight: Frozen\n",
      "resnet.layer4.1.bn2.bias: Frozen\n",
      "resnet.layer4.1.conv3.weight: Frozen\n",
      "resnet.layer4.1.bn3.weight: Frozen\n",
      "resnet.layer4.1.bn3.bias: Frozen\n",
      "resnet.layer4.2.conv1.weight: Frozen\n",
      "resnet.layer4.2.bn1.weight: Frozen\n",
      "resnet.layer4.2.bn1.bias: Frozen\n",
      "resnet.layer4.2.conv2.weight: Frozen\n",
      "resnet.layer4.2.bn2.weight: Frozen\n",
      "resnet.layer4.2.bn2.bias: Frozen\n",
      "resnet.layer4.2.conv3.weight: Frozen\n",
      "resnet.layer4.2.bn3.weight: Frozen\n",
      "resnet.layer4.2.bn3.bias: Frozen\n",
      "inception.Conv2d_1a_3x3.conv.weight: Frozen\n",
      "inception.Conv2d_1a_3x3.bn.weight: Frozen\n",
      "inception.Conv2d_1a_3x3.bn.bias: Frozen\n",
      "inception.Conv2d_2a_3x3.conv.weight: Frozen\n",
      "inception.Conv2d_2a_3x3.bn.weight: Frozen\n",
      "inception.Conv2d_2a_3x3.bn.bias: Frozen\n",
      "inception.Conv2d_2b_3x3.conv.weight: Frozen\n",
      "inception.Conv2d_2b_3x3.bn.weight: Frozen\n",
      "inception.Conv2d_2b_3x3.bn.bias: Frozen\n",
      "inception.Conv2d_3b_1x1.conv.weight: Frozen\n",
      "inception.Conv2d_3b_1x1.bn.weight: Frozen\n",
      "inception.Conv2d_3b_1x1.bn.bias: Frozen\n",
      "inception.Conv2d_4a_3x3.conv.weight: Frozen\n",
      "inception.Conv2d_4a_3x3.bn.weight: Frozen\n",
      "inception.Conv2d_4a_3x3.bn.bias: Frozen\n",
      "inception.Mixed_5b.branch1x1.conv.weight: Frozen\n",
      "inception.Mixed_5b.branch1x1.bn.weight: Frozen\n",
      "inception.Mixed_5b.branch1x1.bn.bias: Frozen\n",
      "inception.Mixed_5b.branch5x5_1.conv.weight: Frozen\n",
      "inception.Mixed_5b.branch5x5_1.bn.weight: Frozen\n",
      "inception.Mixed_5b.branch5x5_1.bn.bias: Frozen\n",
      "inception.Mixed_5b.branch5x5_2.conv.weight: Frozen\n",
      "inception.Mixed_5b.branch5x5_2.bn.weight: Frozen\n",
      "inception.Mixed_5b.branch5x5_2.bn.bias: Frozen\n",
      "inception.Mixed_5b.branch3x3dbl_1.conv.weight: Frozen\n",
      "inception.Mixed_5b.branch3x3dbl_1.bn.weight: Frozen\n",
      "inception.Mixed_5b.branch3x3dbl_1.bn.bias: Frozen\n",
      "inception.Mixed_5b.branch3x3dbl_2.conv.weight: Frozen\n",
      "inception.Mixed_5b.branch3x3dbl_2.bn.weight: Frozen\n",
      "inception.Mixed_5b.branch3x3dbl_2.bn.bias: Frozen\n",
      "inception.Mixed_5b.branch3x3dbl_3.conv.weight: Frozen\n",
      "inception.Mixed_5b.branch3x3dbl_3.bn.weight: Frozen\n",
      "inception.Mixed_5b.branch3x3dbl_3.bn.bias: Frozen\n",
      "inception.Mixed_5b.branch_pool.conv.weight: Frozen\n",
      "inception.Mixed_5b.branch_pool.bn.weight: Frozen\n",
      "inception.Mixed_5b.branch_pool.bn.bias: Frozen\n",
      "inception.Mixed_5c.branch1x1.conv.weight: Frozen\n",
      "inception.Mixed_5c.branch1x1.bn.weight: Frozen\n",
      "inception.Mixed_5c.branch1x1.bn.bias: Frozen\n",
      "inception.Mixed_5c.branch5x5_1.conv.weight: Frozen\n",
      "inception.Mixed_5c.branch5x5_1.bn.weight: Frozen\n",
      "inception.Mixed_5c.branch5x5_1.bn.bias: Frozen\n",
      "inception.Mixed_5c.branch5x5_2.conv.weight: Frozen\n",
      "inception.Mixed_5c.branch5x5_2.bn.weight: Frozen\n",
      "inception.Mixed_5c.branch5x5_2.bn.bias: Frozen\n",
      "inception.Mixed_5c.branch3x3dbl_1.conv.weight: Frozen\n",
      "inception.Mixed_5c.branch3x3dbl_1.bn.weight: Frozen\n",
      "inception.Mixed_5c.branch3x3dbl_1.bn.bias: Frozen\n",
      "inception.Mixed_5c.branch3x3dbl_2.conv.weight: Frozen\n",
      "inception.Mixed_5c.branch3x3dbl_2.bn.weight: Frozen\n",
      "inception.Mixed_5c.branch3x3dbl_2.bn.bias: Frozen\n",
      "inception.Mixed_5c.branch3x3dbl_3.conv.weight: Frozen\n",
      "inception.Mixed_5c.branch3x3dbl_3.bn.weight: Frozen\n",
      "inception.Mixed_5c.branch3x3dbl_3.bn.bias: Frozen\n",
      "inception.Mixed_5c.branch_pool.conv.weight: Frozen\n",
      "inception.Mixed_5c.branch_pool.bn.weight: Frozen\n",
      "inception.Mixed_5c.branch_pool.bn.bias: Frozen\n",
      "inception.Mixed_5d.branch1x1.conv.weight: Frozen\n",
      "inception.Mixed_5d.branch1x1.bn.weight: Frozen\n",
      "inception.Mixed_5d.branch1x1.bn.bias: Frozen\n",
      "inception.Mixed_5d.branch5x5_1.conv.weight: Frozen\n",
      "inception.Mixed_5d.branch5x5_1.bn.weight: Frozen\n",
      "inception.Mixed_5d.branch5x5_1.bn.bias: Frozen\n",
      "inception.Mixed_5d.branch5x5_2.conv.weight: Frozen\n",
      "inception.Mixed_5d.branch5x5_2.bn.weight: Frozen\n",
      "inception.Mixed_5d.branch5x5_2.bn.bias: Frozen\n",
      "inception.Mixed_5d.branch3x3dbl_1.conv.weight: Frozen\n",
      "inception.Mixed_5d.branch3x3dbl_1.bn.weight: Frozen\n",
      "inception.Mixed_5d.branch3x3dbl_1.bn.bias: Frozen\n",
      "inception.Mixed_5d.branch3x3dbl_2.conv.weight: Frozen\n",
      "inception.Mixed_5d.branch3x3dbl_2.bn.weight: Frozen\n",
      "inception.Mixed_5d.branch3x3dbl_2.bn.bias: Frozen\n",
      "inception.Mixed_5d.branch3x3dbl_3.conv.weight: Frozen\n",
      "inception.Mixed_5d.branch3x3dbl_3.bn.weight: Frozen\n",
      "inception.Mixed_5d.branch3x3dbl_3.bn.bias: Frozen\n",
      "inception.Mixed_5d.branch_pool.conv.weight: Frozen\n",
      "inception.Mixed_5d.branch_pool.bn.weight: Frozen\n",
      "inception.Mixed_5d.branch_pool.bn.bias: Frozen\n",
      "inception.Mixed_6a.branch3x3.conv.weight: Frozen\n",
      "inception.Mixed_6a.branch3x3.bn.weight: Frozen\n",
      "inception.Mixed_6a.branch3x3.bn.bias: Frozen\n",
      "inception.Mixed_6a.branch3x3dbl_1.conv.weight: Frozen\n",
      "inception.Mixed_6a.branch3x3dbl_1.bn.weight: Frozen\n",
      "inception.Mixed_6a.branch3x3dbl_1.bn.bias: Frozen\n",
      "inception.Mixed_6a.branch3x3dbl_2.conv.weight: Frozen\n",
      "inception.Mixed_6a.branch3x3dbl_2.bn.weight: Frozen\n",
      "inception.Mixed_6a.branch3x3dbl_2.bn.bias: Frozen\n",
      "inception.Mixed_6a.branch3x3dbl_3.conv.weight: Frozen\n",
      "inception.Mixed_6a.branch3x3dbl_3.bn.weight: Frozen\n",
      "inception.Mixed_6a.branch3x3dbl_3.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch1x1.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch1x1.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch1x1.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch7x7_1.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7_1.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7_1.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch7x7_2.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7_2.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7_2.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch7x7_3.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7_3.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7_3.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_1.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_1.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_1.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_2.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_2.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_2.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_3.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_3.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_3.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_4.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_4.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_4.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_5.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_5.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_5.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch_pool.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch_pool.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch_pool.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch1x1.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch1x1.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch1x1.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch7x7_1.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7_1.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7_1.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch7x7_2.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7_2.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7_2.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch7x7_3.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7_3.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7_3.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_1.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_1.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_1.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_2.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_2.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_2.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_3.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_3.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_3.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_4.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_4.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_4.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_5.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_5.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_5.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch_pool.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch_pool.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch_pool.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch1x1.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch1x1.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch1x1.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch7x7_1.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7_1.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7_1.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch7x7_2.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7_2.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7_2.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch7x7_3.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7_3.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7_3.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_1.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_1.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_1.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_2.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_2.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_2.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_3.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_3.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_3.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_4.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_4.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_4.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_5.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_5.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_5.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch_pool.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch_pool.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch_pool.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch1x1.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch1x1.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch1x1.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch7x7_1.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7_1.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7_1.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch7x7_2.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7_2.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7_2.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch7x7_3.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7_3.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7_3.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_1.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_1.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_1.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_2.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_2.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_2.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_3.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_3.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_3.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_4.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_4.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_4.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_5.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_5.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_5.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch_pool.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch_pool.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch_pool.bn.bias: Frozen\n",
      "inception.Mixed_7a.branch3x3_1.conv.weight: Frozen\n",
      "inception.Mixed_7a.branch3x3_1.bn.weight: Frozen\n",
      "inception.Mixed_7a.branch3x3_1.bn.bias: Frozen\n",
      "inception.Mixed_7a.branch3x3_2.conv.weight: Frozen\n",
      "inception.Mixed_7a.branch3x3_2.bn.weight: Frozen\n",
      "inception.Mixed_7a.branch3x3_2.bn.bias: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_1.conv.weight: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_1.bn.weight: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_1.bn.bias: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_2.conv.weight: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_2.bn.weight: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_2.bn.bias: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_3.conv.weight: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_3.bn.weight: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_3.bn.bias: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_4.conv.weight: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_4.bn.weight: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_4.bn.bias: Frozen\n",
      "inception.Mixed_7b.branch1x1.conv.weight: Frozen\n",
      "inception.Mixed_7b.branch1x1.bn.weight: Frozen\n",
      "inception.Mixed_7b.branch1x1.bn.bias: Frozen\n",
      "inception.Mixed_7b.branch3x3_1.conv.weight: Frozen\n",
      "inception.Mixed_7b.branch3x3_1.bn.weight: Frozen\n",
      "inception.Mixed_7b.branch3x3_1.bn.bias: Frozen\n",
      "inception.Mixed_7b.branch3x3_2a.conv.weight: Frozen\n",
      "inception.Mixed_7b.branch3x3_2a.bn.weight: Frozen\n",
      "inception.Mixed_7b.branch3x3_2a.bn.bias: Frozen\n",
      "inception.Mixed_7b.branch3x3_2b.conv.weight: Frozen\n",
      "inception.Mixed_7b.branch3x3_2b.bn.weight: Frozen\n",
      "inception.Mixed_7b.branch3x3_2b.bn.bias: Frozen\n",
      "inception.Mixed_7b.branch3x3dbl_1.conv.weight: Frozen\n",
      "inception.Mixed_7b.branch3x3dbl_1.bn.weight: Frozen\n",
      "inception.Mixed_7b.branch3x3dbl_1.bn.bias: Frozen\n",
      "inception.Mixed_7b.branch3x3dbl_2.conv.weight: Frozen\n",
      "inception.Mixed_7b.branch3x3dbl_2.bn.weight: Frozen\n",
      "inception.Mixed_7b.branch3x3dbl_2.bn.bias: Frozen\n",
      "inception.Mixed_7b.branch3x3dbl_3a.conv.weight: Frozen\n",
      "inception.Mixed_7b.branch3x3dbl_3a.bn.weight: Frozen\n",
      "inception.Mixed_7b.branch3x3dbl_3a.bn.bias: Frozen\n",
      "inception.Mixed_7b.branch3x3dbl_3b.conv.weight: Frozen\n",
      "inception.Mixed_7b.branch3x3dbl_3b.bn.weight: Frozen\n",
      "inception.Mixed_7b.branch3x3dbl_3b.bn.bias: Frozen\n",
      "inception.Mixed_7b.branch_pool.conv.weight: Frozen\n",
      "inception.Mixed_7b.branch_pool.bn.weight: Frozen\n",
      "inception.Mixed_7b.branch_pool.bn.bias: Frozen\n",
      "inception.Mixed_7c.branch1x1.conv.weight: Frozen\n",
      "inception.Mixed_7c.branch1x1.bn.weight: Frozen\n",
      "inception.Mixed_7c.branch1x1.bn.bias: Frozen\n",
      "inception.Mixed_7c.branch3x3_1.conv.weight: Frozen\n",
      "inception.Mixed_7c.branch3x3_1.bn.weight: Frozen\n",
      "inception.Mixed_7c.branch3x3_1.bn.bias: Frozen\n",
      "inception.Mixed_7c.branch3x3_2a.conv.weight: Frozen\n",
      "inception.Mixed_7c.branch3x3_2a.bn.weight: Frozen\n",
      "inception.Mixed_7c.branch3x3_2a.bn.bias: Frozen\n",
      "inception.Mixed_7c.branch3x3_2b.conv.weight: Frozen\n",
      "inception.Mixed_7c.branch3x3_2b.bn.weight: Frozen\n",
      "inception.Mixed_7c.branch3x3_2b.bn.bias: Frozen\n",
      "inception.Mixed_7c.branch3x3dbl_1.conv.weight: Frozen\n",
      "inception.Mixed_7c.branch3x3dbl_1.bn.weight: Frozen\n",
      "inception.Mixed_7c.branch3x3dbl_1.bn.bias: Frozen\n",
      "inception.Mixed_7c.branch3x3dbl_2.conv.weight: Frozen\n",
      "inception.Mixed_7c.branch3x3dbl_2.bn.weight: Frozen\n",
      "inception.Mixed_7c.branch3x3dbl_2.bn.bias: Frozen\n",
      "inception.Mixed_7c.branch3x3dbl_3a.conv.weight: Frozen\n",
      "inception.Mixed_7c.branch3x3dbl_3a.bn.weight: Frozen\n",
      "inception.Mixed_7c.branch3x3dbl_3a.bn.bias: Frozen\n",
      "inception.Mixed_7c.branch3x3dbl_3b.conv.weight: Frozen\n",
      "inception.Mixed_7c.branch3x3dbl_3b.bn.weight: Frozen\n",
      "inception.Mixed_7c.branch3x3dbl_3b.bn.bias: Frozen\n",
      "inception.Mixed_7c.branch_pool.conv.weight: Frozen\n",
      "inception.Mixed_7c.branch_pool.bn.weight: Frozen\n",
      "inception.Mixed_7c.branch_pool.bn.bias: Frozen\n",
      "batch_norm.weight: Trainable\n",
      "batch_norm.bias: Trainable\n",
      "fc.weight: Trainable\n",
      "fc.bias: Trainable\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "from torchvision.models import inception_v3\n",
    "\n",
    "\n",
    "torch.set_printoptions(threshold=10_000)  # Adjust threshold if needed\n",
    "print(model)\n",
    "# summary(model, input_size=[(1, 3, 224, 224), (1, 3, 299, 299)])\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {'Trainable' if param.requires_grad else 'Frozen'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|| 1/1 [00:01<00:00,  1.24s/it, accuracy=100, loss=0.407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5: Train Loss = 0.4066, Train Accuracy = 100.00%\n",
      "Validation Loss = 0.5250, Validation Accuracy = 100.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|| 1/1 [00:01<00:00,  1.03s/it, accuracy=92, loss=0.301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/5: Train Loss = 0.3008, Train Accuracy = 92.00%\n",
      "Validation Loss = 0.4981, Validation Accuracy = 85.71%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|| 1/1 [00:01<00:00,  1.01s/it, accuracy=100, loss=0.204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/5: Train Loss = 0.2040, Train Accuracy = 100.00%\n",
      "Validation Loss = 0.4746, Validation Accuracy = 85.71%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|| 1/1 [00:01<00:00,  1.01s/it, accuracy=100, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/5: Train Loss = 0.2013, Train Accuracy = 100.00%\n",
      "Validation Loss = 0.4581, Validation Accuracy = 85.71%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|| 1/1 [00:01<00:00,  1.00s/it, accuracy=100, loss=0.154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/5: Train Loss = 0.1545, Train Accuracy = 100.00%\n",
      "Validation Loss = 0.4484, Validation Accuracy = 85.71%\n",
      "\n",
      "Training complete\n",
      "\n",
      "Additional Metrics:\n",
      "False Positive Rate (FPR): 0.3333\n",
      "False Negative Rate (FNR): 0.0000\n",
      "AUC-ROC Score: 1.0000\n",
      "TN: 2, FP: 1, FN: 0, TP: 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.67      0.80         3\n",
      "         1.0       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.86         7\n",
      "   macro avg       0.90      0.83      0.84         7\n",
      "weighted avg       0.89      0.86      0.85         7\n",
      "\n",
      "Accuracy Score: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "def train_model(model, train_loader_resnet, train_loader_inception, val_loader_resnet, val_loader_inception, \n",
    "                criterion, optimizer, epochs=5):\n",
    "   \n",
    "    \n",
    "    history = {\n",
    "        \"epoch\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"train_accuracy\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_accuracy\": []\n",
    "    }\n",
    "    metrics = {\n",
    "    \"epoch\": [],\n",
    "    \"class_names\": [],\n",
    "    \"confusion_matrix\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"fpr\": [],\n",
    "    \"fnr\": [],\n",
    "    \"auc_roc\": [],\n",
    "    \"roc_curve_fpr\": [],\n",
    "    \"roc_curve_tpr\": []\n",
    "}\n",
    "\n",
    "       \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Use tqdm on the zip object directly\n",
    "        progress_bar = tqdm(zip(train_loader_resnet, train_loader_inception), \n",
    "                             total=len(train_loader_resnet), \n",
    "                             desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for (resnet_batch, inception_batch) in progress_bar:\n",
    "            x_resnet, y = resnet_batch\n",
    "            x_inception, _ = inception_batch\n",
    "            \n",
    "            # Move data to the appropriate device\n",
    "            x_resnet, x_inception, y = x_resnet.to(device), x_inception.to(device), y.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_resnet, x_inception).squeeze()\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "            # Update progress bar with loss and accuracy\n",
    "            progress_bar.set_postfix(loss=loss.item(), accuracy=100 * correct / total)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader_resnet)\n",
    "        epoch_acc = 100 * correct / total\n",
    "\n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        y_true, y_pred, y_prob = [], [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (resnet_batch, inception_batch) in zip(val_loader_resnet, val_loader_inception):\n",
    "                x_resnet, y = resnet_batch\n",
    "                x_inception, _ = inception_batch\n",
    "                x_resnet, x_inception, y = x_resnet.to(device), x_inception.to(device), y.to(device).float()\n",
    "\n",
    "                outputs = model(x_resnet, x_inception).squeeze()\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = (outputs > 0.5).float()\n",
    "                val_correct += (preds == y).sum().item()\n",
    "                val_total += y.size(0)\n",
    "                \n",
    "                y_true.extend(y.cpu().numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "                y_prob.extend(outputs.cpu().numpy())\n",
    "\n",
    "\n",
    "        val_loss /= len(val_loader_resnet)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}: Train Loss = {epoch_loss:.4f}, Train Accuracy = {epoch_acc:.2f}%\")\n",
    "        print(f\"Validation Loss = {val_loss:.4f}, Validation Accuracy = {val_acc:.2f}%\\n\")\n",
    "        \n",
    "        # Store metrics\n",
    "        history[\"epoch\"].append(epoch + 1)\n",
    "        history[\"train_loss\"].append(epoch_loss)\n",
    "        history[\"train_accuracy\"].append(epoch_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_accuracy\"].append(val_acc)\n",
    "\n",
    "\n",
    "        model.train()  # Return model to training mode\n",
    "\n",
    "    print(\"Training complete\")\n",
    "   \n",
    "    # Compute Additional Metrics\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    auc_score = roc_auc_score(y_true, y_prob)\n",
    "    fpr_vals, tpr_vals, _ = roc_curve(y_true, y_prob)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (fn + tp)\n",
    "    \n",
    "    print(\"\\nAdditional Metrics:\")\n",
    "    print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n",
    "    print(f\"False Negative Rate (FNR): {fnr:.4f}\")\n",
    "    print(f\"AUC-ROC Score: {auc_score:.4f}\")\n",
    "    print(f\"TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\")  # Explicit print\n",
    "    # Save Additional Metrics\n",
    "  # Update metrics dictionary for the current epoch (replace `epoch_number` with actual epoch)\n",
    "    metrics[\"class_names\"].append(class_names)\n",
    "    metrics[\"confusion_matrix\"].append(conf_matrix.ravel())  # Flatten confusion matrix to a list\n",
    "    metrics[\"fpr\"].append(fpr)\n",
    "    metrics[\"fnr\"].append(fnr)\n",
    "    metrics[\"auc_roc\"].append(auc_score)\n",
    "    metrics[\"roc_curve_fpr\"].append(fpr_vals)\n",
    "    metrics[\"roc_curve_tpr\"].append(tpr_vals)\n",
    "   \n",
    "    return history, metrics  # Return history dictionary for immediate plotting if needed\n",
    "\n",
    "\n",
    "history, metrics = train_model(model, train_loader_resnet, train_loader_inception, \n",
    "                      val_loader_resnet, val_loader_inception, \n",
    "                      criterion, optimizer, epochs=5 )\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (resnet_batch, inception_batch) in zip(val_loader_resnet, val_loader_inception):\n",
    "        x_resnet, y = resnet_batch\n",
    "        x_inception, _ = inception_batch\n",
    "        x_resnet, x_inception, y = x_resnet.to(\"cuda\"), x_inception.to(\"cuda\"), y.to(\"cuda\").float()\n",
    "        outputs = model(x_resnet, x_inception).squeeze()\n",
    "        preds = (outputs > 0.5).cpu().numpy()\n",
    "        y_true.extend(y.cpu().numpy())\n",
    "        y_pred.extend(preds)\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(f\"Accuracy Score: {accuracy_score(y_true, y_pred)}\")\n",
    "\n",
    "\n",
    "  # Save history to CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the last 50 layers of Inception and the last 94 layers of ResNet\n",
    "for param in list(model.inception.parameters())[-50:]:\n",
    "    param.requires_grad = True\n",
    "for param in list(model.resnet.parameters())[-94:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    " \n",
    "# for param in model.resnet.layer4.parameters():  # Unfreezing layer4 (last block)\n",
    "#     param.requires_grad = True\n",
    "    \n",
    "# Reset optimizer with a lower learning rate\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FusionModel(\n",
      "  (resnet): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (inception): Inception3(\n",
      "    (Conv2d_1a_3x3): BasicConv2d(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (Conv2d_2a_3x3): BasicConv2d(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (Conv2d_2b_3x3): BasicConv2d(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (Conv2d_3b_1x1): BasicConv2d(\n",
      "      (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (Conv2d_4a_3x3): BasicConv2d(\n",
      "      (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (Mixed_5b): InceptionA(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch5x5_1): BasicConv2d(\n",
      "        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch5x5_2): BasicConv2d(\n",
      "        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_5c): InceptionA(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch5x5_1): BasicConv2d(\n",
      "        (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch5x5_2): BasicConv2d(\n",
      "        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_5d): InceptionA(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch5x5_1): BasicConv2d(\n",
      "        (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch5x5_2): BasicConv2d(\n",
      "        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_6a): InceptionB(\n",
      "      (branch3x3): BasicConv2d(\n",
      "        (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_6b): InceptionC(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_2): BasicConv2d(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_3): BasicConv2d(\n",
      "        (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_4): BasicConv2d(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_5): BasicConv2d(\n",
      "        (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_6c): InceptionC(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_2): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_3): BasicConv2d(\n",
      "        (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_4): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_5): BasicConv2d(\n",
      "        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_6d): InceptionC(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_2): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_3): BasicConv2d(\n",
      "        (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_4): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_5): BasicConv2d(\n",
      "        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_6e): InceptionC(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_2): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7_3): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_4): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7dbl_5): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_7a): InceptionD(\n",
      "      (branch3x3_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3_2): BasicConv2d(\n",
      "        (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7x3_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7x3_2): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7x3_3): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch7x7x3_4): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_7b): InceptionE(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3_1): BasicConv2d(\n",
      "        (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3_2a): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3_2b): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_3a): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_3b): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (Mixed_7c): InceptionE(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3_1): BasicConv2d(\n",
      "        (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3_2a): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3_2b): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_3a): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch3x3dbl_3b): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (batch_norm): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.55, inplace=False)\n",
      "  (fc): Linear(in_features=4096, out_features=1, bias=True)\n",
      ")\n",
      "resnet.conv1.weight: Frozen\n",
      "resnet.bn1.weight: Frozen\n",
      "resnet.bn1.bias: Frozen\n",
      "resnet.layer1.0.conv1.weight: Frozen\n",
      "resnet.layer1.0.bn1.weight: Frozen\n",
      "resnet.layer1.0.bn1.bias: Frozen\n",
      "resnet.layer1.0.conv2.weight: Frozen\n",
      "resnet.layer1.0.bn2.weight: Frozen\n",
      "resnet.layer1.0.bn2.bias: Frozen\n",
      "resnet.layer1.0.conv3.weight: Frozen\n",
      "resnet.layer1.0.bn3.weight: Frozen\n",
      "resnet.layer1.0.bn3.bias: Frozen\n",
      "resnet.layer1.0.downsample.0.weight: Frozen\n",
      "resnet.layer1.0.downsample.1.weight: Frozen\n",
      "resnet.layer1.0.downsample.1.bias: Frozen\n",
      "resnet.layer1.1.conv1.weight: Frozen\n",
      "resnet.layer1.1.bn1.weight: Frozen\n",
      "resnet.layer1.1.bn1.bias: Frozen\n",
      "resnet.layer1.1.conv2.weight: Frozen\n",
      "resnet.layer1.1.bn2.weight: Frozen\n",
      "resnet.layer1.1.bn2.bias: Frozen\n",
      "resnet.layer1.1.conv3.weight: Frozen\n",
      "resnet.layer1.1.bn3.weight: Frozen\n",
      "resnet.layer1.1.bn3.bias: Frozen\n",
      "resnet.layer1.2.conv1.weight: Frozen\n",
      "resnet.layer1.2.bn1.weight: Frozen\n",
      "resnet.layer1.2.bn1.bias: Frozen\n",
      "resnet.layer1.2.conv2.weight: Frozen\n",
      "resnet.layer1.2.bn2.weight: Frozen\n",
      "resnet.layer1.2.bn2.bias: Frozen\n",
      "resnet.layer1.2.conv3.weight: Frozen\n",
      "resnet.layer1.2.bn3.weight: Frozen\n",
      "resnet.layer1.2.bn3.bias: Frozen\n",
      "resnet.layer2.0.conv1.weight: Frozen\n",
      "resnet.layer2.0.bn1.weight: Frozen\n",
      "resnet.layer2.0.bn1.bias: Frozen\n",
      "resnet.layer2.0.conv2.weight: Frozen\n",
      "resnet.layer2.0.bn2.weight: Frozen\n",
      "resnet.layer2.0.bn2.bias: Frozen\n",
      "resnet.layer2.0.conv3.weight: Frozen\n",
      "resnet.layer2.0.bn3.weight: Frozen\n",
      "resnet.layer2.0.bn3.bias: Frozen\n",
      "resnet.layer2.0.downsample.0.weight: Frozen\n",
      "resnet.layer2.0.downsample.1.weight: Frozen\n",
      "resnet.layer2.0.downsample.1.bias: Frozen\n",
      "resnet.layer2.1.conv1.weight: Frozen\n",
      "resnet.layer2.1.bn1.weight: Frozen\n",
      "resnet.layer2.1.bn1.bias: Frozen\n",
      "resnet.layer2.1.conv2.weight: Frozen\n",
      "resnet.layer2.1.bn2.weight: Frozen\n",
      "resnet.layer2.1.bn2.bias: Frozen\n",
      "resnet.layer2.1.conv3.weight: Frozen\n",
      "resnet.layer2.1.bn3.weight: Frozen\n",
      "resnet.layer2.1.bn3.bias: Frozen\n",
      "resnet.layer2.2.conv1.weight: Frozen\n",
      "resnet.layer2.2.bn1.weight: Frozen\n",
      "resnet.layer2.2.bn1.bias: Frozen\n",
      "resnet.layer2.2.conv2.weight: Frozen\n",
      "resnet.layer2.2.bn2.weight: Frozen\n",
      "resnet.layer2.2.bn2.bias: Frozen\n",
      "resnet.layer2.2.conv3.weight: Frozen\n",
      "resnet.layer2.2.bn3.weight: Frozen\n",
      "resnet.layer2.2.bn3.bias: Frozen\n",
      "resnet.layer2.3.conv1.weight: Frozen\n",
      "resnet.layer2.3.bn1.weight: Frozen\n",
      "resnet.layer2.3.bn1.bias: Trainable\n",
      "resnet.layer2.3.conv2.weight: Trainable\n",
      "resnet.layer2.3.bn2.weight: Trainable\n",
      "resnet.layer2.3.bn2.bias: Trainable\n",
      "resnet.layer2.3.conv3.weight: Trainable\n",
      "resnet.layer2.3.bn3.weight: Trainable\n",
      "resnet.layer2.3.bn3.bias: Trainable\n",
      "resnet.layer3.0.conv1.weight: Trainable\n",
      "resnet.layer3.0.bn1.weight: Trainable\n",
      "resnet.layer3.0.bn1.bias: Trainable\n",
      "resnet.layer3.0.conv2.weight: Trainable\n",
      "resnet.layer3.0.bn2.weight: Trainable\n",
      "resnet.layer3.0.bn2.bias: Trainable\n",
      "resnet.layer3.0.conv3.weight: Trainable\n",
      "resnet.layer3.0.bn3.weight: Trainable\n",
      "resnet.layer3.0.bn3.bias: Trainable\n",
      "resnet.layer3.0.downsample.0.weight: Trainable\n",
      "resnet.layer3.0.downsample.1.weight: Trainable\n",
      "resnet.layer3.0.downsample.1.bias: Trainable\n",
      "resnet.layer3.1.conv1.weight: Trainable\n",
      "resnet.layer3.1.bn1.weight: Trainable\n",
      "resnet.layer3.1.bn1.bias: Trainable\n",
      "resnet.layer3.1.conv2.weight: Trainable\n",
      "resnet.layer3.1.bn2.weight: Trainable\n",
      "resnet.layer3.1.bn2.bias: Trainable\n",
      "resnet.layer3.1.conv3.weight: Trainable\n",
      "resnet.layer3.1.bn3.weight: Trainable\n",
      "resnet.layer3.1.bn3.bias: Trainable\n",
      "resnet.layer3.2.conv1.weight: Trainable\n",
      "resnet.layer3.2.bn1.weight: Trainable\n",
      "resnet.layer3.2.bn1.bias: Trainable\n",
      "resnet.layer3.2.conv2.weight: Trainable\n",
      "resnet.layer3.2.bn2.weight: Trainable\n",
      "resnet.layer3.2.bn2.bias: Trainable\n",
      "resnet.layer3.2.conv3.weight: Trainable\n",
      "resnet.layer3.2.bn3.weight: Trainable\n",
      "resnet.layer3.2.bn3.bias: Trainable\n",
      "resnet.layer3.3.conv1.weight: Trainable\n",
      "resnet.layer3.3.bn1.weight: Trainable\n",
      "resnet.layer3.3.bn1.bias: Trainable\n",
      "resnet.layer3.3.conv2.weight: Trainable\n",
      "resnet.layer3.3.bn2.weight: Trainable\n",
      "resnet.layer3.3.bn2.bias: Trainable\n",
      "resnet.layer3.3.conv3.weight: Trainable\n",
      "resnet.layer3.3.bn3.weight: Trainable\n",
      "resnet.layer3.3.bn3.bias: Trainable\n",
      "resnet.layer3.4.conv1.weight: Trainable\n",
      "resnet.layer3.4.bn1.weight: Trainable\n",
      "resnet.layer3.4.bn1.bias: Trainable\n",
      "resnet.layer3.4.conv2.weight: Trainable\n",
      "resnet.layer3.4.bn2.weight: Trainable\n",
      "resnet.layer3.4.bn2.bias: Trainable\n",
      "resnet.layer3.4.conv3.weight: Trainable\n",
      "resnet.layer3.4.bn3.weight: Trainable\n",
      "resnet.layer3.4.bn3.bias: Trainable\n",
      "resnet.layer3.5.conv1.weight: Trainable\n",
      "resnet.layer3.5.bn1.weight: Trainable\n",
      "resnet.layer3.5.bn1.bias: Trainable\n",
      "resnet.layer3.5.conv2.weight: Trainable\n",
      "resnet.layer3.5.bn2.weight: Trainable\n",
      "resnet.layer3.5.bn2.bias: Trainable\n",
      "resnet.layer3.5.conv3.weight: Trainable\n",
      "resnet.layer3.5.bn3.weight: Trainable\n",
      "resnet.layer3.5.bn3.bias: Trainable\n",
      "resnet.layer4.0.conv1.weight: Trainable\n",
      "resnet.layer4.0.bn1.weight: Trainable\n",
      "resnet.layer4.0.bn1.bias: Trainable\n",
      "resnet.layer4.0.conv2.weight: Trainable\n",
      "resnet.layer4.0.bn2.weight: Trainable\n",
      "resnet.layer4.0.bn2.bias: Trainable\n",
      "resnet.layer4.0.conv3.weight: Trainable\n",
      "resnet.layer4.0.bn3.weight: Trainable\n",
      "resnet.layer4.0.bn3.bias: Trainable\n",
      "resnet.layer4.0.downsample.0.weight: Trainable\n",
      "resnet.layer4.0.downsample.1.weight: Trainable\n",
      "resnet.layer4.0.downsample.1.bias: Trainable\n",
      "resnet.layer4.1.conv1.weight: Trainable\n",
      "resnet.layer4.1.bn1.weight: Trainable\n",
      "resnet.layer4.1.bn1.bias: Trainable\n",
      "resnet.layer4.1.conv2.weight: Trainable\n",
      "resnet.layer4.1.bn2.weight: Trainable\n",
      "resnet.layer4.1.bn2.bias: Trainable\n",
      "resnet.layer4.1.conv3.weight: Trainable\n",
      "resnet.layer4.1.bn3.weight: Trainable\n",
      "resnet.layer4.1.bn3.bias: Trainable\n",
      "resnet.layer4.2.conv1.weight: Trainable\n",
      "resnet.layer4.2.bn1.weight: Trainable\n",
      "resnet.layer4.2.bn1.bias: Trainable\n",
      "resnet.layer4.2.conv2.weight: Trainable\n",
      "resnet.layer4.2.bn2.weight: Trainable\n",
      "resnet.layer4.2.bn2.bias: Trainable\n",
      "resnet.layer4.2.conv3.weight: Trainable\n",
      "resnet.layer4.2.bn3.weight: Trainable\n",
      "resnet.layer4.2.bn3.bias: Trainable\n",
      "inception.Conv2d_1a_3x3.conv.weight: Frozen\n",
      "inception.Conv2d_1a_3x3.bn.weight: Frozen\n",
      "inception.Conv2d_1a_3x3.bn.bias: Frozen\n",
      "inception.Conv2d_2a_3x3.conv.weight: Frozen\n",
      "inception.Conv2d_2a_3x3.bn.weight: Frozen\n",
      "inception.Conv2d_2a_3x3.bn.bias: Frozen\n",
      "inception.Conv2d_2b_3x3.conv.weight: Frozen\n",
      "inception.Conv2d_2b_3x3.bn.weight: Frozen\n",
      "inception.Conv2d_2b_3x3.bn.bias: Frozen\n",
      "inception.Conv2d_3b_1x1.conv.weight: Frozen\n",
      "inception.Conv2d_3b_1x1.bn.weight: Frozen\n",
      "inception.Conv2d_3b_1x1.bn.bias: Frozen\n",
      "inception.Conv2d_4a_3x3.conv.weight: Frozen\n",
      "inception.Conv2d_4a_3x3.bn.weight: Frozen\n",
      "inception.Conv2d_4a_3x3.bn.bias: Frozen\n",
      "inception.Mixed_5b.branch1x1.conv.weight: Frozen\n",
      "inception.Mixed_5b.branch1x1.bn.weight: Frozen\n",
      "inception.Mixed_5b.branch1x1.bn.bias: Frozen\n",
      "inception.Mixed_5b.branch5x5_1.conv.weight: Frozen\n",
      "inception.Mixed_5b.branch5x5_1.bn.weight: Frozen\n",
      "inception.Mixed_5b.branch5x5_1.bn.bias: Frozen\n",
      "inception.Mixed_5b.branch5x5_2.conv.weight: Frozen\n",
      "inception.Mixed_5b.branch5x5_2.bn.weight: Frozen\n",
      "inception.Mixed_5b.branch5x5_2.bn.bias: Frozen\n",
      "inception.Mixed_5b.branch3x3dbl_1.conv.weight: Frozen\n",
      "inception.Mixed_5b.branch3x3dbl_1.bn.weight: Frozen\n",
      "inception.Mixed_5b.branch3x3dbl_1.bn.bias: Frozen\n",
      "inception.Mixed_5b.branch3x3dbl_2.conv.weight: Frozen\n",
      "inception.Mixed_5b.branch3x3dbl_2.bn.weight: Frozen\n",
      "inception.Mixed_5b.branch3x3dbl_2.bn.bias: Frozen\n",
      "inception.Mixed_5b.branch3x3dbl_3.conv.weight: Frozen\n",
      "inception.Mixed_5b.branch3x3dbl_3.bn.weight: Frozen\n",
      "inception.Mixed_5b.branch3x3dbl_3.bn.bias: Frozen\n",
      "inception.Mixed_5b.branch_pool.conv.weight: Frozen\n",
      "inception.Mixed_5b.branch_pool.bn.weight: Frozen\n",
      "inception.Mixed_5b.branch_pool.bn.bias: Frozen\n",
      "inception.Mixed_5c.branch1x1.conv.weight: Frozen\n",
      "inception.Mixed_5c.branch1x1.bn.weight: Frozen\n",
      "inception.Mixed_5c.branch1x1.bn.bias: Frozen\n",
      "inception.Mixed_5c.branch5x5_1.conv.weight: Frozen\n",
      "inception.Mixed_5c.branch5x5_1.bn.weight: Frozen\n",
      "inception.Mixed_5c.branch5x5_1.bn.bias: Frozen\n",
      "inception.Mixed_5c.branch5x5_2.conv.weight: Frozen\n",
      "inception.Mixed_5c.branch5x5_2.bn.weight: Frozen\n",
      "inception.Mixed_5c.branch5x5_2.bn.bias: Frozen\n",
      "inception.Mixed_5c.branch3x3dbl_1.conv.weight: Frozen\n",
      "inception.Mixed_5c.branch3x3dbl_1.bn.weight: Frozen\n",
      "inception.Mixed_5c.branch3x3dbl_1.bn.bias: Frozen\n",
      "inception.Mixed_5c.branch3x3dbl_2.conv.weight: Frozen\n",
      "inception.Mixed_5c.branch3x3dbl_2.bn.weight: Frozen\n",
      "inception.Mixed_5c.branch3x3dbl_2.bn.bias: Frozen\n",
      "inception.Mixed_5c.branch3x3dbl_3.conv.weight: Frozen\n",
      "inception.Mixed_5c.branch3x3dbl_3.bn.weight: Frozen\n",
      "inception.Mixed_5c.branch3x3dbl_3.bn.bias: Frozen\n",
      "inception.Mixed_5c.branch_pool.conv.weight: Frozen\n",
      "inception.Mixed_5c.branch_pool.bn.weight: Frozen\n",
      "inception.Mixed_5c.branch_pool.bn.bias: Frozen\n",
      "inception.Mixed_5d.branch1x1.conv.weight: Frozen\n",
      "inception.Mixed_5d.branch1x1.bn.weight: Frozen\n",
      "inception.Mixed_5d.branch1x1.bn.bias: Frozen\n",
      "inception.Mixed_5d.branch5x5_1.conv.weight: Frozen\n",
      "inception.Mixed_5d.branch5x5_1.bn.weight: Frozen\n",
      "inception.Mixed_5d.branch5x5_1.bn.bias: Frozen\n",
      "inception.Mixed_5d.branch5x5_2.conv.weight: Frozen\n",
      "inception.Mixed_5d.branch5x5_2.bn.weight: Frozen\n",
      "inception.Mixed_5d.branch5x5_2.bn.bias: Frozen\n",
      "inception.Mixed_5d.branch3x3dbl_1.conv.weight: Frozen\n",
      "inception.Mixed_5d.branch3x3dbl_1.bn.weight: Frozen\n",
      "inception.Mixed_5d.branch3x3dbl_1.bn.bias: Frozen\n",
      "inception.Mixed_5d.branch3x3dbl_2.conv.weight: Frozen\n",
      "inception.Mixed_5d.branch3x3dbl_2.bn.weight: Frozen\n",
      "inception.Mixed_5d.branch3x3dbl_2.bn.bias: Frozen\n",
      "inception.Mixed_5d.branch3x3dbl_3.conv.weight: Frozen\n",
      "inception.Mixed_5d.branch3x3dbl_3.bn.weight: Frozen\n",
      "inception.Mixed_5d.branch3x3dbl_3.bn.bias: Frozen\n",
      "inception.Mixed_5d.branch_pool.conv.weight: Frozen\n",
      "inception.Mixed_5d.branch_pool.bn.weight: Frozen\n",
      "inception.Mixed_5d.branch_pool.bn.bias: Frozen\n",
      "inception.Mixed_6a.branch3x3.conv.weight: Frozen\n",
      "inception.Mixed_6a.branch3x3.bn.weight: Frozen\n",
      "inception.Mixed_6a.branch3x3.bn.bias: Frozen\n",
      "inception.Mixed_6a.branch3x3dbl_1.conv.weight: Frozen\n",
      "inception.Mixed_6a.branch3x3dbl_1.bn.weight: Frozen\n",
      "inception.Mixed_6a.branch3x3dbl_1.bn.bias: Frozen\n",
      "inception.Mixed_6a.branch3x3dbl_2.conv.weight: Frozen\n",
      "inception.Mixed_6a.branch3x3dbl_2.bn.weight: Frozen\n",
      "inception.Mixed_6a.branch3x3dbl_2.bn.bias: Frozen\n",
      "inception.Mixed_6a.branch3x3dbl_3.conv.weight: Frozen\n",
      "inception.Mixed_6a.branch3x3dbl_3.bn.weight: Frozen\n",
      "inception.Mixed_6a.branch3x3dbl_3.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch1x1.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch1x1.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch1x1.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch7x7_1.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7_1.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7_1.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch7x7_2.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7_2.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7_2.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch7x7_3.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7_3.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7_3.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_1.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_1.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_1.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_2.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_2.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_2.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_3.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_3.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_3.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_4.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_4.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_4.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_5.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_5.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch7x7dbl_5.bn.bias: Frozen\n",
      "inception.Mixed_6b.branch_pool.conv.weight: Frozen\n",
      "inception.Mixed_6b.branch_pool.bn.weight: Frozen\n",
      "inception.Mixed_6b.branch_pool.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch1x1.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch1x1.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch1x1.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch7x7_1.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7_1.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7_1.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch7x7_2.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7_2.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7_2.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch7x7_3.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7_3.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7_3.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_1.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_1.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_1.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_2.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_2.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_2.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_3.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_3.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_3.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_4.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_4.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_4.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_5.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_5.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch7x7dbl_5.bn.bias: Frozen\n",
      "inception.Mixed_6c.branch_pool.conv.weight: Frozen\n",
      "inception.Mixed_6c.branch_pool.bn.weight: Frozen\n",
      "inception.Mixed_6c.branch_pool.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch1x1.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch1x1.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch1x1.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch7x7_1.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7_1.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7_1.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch7x7_2.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7_2.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7_2.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch7x7_3.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7_3.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7_3.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_1.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_1.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_1.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_2.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_2.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_2.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_3.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_3.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_3.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_4.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_4.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_4.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_5.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_5.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch7x7dbl_5.bn.bias: Frozen\n",
      "inception.Mixed_6d.branch_pool.conv.weight: Frozen\n",
      "inception.Mixed_6d.branch_pool.bn.weight: Frozen\n",
      "inception.Mixed_6d.branch_pool.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch1x1.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch1x1.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch1x1.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch7x7_1.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7_1.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7_1.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch7x7_2.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7_2.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7_2.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch7x7_3.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7_3.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7_3.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_1.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_1.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_1.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_2.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_2.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_2.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_3.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_3.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_3.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_4.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_4.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_4.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_5.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_5.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch7x7dbl_5.bn.bias: Frozen\n",
      "inception.Mixed_6e.branch_pool.conv.weight: Frozen\n",
      "inception.Mixed_6e.branch_pool.bn.weight: Frozen\n",
      "inception.Mixed_6e.branch_pool.bn.bias: Frozen\n",
      "inception.Mixed_7a.branch3x3_1.conv.weight: Frozen\n",
      "inception.Mixed_7a.branch3x3_1.bn.weight: Frozen\n",
      "inception.Mixed_7a.branch3x3_1.bn.bias: Frozen\n",
      "inception.Mixed_7a.branch3x3_2.conv.weight: Frozen\n",
      "inception.Mixed_7a.branch3x3_2.bn.weight: Frozen\n",
      "inception.Mixed_7a.branch3x3_2.bn.bias: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_1.conv.weight: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_1.bn.weight: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_1.bn.bias: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_2.conv.weight: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_2.bn.weight: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_2.bn.bias: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_3.conv.weight: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_3.bn.weight: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_3.bn.bias: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_4.conv.weight: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_4.bn.weight: Frozen\n",
      "inception.Mixed_7a.branch7x7x3_4.bn.bias: Frozen\n",
      "inception.Mixed_7b.branch1x1.conv.weight: Frozen\n",
      "inception.Mixed_7b.branch1x1.bn.weight: Frozen\n",
      "inception.Mixed_7b.branch1x1.bn.bias: Frozen\n",
      "inception.Mixed_7b.branch3x3_1.conv.weight: Frozen\n",
      "inception.Mixed_7b.branch3x3_1.bn.weight: Trainable\n",
      "inception.Mixed_7b.branch3x3_1.bn.bias: Trainable\n",
      "inception.Mixed_7b.branch3x3_2a.conv.weight: Trainable\n",
      "inception.Mixed_7b.branch3x3_2a.bn.weight: Trainable\n",
      "inception.Mixed_7b.branch3x3_2a.bn.bias: Trainable\n",
      "inception.Mixed_7b.branch3x3_2b.conv.weight: Trainable\n",
      "inception.Mixed_7b.branch3x3_2b.bn.weight: Trainable\n",
      "inception.Mixed_7b.branch3x3_2b.bn.bias: Trainable\n",
      "inception.Mixed_7b.branch3x3dbl_1.conv.weight: Trainable\n",
      "inception.Mixed_7b.branch3x3dbl_1.bn.weight: Trainable\n",
      "inception.Mixed_7b.branch3x3dbl_1.bn.bias: Trainable\n",
      "inception.Mixed_7b.branch3x3dbl_2.conv.weight: Trainable\n",
      "inception.Mixed_7b.branch3x3dbl_2.bn.weight: Trainable\n",
      "inception.Mixed_7b.branch3x3dbl_2.bn.bias: Trainable\n",
      "inception.Mixed_7b.branch3x3dbl_3a.conv.weight: Trainable\n",
      "inception.Mixed_7b.branch3x3dbl_3a.bn.weight: Trainable\n",
      "inception.Mixed_7b.branch3x3dbl_3a.bn.bias: Trainable\n",
      "inception.Mixed_7b.branch3x3dbl_3b.conv.weight: Trainable\n",
      "inception.Mixed_7b.branch3x3dbl_3b.bn.weight: Trainable\n",
      "inception.Mixed_7b.branch3x3dbl_3b.bn.bias: Trainable\n",
      "inception.Mixed_7b.branch_pool.conv.weight: Trainable\n",
      "inception.Mixed_7b.branch_pool.bn.weight: Trainable\n",
      "inception.Mixed_7b.branch_pool.bn.bias: Trainable\n",
      "inception.Mixed_7c.branch1x1.conv.weight: Trainable\n",
      "inception.Mixed_7c.branch1x1.bn.weight: Trainable\n",
      "inception.Mixed_7c.branch1x1.bn.bias: Trainable\n",
      "inception.Mixed_7c.branch3x3_1.conv.weight: Trainable\n",
      "inception.Mixed_7c.branch3x3_1.bn.weight: Trainable\n",
      "inception.Mixed_7c.branch3x3_1.bn.bias: Trainable\n",
      "inception.Mixed_7c.branch3x3_2a.conv.weight: Trainable\n",
      "inception.Mixed_7c.branch3x3_2a.bn.weight: Trainable\n",
      "inception.Mixed_7c.branch3x3_2a.bn.bias: Trainable\n",
      "inception.Mixed_7c.branch3x3_2b.conv.weight: Trainable\n",
      "inception.Mixed_7c.branch3x3_2b.bn.weight: Trainable\n",
      "inception.Mixed_7c.branch3x3_2b.bn.bias: Trainable\n",
      "inception.Mixed_7c.branch3x3dbl_1.conv.weight: Trainable\n",
      "inception.Mixed_7c.branch3x3dbl_1.bn.weight: Trainable\n",
      "inception.Mixed_7c.branch3x3dbl_1.bn.bias: Trainable\n",
      "inception.Mixed_7c.branch3x3dbl_2.conv.weight: Trainable\n",
      "inception.Mixed_7c.branch3x3dbl_2.bn.weight: Trainable\n",
      "inception.Mixed_7c.branch3x3dbl_2.bn.bias: Trainable\n",
      "inception.Mixed_7c.branch3x3dbl_3a.conv.weight: Trainable\n",
      "inception.Mixed_7c.branch3x3dbl_3a.bn.weight: Trainable\n",
      "inception.Mixed_7c.branch3x3dbl_3a.bn.bias: Trainable\n",
      "inception.Mixed_7c.branch3x3dbl_3b.conv.weight: Trainable\n",
      "inception.Mixed_7c.branch3x3dbl_3b.bn.weight: Trainable\n",
      "inception.Mixed_7c.branch3x3dbl_3b.bn.bias: Trainable\n",
      "inception.Mixed_7c.branch_pool.conv.weight: Trainable\n",
      "inception.Mixed_7c.branch_pool.bn.weight: Trainable\n",
      "inception.Mixed_7c.branch_pool.bn.bias: Trainable\n",
      "batch_norm.weight: Trainable\n",
      "batch_norm.bias: Trainable\n",
      "fc.weight: Trainable\n",
      "fc.bias: Trainable\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "from torchvision.models import inception_v3\n",
    "\n",
    "\n",
    "torch.set_printoptions(threshold=10_000)  # Adjust threshold if needed\n",
    "print(model)\n",
    "# summary(model, input_size=[(1, 3, 224, 224), (1, 3, 299, 299)])\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {'Trainable' if param.requires_grad else 'Frozen'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|| 42/42 [00:52<00:00,  1.25s/it, accuracy=90.5, loss=0.344] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5: Train Loss = 0.2318, Train Accuracy = 90.47%\n",
      "Validation Loss = 0.2295, Validation Accuracy = 91.62%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|| 42/42 [00:51<00:00,  1.22s/it, accuracy=97.1, loss=0.0668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/5: Train Loss = 0.0853, Train Accuracy = 97.15%\n",
      "Validation Loss = 0.2236, Validation Accuracy = 93.71%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|| 42/42 [00:51<00:00,  1.23s/it, accuracy=99.4, loss=0.0937] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/5: Train Loss = 0.0209, Train Accuracy = 99.40%\n",
      "Validation Loss = 0.2276, Validation Accuracy = 94.01%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|| 42/42 [00:51<00:00,  1.22s/it, accuracy=99.5, loss=0.00181] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/5: Train Loss = 0.0166, Train Accuracy = 99.55%\n",
      "Validation Loss = 0.2029, Validation Accuracy = 94.31%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|| 42/42 [00:54<00:00,  1.29s/it, accuracy=99.5, loss=0.00728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/5: Train Loss = 0.0156, Train Accuracy = 99.47%\n",
      "Validation Loss = 0.2540, Validation Accuracy = 93.71%\n",
      "\n",
      "Training complete\n",
      "**Fine-Tuned Model Evaluation**\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.93      0.94       180\n",
      "         1.0       0.92      0.94      0.93       154\n",
      "\n",
      "    accuracy                           0.94       334\n",
      "   macro avg       0.94      0.94      0.94       334\n",
      "weighted avg       0.94      0.94      0.94       334\n",
      "\n",
      "Accuracy Score: 0.937125748502994\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fine-tune the model\n",
    "train_model(model, train_loader_resnet, train_loader_inception, val_loader_resnet, val_loader_inception)\n",
    "# **Evaluation after Fine-Tuning**\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (resnet_batch, inception_batch) in zip(val_loader_resnet, val_loader_inception):\n",
    "        x_resnet, y = resnet_batch\n",
    "        x_inception, _ = inception_batch\n",
    "        x_resnet, x_inception, y = x_resnet.to(device), x_inception.to(device), y.to(device).float()\n",
    "        outputs = model(x_resnet, x_inception).squeeze()\n",
    "        preds = (outputs > 0.5).cpu().numpy()\n",
    "        y_true.extend(y.cpu().numpy())\n",
    "        y_pred.extend(preds)\n",
    "\n",
    "print(\"**Fine-Tuned Model Evaluation**\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(f\"Accuracy Score: {accuracy_score(y_true, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the PyTorch model\n",
    "torch.save(model, \"Saved models/resnet_inception_fullmodel_cropped.pth\")\n",
    "scripted_model = torch.jit.script(model)  # Convert model to TorchScript\n",
    "scripted_model.save(\"Saved models/resnet_inception_fullmodel_cropped.pt\")\n",
    "\n",
    "# STATE DICT SAVED, WEIGHS ONLY\n",
    "# from fusion_model import FusionModel\n",
    "# torch.save(model.state_dict(), 'Saved models/resnet_inception_fullmodel_cropped.pth')\n",
    "# state_dict = torch.load('Saved models/resnet_inception_fullmodel_cropped.pth', map_location=device)\n",
    "# fussionmodel = FusionModel()\n",
    "# fussionmodel.load_state_dict(state_dict)\n",
    "# fussionmodel.to(device)\n",
    "# fussionmodel.eval()\n",
    "\n",
    "# FULL MODEL\n",
    "# from fusion_model import FusionModel\n",
    "\n",
    "# fussionmodel = torch.load('Saved models/resnet_inception_fullmodel_cropped.pth')\n",
    "# fussionmodel.to(device)\n",
    "# fussionmodel.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Appropriate', 'Inappropriate']\n",
      "Predicted Class for ./DARKNET/test.jpg: Appropriate\n",
      "Predicted Class for ./DARKNET/test1.jpg: Appropriate\n",
      "Predicted Class for ./DARKNET/test2.jpg: Inappropriate\n",
      "Predicted Class for ./DARKNET/test3.jpg: Inappropriate\n",
      "Predicted Class for ./DARKNET/test4.jpg: Inappropriate\n",
      "Predicted Class for ./DARKNET/test5.jpg: Inappropriate\n",
      "Predicted Class for ./DARKNET/test6.jpg: Inappropriate\n",
      "Predicted Class for ./DARKNET/test7.jpg: Appropriate\n",
      "Predicted Class for ./DARKNET/zeb.jpg: Appropriate\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "class_names = ['Appropriate', 'Inappropriate']\n",
    "\n",
    "# Suppress specific UserWarning for InceptionV3\n",
    "warnings.filterwarnings(\"ignore\", message=\"Scripted Inception3 always returns Inception3 Tuple\")\n",
    "\n",
    "# Load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# fussionmodel = torch.load('Saved models/resnet_inception_fullmodel_cropped.pth')\n",
    "# fussionmodel.to(device)\n",
    "# fussionmodel.eval() \n",
    "\n",
    "fuse = torch.jit.load(\"Saved models/resnet_inception_fullmodel_cropped.pt\")\n",
    "fuse.to(device)\n",
    "fuse.eval()\n",
    "\n",
    "# # Perform the forward pass during prediction\n",
    "# with torch.no_grad():  # Disable gradient tracking during inference for efficiency\n",
    "#     x_resnet = x_resnet.to(device)  # Make sure the input is on the correct device (CPU/GPU)\n",
    "#     x_inception = x_inception.to(device)\n",
    "#     output = model(x_resnet, x_inception)\n",
    "# # Image preprocessing functions equivalent to TensorFlow preprocessing\n",
    "\n",
    "resnet_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ResNet normalization\n",
    "])\n",
    "\n",
    "inception_transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Inception normalization\n",
    "])\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img_resnet = Image.open(img_path).convert('RGB')\n",
    "    img_inception = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    # Apply transformations for ResNet and Inception\n",
    "    img_tensor_resnet = resnet_transform(img_resnet).unsqueeze(0)  # Add batch dimension\n",
    "    img_tensor_inception = inception_transform(img_inception).unsqueeze(0)\n",
    "    \n",
    "    return img_tensor_resnet, img_tensor_inception\n",
    "\n",
    "def predict_image(img_path):\n",
    "    img_tensor_resnet, img_tensor_inception = preprocess_image(img_path)\n",
    "    img_tensor_resnet, img_tensor_inception = img_tensor_resnet.to(device), img_tensor_inception.to(device)\n",
    "\n",
    "    # Forward pass to get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = fuse(img_tensor_resnet, img_tensor_inception)\n",
    "        predicted_value = outputs.item()  # Extract scalar prediction\n",
    "\n",
    "    # Return class name based on threshold\n",
    "    predicted_class = class_names[int(predicted_value > 0.5)]\n",
    "    return predicted_class\n",
    "\n",
    "print(class_names)\n",
    "# Test prediction\n",
    "new_img_path = './DARKNET/test.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test1.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test2.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test3.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test4.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test5.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test6.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test7.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/zeb.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_from_history(history):\n",
    "    # Extract data from history\n",
    "    epochs = history[\"epoch\"]\n",
    "    train_losses = history[\"train_loss\"]\n",
    "    val_losses = history[\"val_loss\"]\n",
    "    train_accuracies = history[\"train_accuracy\"]\n",
    "    val_accuracies = history[\"val_accuracy\"]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\", color=\"blue\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\", color=\"red\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Train Accuracy\", color=\"blue\")\n",
    "    plt.plot(epochs, val_accuracies, label=\"Validation Accuracy\", color=\"red\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('training_history.png')  # PNG image\n",
    "    plt.show()\n",
    "    \n",
    "def plot_roc_curve(metrics):\n",
    "    auc_score = metrics['auc_roc'][0]\n",
    "  \n",
    "    roc_fpr = list(map(float, metrics['roc_curve_fpr'][0].split(', ')))\n",
    "    roc_tpr = list(map(float, metrics['roc_curve_tpr'][0].split(', ')))\n",
    "    # Plot ROC Curve\n",
    "    plt.figure()\n",
    "    plt.plot(roc_fpr, roc_tpr, label=f\"AUC = {auc_score:.4f}\")\n",
    "    plt.plot([0, 1], [0, 1], 'r--')  # Diagonal line\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_from_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(history)\n",
    "\n",
    "df.to_csv('Saved models/training_history2.csv', index=False)\n",
    "history_df = pd.read_csv('Saved models/training_history2.csv')\n",
    "\n",
    "history = {\n",
    "    'epoch': history_df['epoch'].tolist(),\n",
    "    'train_loss': history_df['train_loss'].tolist(),\n",
    "    'val_loss': history_df['val_loss'].tolist(),\n",
    "    'train_accuracy': history_df['train_accuracy'].tolist(),\n",
    "    'val_accuracy': history_df['val_accuracy'].tolist(),\n",
    "}\n",
    "\n",
    "plot_from_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Flatten data for CSV saving\n",
    "metrics_flat = {\n",
    "    \"class_names\": metrics[\"class_names\"],\n",
    "    \"fpr\": metrics[\"fpr\"],\n",
    "    \"fnr\": metrics[\"fnr\"],\n",
    "    \"auc_roc\": metrics[\"auc_roc\"],\n",
    "    \"roc_curve_fpr\": [', '.join(map(str, fpr_vals)) for fpr_vals in metrics[\"roc_curve_fpr\"]],\n",
    "    \"roc_curve_tpr\": [', '.join(map(str, tpr_vals)) for tpr_vals in metrics[\"roc_curve_tpr\"]],\n",
    "    \"confusion_matrix\": [', '.join(map(str, cm)) for cm in metrics[\"confusion_matrix\"]]\n",
    "}\n",
    "plot_roc_curve(metrics_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Save metrics to CSV\n",
    "metrics_df = pd.DataFrame(metrics_flat)\n",
    "metrics_df.to_csv('Saved models/first_training_metrics.csv', index=False)\n",
    "\n",
    "\n",
    "# Load the CSV file containing the final metrics\n",
    "metrics_df = pd.read_csv('Saved models/metrics_for_plotting.csv')\n",
    "\n",
    "# Extract individual metrics\n",
    "auc_score = metrics_df['auc_roc'][0]\n",
    "fpr = metrics_df['fpr'][0]\n",
    "fnr = metrics_df['fnr'][0]\n",
    "\n",
    "# Extract confusion matrix (flattened form)\n",
    "conf_matrix = list(map(int, metrics_df['confusion_matrix'][0].split(', ')))  # Convert string to list of integers\n",
    "tn, fp, fn, tp = conf_matrix\n",
    "\n",
    "# Extract ROC curve data (flattened form)\n",
    "roc_fpr = list(map(float, metrics_df['roc_curve_fpr'][0].split(', ')))\n",
    "roc_tpr = list(map(float, metrics_df['roc_curve_tpr'][0].split(', ')))\n",
    "# Confusion Matrix heatmap\n",
    "conf_matrix = np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Predicted: 0', 'Predicted: 1'], yticklabels=['Actual: 0', 'Actual: 1'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Plot FPR and FNR\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(['FPR', 'FNR'], [fpr, fnr], color=['red', 'blue'])\n",
    "plt.ylabel('Rate')\n",
    "plt.title('Final FPR and FNR')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "# Plot ROC Curve\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(roc_fpr, roc_tpr, color='darkblue', label='ROC Curve')\n",
    "# plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "# plt.xlabel('False Positive Rate (FPR)')\n",
    "# plt.ylabel('True Positive Rate (TPR)')\n",
    "# plt.title('ROC Curve')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(roc_fpr, roc_tpr, label=f\"AUC = {auc_score:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], 'r--')  # Diagonal line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResnetxSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory found: ./cropped_images\n",
      "Contents: ['Appropriate', 'Inappropriate']\n",
      "['Appropriate', 'Inappropriate']\n",
      "Total images found: 1666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|| 1666/1666 [00:19<00:00, 87.66it/s] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "# Dataset path\n",
    "dataset_path = \"./cropped_images\"\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"Directory not found: {dataset_path}\")\n",
    "else:\n",
    "    print(f\"Directory found: {dataset_path}\")\n",
    "    print(\"Contents:\", os.listdir(dataset_path))\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset and preprocess\n",
    "image_paths = []\n",
    "labels = []\n",
    "class_names = sorted(os.listdir(dataset_path))  # Ensure consistent label ordering\n",
    "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n",
    "print(class_names)\n",
    "for class_name in class_names:\n",
    "    class_folder = os.path.join(dataset_path, class_name)\n",
    "    if os.path.isdir(class_folder):\n",
    "        for img_path in glob.glob(f\"{class_folder}/*.jpg\"):\n",
    "            image_paths.append(img_path)\n",
    "            labels.append(class_to_idx[class_name])\n",
    "\n",
    "print(\"Total images found:\", len(image_paths))\n",
    "\n",
    "# Load and preprocess images\n",
    "images = []\n",
    "\n",
    "for img_path in tqdm(image_paths, desc=\"Loading images\"):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = transform(img)\n",
    "    images.append(img_tensor)\n",
    "\n",
    "images = torch.stack(images)  # Shape: [num_samples, 3, 224, 224]\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Load ResNet50 pre-trained model (feature extractor)\n",
    "resnet = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "resnet.fc = nn.Identity()  # Remove the classification layer\n",
    "resnet.eval()  # Set to evaluation mode\n",
    "\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in list(resnet.parameters())[-75:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Move model and data to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet = resnet.to(device)\n",
    "images = images.to(device)\n",
    "\n",
    "class ResNet50FeatureExtractor(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(ResNet50FeatureExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])  # Keep layers up to the last conv layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # Extract features\n",
    "        return x\n",
    "    \n",
    "# WORKS EXACTLY THE SAME WITH NN.IDENTITY IF nn.Sequential(*list(base_model.children())[:-1]) , gap will be retained\n",
    "base_model_resnet = ResNet50FeatureExtractor(models.resnet50(weights=ResNet50_Weights.DEFAULT))\n",
    "base_model_resnet.eval()\n",
    "base_model_resnet = base_model_resnet.to(device)\n",
    "\n",
    "# Extract features\n",
    "\n",
    "# torch.set_printoptions(threshold=10_000)  # Adjust threshold if needed\n",
    "# print(base_model_resnet)\n",
    "# torch.set_printoptions(threshold=10_000)  # Adjust threshold if needed\n",
    "# print(resnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting features using ResNet50...\")\n",
    "features = []\n",
    "with torch.no_grad():\n",
    "    for img_tensor in tqdm(images, desc=\"Extracting features\"):\n",
    "        img_tensor = img_tensor.unsqueeze(0)  # Add batch dimension\n",
    "        # print(\"Input to model:\", images.shape)  #\n",
    "        feature = resnet(img_tensor)  # Extract features\n",
    "        # print(\"Output of model:\", feature.shape)  #\n",
    "        feature = feature.squeeze().cpu().numpy() \n",
    "        # print(\"Output of model:\", feature.shape)  #\n",
    "        features.append(feature)\n",
    "\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Flattened Feature Shape:\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels.numpy(), test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM Classifier\n",
    "print(\"Training SVM classifier...\")\n",
    "svm_classifier = svm.SVC(kernel='linear', C=1, probability=True, random_state=42)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the SVM classifier\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "y_prob = svm_classifier.predict_proba(X_test)[:, 1]  # Now we use predict_proba()\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred, class_names):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "def fromfile_plot_confusion_matrix(cm, class_names):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "def plot_feature_importance(svm_classifier, feature_names):\n",
    "    if hasattr(svm_classifier, \"coef_\"):\n",
    "        importance = np.abs(svm_classifier.coef_).flatten()\n",
    "        sorted_idx = np.argsort(importance)[::-1]\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(range(len(importance)), importance[sorted_idx], align=\"center\")\n",
    "        plt.xticks(range(len(importance)), np.array(feature_names)[sorted_idx], rotation=90)\n",
    "        plt.xlabel(\"Feature\")\n",
    "        plt.ylabel(\"Importance\")\n",
    "        plt.title(\"Feature Importance in Linear SVM\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Feature importance is only available for linear SVMs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score, roc_auc_score,\n",
    "    confusion_matrix, roc_curve\n",
    ")\n",
    "plot_confusion_matrix(y_test, y_pred, class_names)\n",
    "# Extract TN, FP, FN, TP\n",
    "\n",
    "# Confusion Matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(f\"TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\")  # Explicit print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute FPR and FNR\n",
    "fpr = fp / (fp + tn)\n",
    "fnr = fn / (fn + tp)\n",
    "\n",
    "print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fnr:.4f}\")\n",
    "\n",
    "# Compute AUC-ROC using predict_proba()\n",
    "auc_score = roc_auc_score(y_test, y_prob)\n",
    "print(f\"AUC-ROC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr_vals, tpr_vals, _ = roc_curve(y_test, y_prob)\n",
    "plt.figure()\n",
    "plt.plot(fpr_vals, tpr_vals, label=f\"AUC = {auc_score:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], 'r--')  # Diagonal line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "metrics = {\n",
    "    \"class_names\": class_names,\n",
    "    \"confusion_matrix\": confusion_matrix(y_test, y_pred),\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"fpr\": fpr,\n",
    "    \"fnr\": fnr,\n",
    "    \"auc_roc\": auc_score,\n",
    "    \"roc_curve\": (fpr_vals, tpr_vals)  # Save ROC data for later plotting\n",
    "}\n",
    "\n",
    "# Save as a NumPy file\n",
    "np.savez(\"svm_metrics.npz\", **metrics)\n",
    "\n",
    "# Save using Pickle (optional, for structured data)\n",
    "with open(\"svm_metrics.pkl\", \"wb\") as f:\n",
    "    pickle.dump(metrics, f)\n",
    "\n",
    "print(\"Metrics saved successfully.\")\n",
    "# Load metrics from NumPy file\n",
    "loaded_data = np.load(\"svm_metrics.npz\", allow_pickle=True)\n",
    "\n",
    "# Extract metrics\n",
    "conf_matrix = loaded_data[\"confusion_matrix\"]\n",
    "accuracy = loaded_data[\"accuracy\"]\n",
    "fpr = loaded_data[\"fpr\"]\n",
    "fnr = loaded_data[\"fnr\"]\n",
    "auc_score = loaded_data[\"auc_roc\"]\n",
    "fpr_vals, tpr_vals = loaded_data[\"roc_curve\"]\n",
    "\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fromfile_plot_confusion_matrix(conf_matrix, class_names)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fnr:.4f}\")\n",
    "print(f\"AUC-ROC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure()\n",
    "plt.plot(fpr_vals, tpr_vals, label=f\"AUC = {auc_score:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], 'r--')  # Diagonal line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight: Frozen\n",
      "bn1.weight: Frozen\n",
      "bn1.bias: Frozen\n",
      "layer1.0.conv1.weight: Frozen\n",
      "layer1.0.bn1.weight: Frozen\n",
      "layer1.0.bn1.bias: Frozen\n",
      "layer1.0.conv2.weight: Frozen\n",
      "layer1.0.bn2.weight: Frozen\n",
      "layer1.0.bn2.bias: Frozen\n",
      "layer1.0.conv3.weight: Frozen\n",
      "layer1.0.bn3.weight: Frozen\n",
      "layer1.0.bn3.bias: Frozen\n",
      "layer1.0.downsample.0.weight: Frozen\n",
      "layer1.0.downsample.1.weight: Frozen\n",
      "layer1.0.downsample.1.bias: Frozen\n",
      "layer1.1.conv1.weight: Frozen\n",
      "layer1.1.bn1.weight: Frozen\n",
      "layer1.1.bn1.bias: Frozen\n",
      "layer1.1.conv2.weight: Frozen\n",
      "layer1.1.bn2.weight: Frozen\n",
      "layer1.1.bn2.bias: Frozen\n",
      "layer1.1.conv3.weight: Frozen\n",
      "layer1.1.bn3.weight: Frozen\n",
      "layer1.1.bn3.bias: Frozen\n",
      "layer1.2.conv1.weight: Frozen\n",
      "layer1.2.bn1.weight: Frozen\n",
      "layer1.2.bn1.bias: Frozen\n",
      "layer1.2.conv2.weight: Frozen\n",
      "layer1.2.bn2.weight: Frozen\n",
      "layer1.2.bn2.bias: Frozen\n",
      "layer1.2.conv3.weight: Frozen\n",
      "layer1.2.bn3.weight: Frozen\n",
      "layer1.2.bn3.bias: Frozen\n",
      "layer2.0.conv1.weight: Frozen\n",
      "layer2.0.bn1.weight: Frozen\n",
      "layer2.0.bn1.bias: Frozen\n",
      "layer2.0.conv2.weight: Frozen\n",
      "layer2.0.bn2.weight: Frozen\n",
      "layer2.0.bn2.bias: Frozen\n",
      "layer2.0.conv3.weight: Frozen\n",
      "layer2.0.bn3.weight: Frozen\n",
      "layer2.0.bn3.bias: Frozen\n",
      "layer2.0.downsample.0.weight: Frozen\n",
      "layer2.0.downsample.1.weight: Frozen\n",
      "layer2.0.downsample.1.bias: Frozen\n",
      "layer2.1.conv1.weight: Frozen\n",
      "layer2.1.bn1.weight: Frozen\n",
      "layer2.1.bn1.bias: Frozen\n",
      "layer2.1.conv2.weight: Frozen\n",
      "layer2.1.bn2.weight: Frozen\n",
      "layer2.1.bn2.bias: Frozen\n",
      "layer2.1.conv3.weight: Frozen\n",
      "layer2.1.bn3.weight: Frozen\n",
      "layer2.1.bn3.bias: Frozen\n",
      "layer2.2.conv1.weight: Frozen\n",
      "layer2.2.bn1.weight: Frozen\n",
      "layer2.2.bn1.bias: Frozen\n",
      "layer2.2.conv2.weight: Frozen\n",
      "layer2.2.bn2.weight: Frozen\n",
      "layer2.2.bn2.bias: Frozen\n",
      "layer2.2.conv3.weight: Frozen\n",
      "layer2.2.bn3.weight: Frozen\n",
      "layer2.2.bn3.bias: Frozen\n",
      "layer2.3.conv1.weight: Frozen\n",
      "layer2.3.bn1.weight: Frozen\n",
      "layer2.3.bn1.bias: Frozen\n",
      "layer2.3.conv2.weight: Frozen\n",
      "layer2.3.bn2.weight: Frozen\n",
      "layer2.3.bn2.bias: Frozen\n",
      "layer2.3.conv3.weight: Frozen\n",
      "layer2.3.bn3.weight: Frozen\n",
      "layer2.3.bn3.bias: Frozen\n",
      "layer3.0.conv1.weight: Frozen\n",
      "layer3.0.bn1.weight: Frozen\n",
      "layer3.0.bn1.bias: Frozen\n",
      "layer3.0.conv2.weight: Frozen\n",
      "layer3.0.bn2.weight: Frozen\n",
      "layer3.0.bn2.bias: Frozen\n",
      "layer3.0.conv3.weight: Frozen\n",
      "layer3.0.bn3.weight: Frozen\n",
      "layer3.0.bn3.bias: Frozen\n",
      "layer3.0.downsample.0.weight: Frozen\n",
      "layer3.0.downsample.1.weight: Frozen\n",
      "layer3.0.downsample.1.bias: Frozen\n",
      "layer3.1.conv1.weight: Trainable\n",
      "layer3.1.bn1.weight: Trainable\n",
      "layer3.1.bn1.bias: Trainable\n",
      "layer3.1.conv2.weight: Trainable\n",
      "layer3.1.bn2.weight: Trainable\n",
      "layer3.1.bn2.bias: Trainable\n",
      "layer3.1.conv3.weight: Trainable\n",
      "layer3.1.bn3.weight: Trainable\n",
      "layer3.1.bn3.bias: Trainable\n",
      "layer3.2.conv1.weight: Trainable\n",
      "layer3.2.bn1.weight: Trainable\n",
      "layer3.2.bn1.bias: Trainable\n",
      "layer3.2.conv2.weight: Trainable\n",
      "layer3.2.bn2.weight: Trainable\n",
      "layer3.2.bn2.bias: Trainable\n",
      "layer3.2.conv3.weight: Trainable\n",
      "layer3.2.bn3.weight: Trainable\n",
      "layer3.2.bn3.bias: Trainable\n",
      "layer3.3.conv1.weight: Trainable\n",
      "layer3.3.bn1.weight: Trainable\n",
      "layer3.3.bn1.bias: Trainable\n",
      "layer3.3.conv2.weight: Trainable\n",
      "layer3.3.bn2.weight: Trainable\n",
      "layer3.3.bn2.bias: Trainable\n",
      "layer3.3.conv3.weight: Trainable\n",
      "layer3.3.bn3.weight: Trainable\n",
      "layer3.3.bn3.bias: Trainable\n",
      "layer3.4.conv1.weight: Trainable\n",
      "layer3.4.bn1.weight: Trainable\n",
      "layer3.4.bn1.bias: Trainable\n",
      "layer3.4.conv2.weight: Trainable\n",
      "layer3.4.bn2.weight: Trainable\n",
      "layer3.4.bn2.bias: Trainable\n",
      "layer3.4.conv3.weight: Trainable\n",
      "layer3.4.bn3.weight: Trainable\n",
      "layer3.4.bn3.bias: Trainable\n",
      "layer3.5.conv1.weight: Trainable\n",
      "layer3.5.bn1.weight: Trainable\n",
      "layer3.5.bn1.bias: Trainable\n",
      "layer3.5.conv2.weight: Trainable\n",
      "layer3.5.bn2.weight: Trainable\n",
      "layer3.5.bn2.bias: Trainable\n",
      "layer3.5.conv3.weight: Trainable\n",
      "layer3.5.bn3.weight: Trainable\n",
      "layer3.5.bn3.bias: Trainable\n",
      "layer4.0.conv1.weight: Trainable\n",
      "layer4.0.bn1.weight: Trainable\n",
      "layer4.0.bn1.bias: Trainable\n",
      "layer4.0.conv2.weight: Trainable\n",
      "layer4.0.bn2.weight: Trainable\n",
      "layer4.0.bn2.bias: Trainable\n",
      "layer4.0.conv3.weight: Trainable\n",
      "layer4.0.bn3.weight: Trainable\n",
      "layer4.0.bn3.bias: Trainable\n",
      "layer4.0.downsample.0.weight: Trainable\n",
      "layer4.0.downsample.1.weight: Trainable\n",
      "layer4.0.downsample.1.bias: Trainable\n",
      "layer4.1.conv1.weight: Trainable\n",
      "layer4.1.bn1.weight: Trainable\n",
      "layer4.1.bn1.bias: Trainable\n",
      "layer4.1.conv2.weight: Trainable\n",
      "layer4.1.bn2.weight: Trainable\n",
      "layer4.1.bn2.bias: Trainable\n",
      "layer4.1.conv3.weight: Trainable\n",
      "layer4.1.bn3.weight: Trainable\n",
      "layer4.1.bn3.bias: Trainable\n",
      "layer4.2.conv1.weight: Trainable\n",
      "layer4.2.bn1.weight: Trainable\n",
      "layer4.2.bn1.bias: Trainable\n",
      "layer4.2.conv2.weight: Trainable\n",
      "layer4.2.bn2.weight: Trainable\n",
      "layer4.2.bn2.bias: Trainable\n",
      "layer4.2.conv3.weight: Trainable\n",
      "layer4.2.bn3.weight: Trainable\n",
      "layer4.2.bn3.bias: Trainable\n"
     ]
    }
   ],
   "source": [
    "for name, param in resnet.named_parameters():\n",
    "    print(f\"{name}: {'Trainable' if param.requires_grad else 'Frozen'}\")\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM Classifier\n",
    "print(\"Training SVM classifier...\")\n",
    "svm_classifier = svm.SVC(kernel='linear', C=1, probability=True, random_state=42)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the SVM classifier\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "y_prob = svm_classifier.predict_proba(X_test)[:, 1]  # Now we use predict_proba()\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the class of a new image\n",
    "def predict_image(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    resnet.eval()  # Ensure the model is in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Extract features\n",
    "        feature_map = resnet(img_tensor)  \n",
    "        # Apply global average pooling\n",
    "\n",
    "        flattened_features = feature_map.squeeze().cpu().numpy().reshape(1, -1)  \n",
    "    prediction = svm_classifier.predict(flattened_features)  # Predict using the trained SVM classifier\n",
    "    return class_names[prediction[0]]\n",
    "\n",
    "# Test the new image prediction\n",
    "new_img_path = './DARKNET/test.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test1.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test2.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test3.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test4.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test5.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test6.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test7.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/zeb.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(resnet, \"Saved models/svm/resnetextractor.pth\")\n",
    "\n",
    "import joblib\n",
    "\n",
    "svm_save_path = 'Saved models/svm'\n",
    "svm_model_path = os.path.join(svm_save_path, 'torch_svm.pkl')\n",
    "joblib.dump(svm_classifier, svm_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_29420\\1626553149.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  load_ex = torch.load('Saved models/svm/resnetextractor.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extractor loaded.\n",
      "SVM classifier loaded.\n",
      "['Appropriate', 'Inappropriate']\n",
      "Predicted Class for ./DARKNET/test.jpg: Appropriate\n",
      "Predicted Class for ./DARKNET/test1.jpg: Appropriate\n",
      "Predicted Class for ./DARKNET/test2.jpg: Inappropriate\n",
      "Predicted Class for ./DARKNET/test3.jpg: Inappropriate\n",
      "Predicted Class for ./DARKNET/test4.jpg: Inappropriate\n",
      "Predicted Class for ./DARKNET/test5.jpg: Inappropriate\n",
      "Predicted Class for ./DARKNET/test6.jpg: Inappropriate\n",
      "Predicted Class for ./DARKNET/test7.jpg: Appropriate\n",
      "Predicted Class for ./DARKNET/zeb.jpg: Inappropriate\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class_names = ['Appropriate', 'Inappropriate']\n",
    "\n",
    "# Load the ResNet feature extractor\n",
    "load_ex = torch.load('Saved models/svm/resnetextractor.pth')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_ex.to(device)\n",
    "print(\"Feature extractor loaded.\")\n",
    "\n",
    "# Load the SVM model\n",
    "svm_model_path = os.path.join('Saved models/svm', 'torch_svm.pkl')\n",
    "load_svm = joblib.load(svm_model_path)\n",
    "print(\"SVM classifier loaded.\")\n",
    "resnet_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ResNet normalization\n",
    "])\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img_resnet = Image.open(img_path).convert('RGB')\n",
    "    img_tensor_resnet = resnet_transform(img_resnet).unsqueeze(0).to(device)\n",
    "\n",
    " \n",
    "    \n",
    "    return img_tensor_resnet\n",
    "\n",
    "def predict_image(img_path):\n",
    "    \"\"\"\n",
    "    Predict the class of an image using the saved ResNet-based feature extractor and SVM classifier.\n",
    "\n",
    "    Args:\n",
    "        img_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        str: Predicted class name.\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = resnet_transform(img).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    load_ex.eval()  # Ensure the model is in evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        feature_map = load_ex(img_tensor)  \n",
    "        flattened_features = feature_map.squeeze().cpu().numpy().reshape(1, -1)  \n",
    "    prediction = load_svm.predict(flattened_features)  # Predict using the trained SVM classifier\n",
    "    return class_names[prediction[0]]\n",
    "\n",
    "\n",
    "print(class_names)\n",
    "# Test prediction\n",
    "new_img_path = './DARKNET/test.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test1.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test2.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test3.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test4.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test5.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test6.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/test7.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")\n",
    "new_img_path = './DARKNET/zeb.jpg'\n",
    "predicted_class = predict_image(new_img_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Predict the class of an uploaded image\n",
    "uploaded_image_path = './DARKNET/test7.jpg'\n",
    "predicted_class = predict_imageSVM(uploaded_image_path)\n",
    "print(f\"Predicted Class for {new_img_path}: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING THE CROPPED PERSONS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: ./test\\Appropriate\\00000018.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000018_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000026.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000026_person_1_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000030.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000030_person_7_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000074.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000074_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000084.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000084_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000088.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000088_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000100.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000100_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000102.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000102_person_2_confidence_0.61.jpg\n",
      "Processing image: ./test\\Appropriate\\00000107.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000107_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000115.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000115_person_3_confidence_0.98.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000115_person_5_confidence_0.98.jpg\n",
      "Processing image: ./test\\Appropriate\\00000130.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000130_person_3_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000130_person_7_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000130_person_15_confidence_0.99.jpg\n",
      "Processing image: ./test\\Appropriate\\00000160.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000160_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000166.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000166_person_2_confidence_0.97.jpg\n",
      "Processing image: ./test\\Appropriate\\00000173.jpeg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000173_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000222.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000222_person_22_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000222_person_13_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000222_person_4_confidence_0.90.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000222_person_9_confidence_0.85.jpg\n",
      "Processing image: ./test\\Appropriate\\00000224.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000224_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000225.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000225_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000237.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000237_person_3_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000237_person_7_confidence_0.85.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000237_person_8_confidence_0.62.jpg\n",
      "Processing image: ./test\\Appropriate\\00000239.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_8_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_12_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_4_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_24_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_9_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_54_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_20_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_36_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_57_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_47_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_76_confidence_0.98.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_69_confidence_0.98.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_38_confidence_0.97.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_53_confidence_0.97.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_13_confidence_0.97.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_33_confidence_0.96.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_72_confidence_0.95.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_14_confidence_0.87.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_18_confidence_0.85.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_67_confidence_0.84.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_68_confidence_0.82.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_17_confidence_0.82.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_39_confidence_0.74.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_34_confidence_0.68.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000239_person_19_confidence_0.59.jpg\n",
      "Processing image: ./test\\Appropriate\\00000240.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000240_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000240.png\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000240_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000243.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000243_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000248.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000248_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000252.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000252_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000270.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000270_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000274.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000274_person_5_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000292.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000292_person_5_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000299.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000299_person_3_confidence_0.99.jpg\n",
      "Processing image: ./test\\Appropriate\\00000304.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000304_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000306.png\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000306_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000323.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000323_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000325.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000325_person_2_confidence_0.97.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000325_person_18_confidence_0.92.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000325_person_4_confidence_0.91.jpg\n",
      "Processing image: ./test\\Appropriate\\00000326.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000326_person_1_confidence_0.93.jpg\n",
      "Processing image: ./test\\Appropriate\\00000328.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000328_person_4_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000328_person_7_confidence_0.98.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000328_person_13_confidence_0.98.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000328_person_11_confidence_0.93.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000328_person_17_confidence_0.53.jpg\n",
      "Processing image: ./test\\Appropriate\\00000340.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000340_person_9_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000340_person_6_confidence_0.98.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000340_person_2_confidence_0.98.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000340_person_10_confidence_0.83.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000340_person_12_confidence_0.77.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000340_person_1_confidence_0.62.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000340_person_14_confidence_0.54.jpg\n",
      "Processing image: ./test\\Appropriate\\00000351.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000351_person_5_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000368.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000368_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000378.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000378_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000408.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000408_person_2_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000408_person_4_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000408_person_7_confidence_0.93.jpg\n",
      "Processing image: ./test\\Appropriate\\00000416.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000416_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000435.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000435_person_2_confidence_0.89.jpg\n",
      "Processing image: ./test\\Appropriate\\00000443.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000443_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000458.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000458_person_4_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000458_person_3_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000458_person_8_confidence_0.50.jpg\n",
      "Processing image: ./test\\Appropriate\\00000472.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000472_person_3_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000472_person_6_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000472_person_9_confidence_0.92.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000472_person_12_confidence_0.89.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000472_person_10_confidence_0.86.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000472_person_14_confidence_0.74.jpg\n",
      "Processing image: ./test\\Appropriate\\00000484.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000484_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000506.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000506_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000509.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000509_person_1_confidence_0.60.jpg\n",
      "Processing image: ./test\\Appropriate\\00000529.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000529_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000554.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000554_person_2_confidence_0.98.jpg\n",
      "Processing image: ./test\\Appropriate\\00000604.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000604_person_5_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000618.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000618_person_2_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000618_person_9_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000632.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000632_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000653.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000653_person_2_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000653_person_12_confidence_0.98.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000653_person_6_confidence_0.95.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000653_person_7_confidence_0.94.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000653_person_3_confidence_0.55.jpg\n",
      "Processing image: ./test\\Appropriate\\00000669.png\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000669_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000703.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000703_person_5_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000770.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000770_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000796.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000796_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000812.png\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000812_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000813.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000813_person_5_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000827.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000827_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000835.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000835_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000863.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000863_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000870.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000870_person_2_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000870_person_8_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000875.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000875_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000880.jpeg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000880_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000885.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000885_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000887.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000887_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000902.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000902_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000907.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000907_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000912.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000912_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000916.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000916_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000945.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000945_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000955.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000955_person_2_confidence_0.91.jpg\n",
      "Processing image: ./test\\Appropriate\\00000965.jpeg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000965_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000967.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000967_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000995.jpeg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000995_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000995.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000995_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00000998.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00000998_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00001002.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001002_person_6_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00001015.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001015_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00001016.png\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001016_person_1_confidence_0.59.jpg\n",
      "Processing image: ./test\\Appropriate\\00001044.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001044_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00001046.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001046_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00001068.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001068_person_5_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00001077.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001077_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00001108.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001108_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00001117.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001117_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00001133.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001133_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00001136.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001136_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00001163.png\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001163_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00001164.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001164_person_5_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00001166(1).JPG\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001166(1)_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00001166.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001166_person_4_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001166_person_7_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001166_person_10_confidence_0.75.jpg\n",
      "Processing image: ./test\\Appropriate\\00001168.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001168_person_8_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001168_person_13_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001168_person_9_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\00001193.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\00001193_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\01 (1).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\01 (1)_person_3_confidence_0.90.jpg\n",
      "Processing image: ./test\\Appropriate\\02 (3).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\02 (3)_person_2_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\02 (3)_person_5_confidence_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\02 (8).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\02 (8)_person_2_confidence_0.92.jpg\n",
      "Processing image: ./test\\Appropriate\\02 (9).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\02 (9)_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\05 (1).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\05 (1)_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\05 (7).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\05 (7)_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\05 (8).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\05 (8)_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Appropriate\\06 (2).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\06 (2)_person_2_confidence_0.99.jpg\n",
      "Processing image: ./test\\Appropriate\\07 (8).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Appropriate\\07 (8)_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\0000e2205e460318.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0000e2205e460318_person_5_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0000e2205e460318_person_18_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0000e2205e460318_person_7_confidence_0.98.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0000e2205e460318_person_1_confidence_0.96.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0000e2205e460318_person_11_confidence_0.87.jpg\n",
      "Processing image: ./test\\Inappropriate\\000379e29dd3603a.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\000379e29dd3603a_person_3_confidence_0.97.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\000379e29dd3603a_person_5_confidence_0.73.jpg\n",
      "Processing image: ./test\\Inappropriate\\000ec387d8a66dad.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\000ec387d8a66dad_person_6_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\000ec387d8a66dad_person_2_confidence_0.88.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\000ec387d8a66dad_person_9_confidence_0.79.jpg\n",
      "Processing image: ./test\\Inappropriate\\0074cc9df928b747.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0074cc9df928b747_person_7_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0074cc9df928b747_person_3_confidence_0.99.jpg\n",
      "Processing image: ./test\\Inappropriate\\00de79a1cdd0019d.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\00de79a1cdd0019d_person_4_confidence_0.96.jpg\n",
      "Processing image: ./test\\Inappropriate\\01375c81e3f3627b.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\01375c81e3f3627b_person_1_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\01375c81e3f3627b_person_7_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\01375c81e3f3627b_person_10_confidence_0.62.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\01375c81e3f3627b_person_8_confidence_0.57.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\01375c81e3f3627b_person_9_confidence_0.52.jpg\n",
      "Processing image: ./test\\Inappropriate\\01b4c747bbbcb355.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\01b4c747bbbcb355_person_4_confidence_0.99.jpg\n",
      "Processing image: ./test\\Inappropriate\\02920c8b7c3cbd7d.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\02920c8b7c3cbd7d_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\0451d49133b25e7c.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0451d49133b25e7c_person_3_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0451d49133b25e7c_person_1_confidence_0.95.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0451d49133b25e7c_person_13_confidence_0.94.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0451d49133b25e7c_person_17_confidence_0.94.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0451d49133b25e7c_person_20_confidence_0.93.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0451d49133b25e7c_person_26_confidence_0.91.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0451d49133b25e7c_person_25_confidence_0.91.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0451d49133b25e7c_person_23_confidence_0.90.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0451d49133b25e7c_person_8_confidence_0.89.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0451d49133b25e7c_person_22_confidence_0.88.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0451d49133b25e7c_person_6_confidence_0.86.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0451d49133b25e7c_person_18_confidence_0.80.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0451d49133b25e7c_person_11_confidence_0.78.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0451d49133b25e7c_person_14_confidence_0.73.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0451d49133b25e7c_person_10_confidence_0.63.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0451d49133b25e7c_person_19_confidence_0.58.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0451d49133b25e7c_person_9_confidence_0.53.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0451d49133b25e7c_person_7_confidence_0.53.jpg\n",
      "Processing image: ./test\\Inappropriate\\0480ce69618885d7.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0480ce69618885d7_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\05e187a56f91b1c9.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\05e187a56f91b1c9_person_7_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\05e187a56f91b1c9_person_15_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\05e187a56f91b1c9_person_12_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\05e187a56f91b1c9_person_11_confidence_0.58.jpg\n",
      "Processing image: ./test\\Inappropriate\\05e679c074bcb395.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\05e679c074bcb395_person_3_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\05e679c074bcb395_person_6_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\05fce26b65768cfe.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\05fce26b65768cfe_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\06ac58891617756e.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\06ac58891617756e_person_3_confidence_0.97.jpg\n",
      "Processing image: ./test\\Inappropriate\\08744a180d88946b.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\08744a180d88946b_person_9_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\08744a180d88946b_person_12_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\08744a180d88946b_person_14_confidence_0.98.jpg\n",
      "Processing image: ./test\\Inappropriate\\09380c803a4e77f9.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\09380c803a4e77f9_person_5_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\0ABCDAA1-99E8-4DC7-8281-ADFA15B5DE53.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\0ABCDAA1-99E8-4DC7-8281-ADFA15B5DE53.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_0ABCDAA1-99E8-4DC7-8281-ADFA15B5DE53.jpg\n",
      "Processing image: ./test\\Inappropriate\\0cef75de5f701811.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0cef75de5f701811_person_6_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\0cef75de5f701811_person_4_confidence_0.80.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (199).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (199)_person_6_confidence_0.99.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (223).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (223)_person_4_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (223)_person_8_confidence_0.58.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (227).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (227)_person_8_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (227)_person_3_confidence_0.74.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (234).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (234)_person_3_confidence_0.97.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (234)_person_18_confidence_0.66.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (236).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (236)_person_2_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (236)_person_9_confidence_0.93.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (240).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (240)_person_3_confidence_0.99.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (241).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (241)_person_5_confidence_0.97.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (266).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (266)_person_2_confidence_0.98.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (271).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (271)_person_3_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (271)_person_5_confidence_0.96.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (271)_person_1_confidence_0.54.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (273).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (273)_person_3_confidence_0.99.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (291).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (291)_person_2_confidence_0.99.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (293).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (293)_person_5_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (293)_person_8_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (296).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (296)_person_3_confidence_0.87.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (302).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (302)_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (306).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (306)_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (309).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (309)_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (316).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (316)_person_7_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (316)_person_5_confidence_0.60.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (321).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (321)_person_3_confidence_0.75.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (338).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (338)_person_3_confidence_0.99.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (354).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (354)_person_4_confidence_0.91.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (363).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (363)_person_1_confidence_0.99.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (368).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (368)_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (371).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (371)_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (377).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (377)_person_5_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (382).jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1 (382)_person_1_confidence_0.99.jpg\n",
      "Processing image: ./test\\Inappropriate\\1073e7c99cef3839.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1073e7c99cef3839_person_1_confidence_0.97.jpg\n",
      "Processing image: ./test\\Inappropriate\\10A569B7-3077-4628-AD1C-685829CFE240.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\10A569B7-3077-4628-AD1C-685829CFE240.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_10A569B7-3077-4628-AD1C-685829CFE240.jpg\n",
      "Processing image: ./test\\Inappropriate\\17527_05.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\17527_05_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\1a002e1aab9cf6e5.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1a002e1aab9cf6e5_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\1e5e36da31cc1b8d.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1e5e36da31cc1b8d_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\1F19E79E-EDCE-412F-B27C-7CBB739CE00F.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1F19E79E-EDCE-412F-B27C-7CBB739CE00F_person_6_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1F19E79E-EDCE-412F-B27C-7CBB739CE00F_person_1_confidence_0.90.jpg\n",
      "Processing image: ./test\\Inappropriate\\1f433aeec30ad9ae.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1f433aeec30ad9ae_person_16_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1f433aeec30ad9ae_person_19_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1f433aeec30ad9ae_person_26_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1f433aeec30ad9ae_person_11_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1f433aeec30ad9ae_person_22_confidence_0.98.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1f433aeec30ad9ae_person_3_confidence_0.98.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1f433aeec30ad9ae_person_32_confidence_0.87.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1f433aeec30ad9ae_person_13_confidence_0.82.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1f433aeec30ad9ae_person_28_confidence_0.78.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1f433aeec30ad9ae_person_35_confidence_0.75.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1f433aeec30ad9ae_person_9_confidence_0.71.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1f433aeec30ad9ae_person_31_confidence_0.69.jpg\n",
      "Processing image: ./test\\Inappropriate\\1f8904ec432f43db.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\1f8904ec432f43db_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\204E4BD0-7658-46D3-A2F6-25B6EDF35160.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\204E4BD0-7658-46D3-A2F6-25B6EDF35160_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\2088edb092641108.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\2088edb092641108_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\20c7ae35ce27d71c.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\20c7ae35ce27d71c_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\21033_05.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\21033_05_person_5_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\22336a409fd33c04.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\22336a409fd33c04_person_3_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\22336a409fd33c04_person_6_confidence_0.98.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\22336a409fd33c04_person_7_confidence_0.93.jpg\n",
      "Processing image: ./test\\Inappropriate\\22f7a1d728cd8a04.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\22f7a1d728cd8a04_person_8_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\22f7a1d728cd8a04_person_5_confidence_0.98.jpg\n",
      "Processing image: ./test\\Inappropriate\\237abe3bb1bd4cc7.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\237abe3bb1bd4cc7_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\2425e8bc23aee976.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\2425e8bc23aee976_person_2_confidence_0.91.jpg\n",
      "Processing image: ./test\\Inappropriate\\2556c1316e7c187b.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\2556c1316e7c187b_person_5_confidence_0.98.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\2556c1316e7c187b_person_1_confidence_0.80.jpg\n",
      "Processing image: ./test\\Inappropriate\\2a2f8ef1b2433b65.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\2a2f8ef1b2433b65_person_8_confidence_0.99.jpg\n",
      "Processing image: ./test\\Inappropriate\\2AA7D142-C3C9-4364-9930-4EC9DFA63BD9.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\2AA7D142-C3C9-4364-9930-4EC9DFA63BD9_person_2_confidence_0.98.jpg\n",
      "Processing image: ./test\\Inappropriate\\2fc8cced974fccad.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\2fc8cced974fccad_person_5_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\30C14D88-3F09-4CAB-A628-EC56C1E8E5BD.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\30C14D88-3F09-4CAB-A628-EC56C1E8E5BD.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_30C14D88-3F09-4CAB-A628-EC56C1E8E5BD.jpg\n",
      "Processing image: ./test\\Inappropriate\\31c41140fefbd8be.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\31c41140fefbd8be_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\33358_06.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\33358_06_person_6_confidence_0.93.jpg\n",
      "Processing image: ./test\\Inappropriate\\35e010df6b1b3dfe.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\35e010df6b1b3dfe_person_5_confidence_0.99.jpg\n",
      "Processing image: ./test\\Inappropriate\\365a349e99d312ce.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\365a349e99d312ce_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\38A3D242-CAF9-4608-BCBF-1E5EDA0BD272.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\38A3D242-CAF9-4608-BCBF-1E5EDA0BD272.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_38A3D242-CAF9-4608-BCBF-1E5EDA0BD272.jpg\n",
      "Processing image: ./test\\Inappropriate\\38d27327f378db2e.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\38d27327f378db2e_person_6_confidence_0.96.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\38d27327f378db2e_person_2_confidence_0.96.jpg\n",
      "Processing image: ./test\\Inappropriate\\3a361f760c359c16.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\3a361f760c359c16_person_9_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\3a361f760c359c16_person_5_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\3a361f760c359c16_person_11_confidence_0.97.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\3a361f760c359c16_person_8_confidence_0.92.jpg\n",
      "Processing image: ./test\\Inappropriate\\3decdd736f4079b0.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\3decdd736f4079b0_person_1_confidence_0.97.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\3decdd736f4079b0_person_7_confidence_0.85.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\3decdd736f4079b0_person_5_confidence_0.84.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\3decdd736f4079b0_person_4_confidence_0.72.jpg\n",
      "Processing image: ./test\\Inappropriate\\3EB8B7B9-B59E-4D2C-82BF-4B3D5BA66F81.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\3EB8B7B9-B59E-4D2C-82BF-4B3D5BA66F81.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_3EB8B7B9-B59E-4D2C-82BF-4B3D5BA66F81.jpg\n",
      "Processing image: ./test\\Inappropriate\\3F75B7A9-3F49-4C94-8904-935B4F3242C9.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\3F75B7A9-3F49-4C94-8904-935B4F3242C9_person_2_confidence_0.98.jpg\n",
      "Processing image: ./test\\Inappropriate\\4277dd007c66269d.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\4277dd007c66269d_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\46EAC66F-09A0-4F24-B6B5-DE52216A26CC.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\46EAC66F-09A0-4F24-B6B5-DE52216A26CC.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_46EAC66F-09A0-4F24-B6B5-DE52216A26CC.jpg\n",
      "Processing image: ./test\\Inappropriate\\4a716d0819ab4396.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\4a716d0819ab4396_person_1_confidence_0.98.jpg\n",
      "Processing image: ./test\\Inappropriate\\4B476E76-FB55-42C1-836D-4C98FBB9DFD8.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\4B476E76-FB55-42C1-836D-4C98FBB9DFD8_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\4BD74640-4E2B-41D9-A26C-6151CDDDE7B2.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\4BD74640-4E2B-41D9-A26C-6151CDDDE7B2.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_4BD74640-4E2B-41D9-A26C-6151CDDDE7B2.jpg\n",
      "Processing image: ./test\\Inappropriate\\4BE9DFAD-9AA0-4CD0-BE95-B0535DCA2197.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\4BE9DFAD-9AA0-4CD0-BE95-B0535DCA2197_person_5_confidence_0.85.jpg\n",
      "Processing image: ./test\\Inappropriate\\4c6c3853ecb81e9f.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\4c6c3853ecb81e9f_person_7_confidence_0.96.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\4c6c3853ecb81e9f_person_3_confidence_0.83.jpg\n",
      "Processing image: ./test\\Inappropriate\\4e04745cdd66fdb9.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\4e04745cdd66fdb9_person_3_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\4e04745cdd66fdb9_person_5_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\4e04745cdd66fdb9_person_10_confidence_0.80.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\4e04745cdd66fdb9_person_12_confidence_0.79.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\4e04745cdd66fdb9_person_8_confidence_0.76.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\4e04745cdd66fdb9_person_6_confidence_0.72.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\4e04745cdd66fdb9_person_9_confidence_0.63.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\4e04745cdd66fdb9_person_7_confidence_0.56.jpg\n",
      "Processing image: ./test\\Inappropriate\\4e8efc5c0cd1b325.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\4e8efc5c0cd1b325_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\52F018A1-89FE-4FA8-A4F9-54AD2C34E50E.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\52F018A1-89FE-4FA8-A4F9-54AD2C34E50E_person_2_confidence_0.94.jpg\n",
      "Processing image: ./test\\Inappropriate\\55AED6BE-2EAE-46AC-B5A2-370189486BC4.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\55AED6BE-2EAE-46AC-B5A2-370189486BC4_person_1_confidence_0.76.jpg\n",
      "Processing image: ./test\\Inappropriate\\5a8a39a9bf556fdc.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\5a8a39a9bf556fdc_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\5c7780df9811d1d0.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\5c7780df9811d1d0_person_2_confidence_0.99.jpg\n",
      "Processing image: ./test\\Inappropriate\\5D84C1B7-A155-4320-9246-FF5E30A9ACAF.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\5D84C1B7-A155-4320-9246-FF5E30A9ACAF_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\5D8E8A76-6A8F-42A5-8A31-ABF53C34EED9.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\5D8E8A76-6A8F-42A5-8A31-ABF53C34EED9_person_3_confidence_0.74.jpg\n",
      "Processing image: ./test\\Inappropriate\\6100f6f5c4dd28a7.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\6100f6f5c4dd28a7_person_2_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\62D58462-0BB0-4577-81D6-1A1087B1EED4.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\62D58462-0BB0-4577-81D6-1A1087B1EED4.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_62D58462-0BB0-4577-81D6-1A1087B1EED4.jpg\n",
      "Processing image: ./test\\Inappropriate\\639bff9535aec5a7.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\639bff9535aec5a7_person_7_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\639bff9535aec5a7_person_2_confidence_0.91.jpg\n",
      "Processing image: ./test\\Inappropriate\\64c6ede067e21ce6.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\64c6ede067e21ce6.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_64c6ede067e21ce6.jpg\n",
      "Processing image: ./test\\Inappropriate\\67f89c835588cb3d.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\67f89c835588cb3d_person_4_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\67f89c835588cb3d_person_6_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\67f89c835588cb3d_person_1_confidence_0.54.jpg\n",
      "Processing image: ./test\\Inappropriate\\6bb7322e6be3bbbd.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\6bb7322e6be3bbbd.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_6bb7322e6be3bbbd.jpg\n",
      "Processing image: ./test\\Inappropriate\\6BE7EE95-156F-45AF-B706-3F32660A12D0.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\6BE7EE95-156F-45AF-B706-3F32660A12D0.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_6BE7EE95-156F-45AF-B706-3F32660A12D0.jpg\n",
      "Processing image: ./test\\Inappropriate\\6D22F848-7DE4-4A28-8FB1-C1B2ECBAB28C.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\6D22F848-7DE4-4A28-8FB1-C1B2ECBAB28C_person_5_confidence_0.98.jpg\n",
      "Processing image: ./test\\Inappropriate\\71a84c3181e70e58.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_17_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_6_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_18_confidence_0.98.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_9_confidence_0.97.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_5_confidence_0.96.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_14_confidence_0.95.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_3_confidence_0.93.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_31_confidence_0.93.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_19_confidence_0.91.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_32_confidence_0.89.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_28_confidence_0.88.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_35_confidence_0.88.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_4_confidence_0.87.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_1_confidence_0.87.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_29_confidence_0.80.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_25_confidence_0.79.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_20_confidence_0.73.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_30_confidence_0.70.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_23_confidence_0.64.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_27_confidence_0.60.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\71a84c3181e70e58_person_22_confidence_0.57.jpg\n",
      "Processing image: ./test\\Inappropriate\\749e735478d02f5e.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\749e735478d02f5e.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_749e735478d02f5e.jpg\n",
      "Processing image: ./test\\Inappropriate\\75a95457fb5f6d36.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\75a95457fb5f6d36.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_75a95457fb5f6d36.jpg\n",
      "Processing image: ./test\\Inappropriate\\7A7DC041-AC78-4DEC-94C0-4740A7B84429.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\7A7DC041-AC78-4DEC-94C0-4740A7B84429_person_1_confidence_0.51.jpg\n",
      "Processing image: ./test\\Inappropriate\\7A983B35-84F3-4C66-B839-5B4DBF803DE2.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\7A983B35-84F3-4C66-B839-5B4DBF803DE2.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_7A983B35-84F3-4C66-B839-5B4DBF803DE2.jpg\n",
      "Processing image: ./test\\Inappropriate\\7cd621bc5fa6ac07.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\7cd621bc5fa6ac07_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\7D4FBD9E-4785-429E-BCA9-88D29A5952EB.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\7D4FBD9E-4785-429E-BCA9-88D29A5952EB_person_3_confidence_0.98.jpg\n",
      "Processing image: ./test\\Inappropriate\\7fe7c6e71f3d239e.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\7fe7c6e71f3d239e_person_6_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\83A835B2-FCE8-487F-94DB-9C05865A0385.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\83A835B2-FCE8-487F-94DB-9C05865A0385.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_83A835B2-FCE8-487F-94DB-9C05865A0385.jpg\n",
      "Processing image: ./test\\Inappropriate\\86E7D2A7-2899-46A3-A982-383F534849E2.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\86E7D2A7-2899-46A3-A982-383F534849E2_person_7_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\89F94391-348B-4153-9801-D259AE7275DE.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\89F94391-348B-4153-9801-D259AE7275DE.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_89F94391-348B-4153-9801-D259AE7275DE.jpg\n",
      "Processing image: ./test\\Inappropriate\\8A10BC23-0FE6-411B-A3E1-D7D1FCF35654.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\8A10BC23-0FE6-411B-A3E1-D7D1FCF35654.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_8A10BC23-0FE6-411B-A3E1-D7D1FCF35654.jpg\n",
      "Processing image: ./test\\Inappropriate\\8a52073053e98b25.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\8a52073053e98b25_person_12_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\8a52073053e98b25_person_1_confidence_0.96.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\8a52073053e98b25_person_10_confidence_0.92.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\8a52073053e98b25_person_3_confidence_0.87.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\8a52073053e98b25_person_18_confidence_0.54.jpg\n",
      "Processing image: ./test\\Inappropriate\\8D2FBB3A-4CA5-4986-9898-643578BCE7CC.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\8D2FBB3A-4CA5-4986-9898-643578BCE7CC_person_3_confidence_0.95.jpg\n",
      "Processing image: ./test\\Inappropriate\\8F890AB7-56D5-4D36-A992-55ABBAF4C4DF.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\8F890AB7-56D5-4D36-A992-55ABBAF4C4DF_person_3_confidence_0.99.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\8F890AB7-56D5-4D36-A992-55ABBAF4C4DF_person_5_confidence_0.70.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\8F890AB7-56D5-4D36-A992-55ABBAF4C4DF_person_9_confidence_0.56.jpg\n",
      "Processing image: ./test\\Inappropriate\\9765cf9150e3d5f7.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\9765cf9150e3d5f7_person_3_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\98f538c5fef38418.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\98f538c5fef38418_person_4_confidence_1.00.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\98f538c5fef38418_person_6_confidence_0.85.jpg\n",
      "Processing image: ./test\\Inappropriate\\9E12C4A9-2311-4D3F-A189-15321EEAC4EA.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\9E12C4A9-2311-4D3F-A189-15321EEAC4EA.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_9E12C4A9-2311-4D3F-A189-15321EEAC4EA.jpg\n",
      "Processing image: ./test\\Inappropriate\\9E7872EC-20E2-4ED3-B585-A4505EAE417F.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\9E7872EC-20E2-4ED3-B585-A4505EAE417F_person_4_confidence_1.00.jpg\n",
      "Processing image: ./test\\Inappropriate\\9ED9B230-3379-47A9-953F-674FD7E3973E.jpg\n",
      "Saved image with no person detected: ./no_persons_detectedv2\\9ED9B230-3379-47A9-953F-674FD7E3973E.jpg\n",
      "Saved image with no person detected in cropped folder: ./cropped_imagesv2\\Inappropriate\\no_person_9ED9B230-3379-47A9-953F-674FD7E3973E.jpg\n",
      "Processing image: ./test\\Inappropriate\\9FA50D05-8A85-4CB3-9B1C-91D0C2456B25.jpg\n",
      "Saved cropped person: ./cropped_imagesv2\\Inappropriate\\9FA50D05-8A85-4CB3-9B1C-91D0C2456B25_person_5_confidence_1.00.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "def get_yolo_preds_and_save_crops(net, input_img_path, cropped_folder, no_person_folder, confidence_threshold=0.6, overlapping_threshold=0.3, labels=None):\n",
    "    # Ensure output directories exist\n",
    "    if not os.path.exists(cropped_folder):\n",
    "        os.makedirs(cropped_folder)\n",
    "\n",
    "    if not os.path.exists(no_person_folder):\n",
    "        os.makedirs(no_person_folder)\n",
    "\n",
    "    # Generate random colors for each class\n",
    "    np.random.seed(123)\n",
    "    colors = np.random.randint(0, 255, size=(len(labels), 3), dtype=\"uint8\")\n",
    "\n",
    "    # Get YOLO output layer names\n",
    "    ln = net.getLayerNames()\n",
    "    unconnected_out_layers = net.getUnconnectedOutLayers()\n",
    "    ln = [ln[i - 1] for i in unconnected_out_layers.flatten()]\n",
    "\n",
    "    # Read the input image\n",
    "    image = cv2.imread(input_img_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image {input_img_path}\")\n",
    "        return\n",
    "\n",
    "    (H, W) = image.shape[:2]\n",
    "\n",
    "    # Create a blob from the image\n",
    "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layerOutputs = net.forward(ln)\n",
    "\n",
    "    # Initialize lists for detections\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "\n",
    "    # Loop through each output layer\n",
    "    for output in layerOutputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            if confidence > confidence_threshold and classID == 0:  # Class ID 0 is \"person\"\n",
    "                # Scale bounding boxes back to image dimensions\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                classIDs.append(classID)\n",
    "\n",
    "    # Perform non-maxima suppression to reduce overlapping boxes\n",
    "    bboxes = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, overlapping_threshold)\n",
    "\n",
    "    # If no persons detected, save the original image in both folders\n",
    "    if len(bboxes) == 0:\n",
    "        # Save the image in both the cropped folder and no_person folder\n",
    "        no_person_filename = os.path.join(no_person_folder, os.path.basename(input_img_path))\n",
    "        cv2.imwrite(no_person_filename, image)\n",
    "        print(f\"Saved image with no person detected: {no_person_filename}\")\n",
    "\n",
    "        # Save the entire image in the cropped folder as well\n",
    "        cropped_filename = os.path.join(cropped_folder, f\"no_person_{os.path.basename(input_img_path)}\")\n",
    "        cv2.imwrite(cropped_filename, image)\n",
    "        print(f\"Saved image with no person detected in cropped folder: {cropped_filename}\")\n",
    "        return  # Move on to the next image if no persons are detected\n",
    "\n",
    "    # Collect and save cropped persons\n",
    "    for i in bboxes.flatten():\n",
    "        (x, y) = (boxes[i][0], boxes[i][1])\n",
    "        (w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "        # Ensure the crop is within the image bounds\n",
    "        x = max(0, x)\n",
    "        y = max(0, y)\n",
    "        w = min(W - x, w)\n",
    "        h = min(H - y, h)\n",
    "\n",
    "        # Ensure the cropped region is valid (not empty)\n",
    "        if w > 0 and h > 0:\n",
    "            cropped_person = image[y:y + h, x:x + w]\n",
    "\n",
    "            # Generate a unique filename using the original image's filename, confidence level, and person index\n",
    "            original_filename = os.path.splitext(os.path.basename(input_img_path))[0]\n",
    "            cropped_filename = os.path.join(cropped_folder, f\"{original_filename}_person_{i + 1}_confidence_{confidences[i]:.2f}.jpg\")\n",
    "            cv2.imwrite(cropped_filename, cropped_person)\n",
    "            print(f\"Saved cropped person: {cropped_filename}\")\n",
    "        else:\n",
    "            print(f\"Warning: Empty crop for {input_img_path} at box {boxes[i]}\")\n",
    "\n",
    "\n",
    "def process_images_in_folder(net, input_folder, output_folder, no_person_folder, confidence_threshold=0.5, overlapping_threshold=0.3, labels=None):\n",
    "    # List all images in the folder\n",
    "    for label in ['Appropriate', 'Inappropriate']:  # Iterate over labeled folders\n",
    "        label_folder = os.path.join(input_folder, label)\n",
    "        output_label_folder = os.path.join(output_folder, label)\n",
    "\n",
    "        if not os.path.exists(output_label_folder):\n",
    "            os.makedirs(output_label_folder)\n",
    "\n",
    "        # Process each image in the folder\n",
    "        for img_filename in os.listdir(label_folder):\n",
    "            img_path = os.path.join(label_folder, img_filename)\n",
    "\n",
    "            if os.path.isfile(img_path) and img_filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                print(f\"Processing image: {img_path}\")\n",
    "                # Get YOLO predictions and save the cropped persons\n",
    "                get_yolo_preds_and_save_crops(net, img_path, output_label_folder, no_person_folder, confidence_threshold, overlapping_threshold, labels)\n",
    "\n",
    "try:\n",
    "    # Read labels from coco.txt\n",
    "    with open(\"./DARKNET/coco.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        labels = f.read().strip().split(\"\\n\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: coco.txt file not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "# Load YOLO model\n",
    "yolo_path = \"./DARKNET/model_data/yolov3.cfg\"\n",
    "weights = \"./DARKNET/model_data/yolov3.weights\"\n",
    "net = cv2.dnn.readNetFromDarknet(yolo_path, weights)\n",
    "\n",
    "# Optional: Use CUDA if available\n",
    "# cuda = True\n",
    "# if cuda:\n",
    "#     net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "#     net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "#     print(\"YOLO model is set to use GPU (CUDA).\")\n",
    "# else:\n",
    "#     net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)  # Force CPU\n",
    "#     net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "#     print(\"YOLO model is using CPU.\")\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)  # Force CPU\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    " # Check if the model is using CUDA\n",
    "\n",
    "    \n",
    "# Define the input folder (your dataset folder) and output folder\n",
    "input_folder = \"./test\"  # Folder containing 'Appropriate' and 'Inappropriate'\n",
    "output_folder = \"./cropped_imagesv2\"  # Folder to save cropped person images\n",
    "no_person_folder = \"./no_persons_detectedv2\"  # Folder to save images with no persons detected\n",
    "\n",
    "# Process all images in the input folder\n",
    "process_images_in_folder(net, input_folder, output_folder, no_person_folder, confidence_threshold=0.5, overlapping_threshold=0.3, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO model is set to use: CUDA\n",
      "Processing image: ./test\\Appropriate\\00000018.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000018.jpg: 640x416 1 person, 122.2ms\n",
      "Speed: 2.0ms preprocess, 122.2ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000018_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00000026.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000026.jpg: 320x640 1 person, 111.2ms\n",
      "Speed: 2.0ms preprocess, 111.2ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000026_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00000030.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000030.jpg: 640x640 1 person, 165.0ms\n",
      "Speed: 2.0ms preprocess, 165.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000030_person_1_conf_0.93.jpg\n",
      "Processing image: ./test\\Appropriate\\00000074.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000074.jpg: 640x640 1 person, 159.5ms\n",
      "Speed: 4.0ms preprocess, 159.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000074_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00000084.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000084.jpg: 480x640 1 person, 1 tie, 127.5ms\n",
      "Speed: 4.0ms preprocess, 127.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000084_person_1_conf_0.97.jpg\n",
      "Processing image: ./test\\Appropriate\\00000088.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000088.jpg: 480x640 1 person, 126.3ms\n",
      "Speed: 2.0ms preprocess, 126.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000088_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00000100.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000100.jpg: 448x640 1 person, 1 tie, 118.5ms\n",
      "Speed: 3.0ms preprocess, 118.5ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000100_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00000102.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000102.jpg: 288x640 1 dog, 100.5ms\n",
      "Speed: 1.0ms preprocess, 100.5ms inference, 3.0ms postprocess per image at shape (1, 3, 288, 640)\n",
      "Saved image with no person detected: ./no_person\\00000102.jpg\n",
      "Processing image: ./test\\Appropriate\\00000107.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000107.jpg: 448x640 1 person, 117.5ms\n",
      "Speed: 2.0ms preprocess, 117.5ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000107_person_1_conf_0.97.jpg\n",
      "Processing image: ./test\\Appropriate\\00000115.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000115.jpg: 448x640 2 persons, 118.6ms\n",
      "Speed: 3.0ms preprocess, 118.6ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000115_person_1_conf_0.95.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000115_person_2_conf_0.93.jpg\n",
      "Processing image: ./test\\Appropriate\\00000130.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000130.jpg: 640x512 3 persons, 116.6ms\n",
      "Speed: 5.0ms preprocess, 116.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000130_person_1_conf_0.95.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000130_person_2_conf_0.92.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000130_person_3_conf_0.89.jpg\n",
      "Processing image: ./test\\Appropriate\\00000160.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000160.jpg: 640x640 1 person, 1 tie, 159.3ms\n",
      "Speed: 4.0ms preprocess, 159.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000160_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00000166.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000166.jpg: 320x640 1 person, 104.3ms\n",
      "Speed: 2.0ms preprocess, 104.3ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000166_person_1_conf_0.97.jpg\n",
      "Processing image: ./test\\Appropriate\\00000173.jpeg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000173.jpeg: 640x416 1 person, 114.5ms\n",
      "Speed: 2.0ms preprocess, 114.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000173_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00000222.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000222.jpg: 640x448 5 persons, 114.5ms\n",
      "Speed: 2.0ms preprocess, 114.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000222_person_1_conf_0.87.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000222_person_2_conf_0.86.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000222_person_3_conf_0.84.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000222_person_4_conf_0.81.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000222_person_5_conf_0.77.jpg\n",
      "Processing image: ./test\\Appropriate\\00000224.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000224.jpg: 384x640 1 person, 1 suitcase, 122.5ms\n",
      "Speed: 2.0ms preprocess, 122.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000224_person_1_conf_0.94.jpg\n",
      "Processing image: ./test\\Appropriate\\00000225.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000225.jpg: 608x640 1 person, 154.6ms\n",
      "Speed: 4.0ms preprocess, 154.6ms inference, 2.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000225_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00000237.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000237.jpg: 384x640 1 person, 111.5ms\n",
      "Speed: 3.0ms preprocess, 111.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000237_person_1_conf_0.89.jpg\n",
      "Processing image: ./test\\Appropriate\\00000239.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000239.jpg: 448x640 11 persons, 119.5ms\n",
      "Speed: 3.0ms preprocess, 119.5ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000239_person_1_conf_0.95.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000239_person_2_conf_0.93.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000239_person_3_conf_0.92.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000239_person_4_conf_0.87.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000239_person_5_conf_0.85.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000239_person_6_conf_0.77.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000239_person_7_conf_0.76.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000239_person_8_conf_0.73.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000239_person_9_conf_0.70.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000239_person_10_conf_0.69.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000239_person_11_conf_0.66.jpg\n",
      "Processing image: ./test\\Appropriate\\00000240.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000240.jpg: 640x480 1 person, 117.4ms\n",
      "Speed: 2.0ms preprocess, 117.4ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000240_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00000240.png\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000240.png: 640x640 1 person, 158.6ms\n",
      "Speed: 3.0ms preprocess, 158.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000240_person_1_conf_0.97.jpg\n",
      "Processing image: ./test\\Appropriate\\00000243.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000243.jpg: 640x544 1 person, 152.7ms\n",
      "Speed: 3.1ms preprocess, 152.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000243_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00000248.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000248.jpg: 640x512 1 person, 116.2ms\n",
      "Speed: 5.0ms preprocess, 116.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000248_person_1_conf_0.97.jpg\n",
      "Processing image: ./test\\Appropriate\\00000252.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000252.jpg: 640x512 1 person, 115.1ms\n",
      "Speed: 2.0ms preprocess, 115.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000252_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00000270.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000270.jpg: 640x640 1 person, 157.2ms\n",
      "Speed: 4.0ms preprocess, 157.2ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000270_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00000274.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000274.jpg: 640x544 1 person, 152.1ms\n",
      "Speed: 3.0ms preprocess, 152.1ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000274_person_1_conf_0.94.jpg\n",
      "Processing image: ./test\\Appropriate\\00000292.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000292.jpg: 480x640 1 person, 124.4ms\n",
      "Speed: 3.0ms preprocess, 124.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000292_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00000299.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000299.jpg: 384x640 1 person, 111.5ms\n",
      "Speed: 2.0ms preprocess, 111.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000299_person_1_conf_0.92.jpg\n",
      "Processing image: ./test\\Appropriate\\00000304.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000304.jpg: 384x640 1 person, 109.5ms\n",
      "Speed: 2.0ms preprocess, 109.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000304_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00000306.png\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000306.png: 416x640 1 person, 115.5ms\n",
      "Speed: 3.0ms preprocess, 115.5ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000306_person_1_conf_0.94.jpg\n",
      "Processing image: ./test\\Appropriate\\00000323.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000323.jpg: 640x576 1 person, 153.6ms\n",
      "Speed: 3.0ms preprocess, 153.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000323_person_1_conf_0.93.jpg\n",
      "Processing image: ./test\\Appropriate\\00000325.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000325.jpg: 224x640 4 persons, 77.6ms\n",
      "Speed: 2.0ms preprocess, 77.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000325_person_1_conf_0.91.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000325_person_2_conf_0.82.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000325_person_3_conf_0.77.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000325_person_4_conf_0.69.jpg\n",
      "Processing image: ./test\\Appropriate\\00000326.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000326.jpg: 384x640 1 person, 117.1ms\n",
      "Speed: 2.0ms preprocess, 117.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000326_person_1_conf_0.93.jpg\n",
      "Processing image: ./test\\Appropriate\\00000328.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000328.jpg: 640x448 4 persons, 1 cell phone, 111.5ms\n",
      "Speed: 3.0ms preprocess, 111.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000328_person_1_conf_0.93.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000328_person_2_conf_0.90.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000328_person_3_conf_0.70.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000328_person_4_conf_0.65.jpg\n",
      "Processing image: ./test\\Appropriate\\00000340.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000340.jpg: 448x640 7 persons, 119.5ms\n",
      "Speed: 2.0ms preprocess, 119.5ms inference, 40.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000340_person_1_conf_0.92.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000340_person_2_conf_0.89.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000340_person_3_conf_0.85.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000340_person_4_conf_0.77.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000340_person_5_conf_0.75.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000340_person_6_conf_0.72.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000340_person_7_conf_0.69.jpg\n",
      "Processing image: ./test\\Appropriate\\00000351.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000351.jpg: 640x608 1 person, 1 tie, 156.0ms\n",
      "Speed: 5.0ms preprocess, 156.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000351_person_1_conf_0.93.jpg\n",
      "Processing image: ./test\\Appropriate\\00000368.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000368.jpg: 640x480 1 person, 132.5ms\n",
      "Speed: 2.0ms preprocess, 132.5ms inference, 18.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000368_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00000378.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000378.jpg: 288x640 1 person, 1 tie, 101.5ms\n",
      "Speed: 2.5ms preprocess, 101.5ms inference, 3.0ms postprocess per image at shape (1, 3, 288, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000378_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00000408.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000408.jpg: 448x640 3 persons, 1 tie, 121.5ms\n",
      "Speed: 3.0ms preprocess, 121.5ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000408_person_1_conf_0.95.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000408_person_2_conf_0.94.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000408_person_3_conf_0.89.jpg\n",
      "Processing image: ./test\\Appropriate\\00000416.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000416.jpg: 448x640 2 persons, 118.6ms\n",
      "Speed: 3.0ms preprocess, 118.6ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000416_person_1_conf_0.85.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000416_person_2_conf_0.78.jpg\n",
      "Processing image: ./test\\Appropriate\\00000435.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000435.jpg: 640x640 1 person, 159.1ms\n",
      "Speed: 5.0ms preprocess, 159.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000435_person_1_conf_0.79.jpg\n",
      "Processing image: ./test\\Appropriate\\00000443.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000443.jpg: 640x512 1 person, 120.5ms\n",
      "Speed: 2.0ms preprocess, 120.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000443_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00000458.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000458.jpg: 640x448 2 persons, 1 bicycle, 1 handbag, 114.2ms\n",
      "Speed: 2.0ms preprocess, 114.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000458_person_1_conf_0.93.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000458_person_2_conf_0.92.jpg\n",
      "Processing image: ./test\\Appropriate\\00000472.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000472.jpg: 384x640 5 persons, 112.4ms\n",
      "Speed: 1.0ms preprocess, 112.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000472_person_1_conf_0.92.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000472_person_2_conf_0.90.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000472_person_3_conf_0.82.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000472_person_4_conf_0.72.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000472_person_5_conf_0.67.jpg\n",
      "Processing image: ./test\\Appropriate\\00000484.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000484.jpg: 384x640 1 person, 109.1ms\n",
      "Speed: 2.0ms preprocess, 109.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000484_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00000506.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000506.jpg: 640x640 1 person, 160.5ms\n",
      "Speed: 3.0ms preprocess, 160.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000506_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00000509.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000509.jpg: 448x640 1 person, 2 cars, 119.5ms\n",
      "Speed: 3.0ms preprocess, 119.5ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000509_person_1_conf_0.79.jpg\n",
      "Processing image: ./test\\Appropriate\\00000529.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000529.jpg: 640x480 1 person, 116.5ms\n",
      "Speed: 2.0ms preprocess, 116.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000529_person_1_conf_0.94.jpg\n",
      "Processing image: ./test\\Appropriate\\00000554.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000554.jpg: 512x640 1 person, 127.0ms\n",
      "Speed: 4.0ms preprocess, 127.0ms inference, 3.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000554_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00000604.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000604.jpg: 640x480 1 person, 116.5ms\n",
      "Speed: 2.0ms preprocess, 116.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000604_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00000618.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000618.jpg: 384x640 2 persons, 1 frisbee, 110.5ms\n",
      "Speed: 2.0ms preprocess, 110.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000618_person_1_conf_0.94.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000618_person_2_conf_0.93.jpg\n",
      "Processing image: ./test\\Appropriate\\00000632.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000632.jpg: 640x640 1 person, 158.5ms\n",
      "Speed: 4.5ms preprocess, 158.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000632_person_1_conf_0.93.jpg\n",
      "Processing image: ./test\\Appropriate\\00000653.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000653.jpg: 640x480 8 persons, 1 tie, 116.5ms\n",
      "Speed: 4.0ms preprocess, 116.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000653_person_1_conf_0.92.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000653_person_2_conf_0.89.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000653_person_3_conf_0.87.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000653_person_4_conf_0.87.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000653_person_5_conf_0.78.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000653_person_6_conf_0.67.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000653_person_7_conf_0.64.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000653_person_8_conf_0.64.jpg\n",
      "Processing image: ./test\\Appropriate\\00000669.png\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000669.png: 640x640 1 person, 158.7ms\n",
      "Speed: 5.0ms preprocess, 158.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000669_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00000703.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000703.jpg: 640x448 1 person, 113.3ms\n",
      "Speed: 2.0ms preprocess, 113.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000703_person_1_conf_0.93.jpg\n",
      "Processing image: ./test\\Appropriate\\00000770.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000770.jpg: 384x640 1 person, 112.0ms\n",
      "Speed: 2.0ms preprocess, 112.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000770_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00000796.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000796.jpg: 640x512 1 person, 117.1ms\n",
      "Speed: 2.0ms preprocess, 117.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000796_person_1_conf_0.92.jpg\n",
      "Processing image: ./test\\Appropriate\\00000812.png\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000812.png: 448x640 1 person, 1 chair, 121.2ms\n",
      "Speed: 2.0ms preprocess, 121.2ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000812_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00000813.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000813.jpg: 384x640 1 person, 108.6ms\n",
      "Speed: 2.0ms preprocess, 108.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000813_person_1_conf_0.97.jpg\n",
      "Processing image: ./test\\Appropriate\\00000827.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000827.jpg: 384x640 1 person, 1 tie, 107.2ms\n",
      "Speed: 2.0ms preprocess, 107.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000827_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00000835.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000835.jpg: 384x640 1 person, 107.5ms\n",
      "Speed: 1.0ms preprocess, 107.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000835_person_1_conf_0.93.jpg\n",
      "Processing image: ./test\\Appropriate\\00000863.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000863.jpg: 448x640 1 person, 116.8ms\n",
      "Speed: 3.0ms preprocess, 116.8ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000863_person_1_conf_0.92.jpg\n",
      "Processing image: ./test\\Appropriate\\00000870.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000870.jpg: 544x640 2 persons, 143.5ms\n",
      "Speed: 3.0ms preprocess, 143.5ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000870_person_1_conf_0.96.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000870_person_2_conf_0.81.jpg\n",
      "Processing image: ./test\\Appropriate\\00000875.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000875.jpg: 320x640 1 person, 100.5ms\n",
      "Speed: 2.0ms preprocess, 100.5ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000875_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00000880.jpeg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000880.jpeg: 640x448 1 person, 110.6ms\n",
      "Speed: 3.0ms preprocess, 110.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000880_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00000885.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000885.jpg: 416x640 1 person, 112.5ms\n",
      "Speed: 2.0ms preprocess, 112.5ms inference, 3.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000885_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00000887.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000887.jpg: 512x640 1 person, 121.5ms\n",
      "Speed: 2.0ms preprocess, 121.5ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000887_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00000902.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000902.jpg: 448x640 1 person, 116.6ms\n",
      "Speed: 3.0ms preprocess, 116.6ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000902_person_1_conf_0.93.jpg\n",
      "Processing image: ./test\\Appropriate\\00000907.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000907.jpg: 384x640 1 person, 110.0ms\n",
      "Speed: 4.0ms preprocess, 110.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000907_person_1_conf_0.97.jpg\n",
      "Processing image: ./test\\Appropriate\\00000912.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000912.jpg: 384x640 1 person, 106.2ms\n",
      "Speed: 3.0ms preprocess, 106.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000912_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00000916.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000916.jpg: 384x640 1 person, 108.5ms\n",
      "Speed: 4.0ms preprocess, 108.5ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000916_person_1_conf_0.94.jpg\n",
      "Processing image: ./test\\Appropriate\\00000945.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000945.jpg: 384x640 1 person, 109.5ms\n",
      "Speed: 3.0ms preprocess, 109.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000945_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00000955.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000955.jpg: 384x640 1 person, 108.5ms\n",
      "Speed: 1.0ms preprocess, 108.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000955_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00000965.jpeg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000965.jpeg: 416x640 1 person, 117.1ms\n",
      "Speed: 2.0ms preprocess, 117.1ms inference, 3.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000965_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00000967.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000967.jpg: 448x640 1 person, 121.1ms\n",
      "Speed: 2.0ms preprocess, 121.1ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000967_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00000995.jpeg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000995.jpeg: 448x640 1 person, 2 cars, 120.4ms\n",
      "Speed: 1.0ms preprocess, 120.4ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000995_person_1_conf_0.92.jpg\n",
      "Processing image: ./test\\Appropriate\\00000995.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000995.jpg: 384x640 1 person, 114.0ms\n",
      "Speed: 2.0ms preprocess, 114.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000995_person_1_conf_0.94.jpg\n",
      "Processing image: ./test\\Appropriate\\00000998.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00000998.jpg: 384x640 1 person, 113.5ms\n",
      "Speed: 2.0ms preprocess, 113.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00000998_person_1_conf_0.97.jpg\n",
      "Processing image: ./test\\Appropriate\\00001002.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00001002.jpg: 384x640 1 person, 110.5ms\n",
      "Speed: 3.0ms preprocess, 110.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001002_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00001015.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00001015.jpg: 480x640 1 person, 125.2ms\n",
      "Speed: 2.0ms preprocess, 125.2ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001015_person_1_conf_0.97.jpg\n",
      "Processing image: ./test\\Appropriate\\00001016.png\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00001016.png: 320x640 1 person, 105.5ms\n",
      "Speed: 2.0ms preprocess, 105.5ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001016_person_1_conf_0.91.jpg\n",
      "Processing image: ./test\\Appropriate\\00001044.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00001044.jpg: 448x640 2 persons, 119.5ms\n",
      "Speed: 3.0ms preprocess, 119.5ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001044_person_1_conf_0.87.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001044_person_2_conf_0.76.jpg\n",
      "Processing image: ./test\\Appropriate\\00001046.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00001046.jpg: 480x640 1 person, 1 cell phone, 122.1ms\n",
      "Speed: 2.0ms preprocess, 122.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001046_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00001068.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00001068.jpg: 640x448 1 person, 113.2ms\n",
      "Speed: 2.0ms preprocess, 113.2ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001068_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00001077.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00001077.jpg: 384x640 1 person, 109.5ms\n",
      "Speed: 2.0ms preprocess, 109.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001077_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00001108.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00001108.jpg: 640x608 1 person, 154.0ms\n",
      "Speed: 8.0ms preprocess, 154.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001108_person_1_conf_0.91.jpg\n",
      "Processing image: ./test\\Appropriate\\00001117.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00001117.jpg: 320x640 1 person, 103.5ms\n",
      "Speed: 2.0ms preprocess, 103.5ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001117_person_1_conf_0.93.jpg\n",
      "Processing image: ./test\\Appropriate\\00001133.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00001133.jpg: 640x640 1 person, 158.0ms\n",
      "Speed: 4.0ms preprocess, 158.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001133_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00001136.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00001136.jpg: 384x640 1 person, 111.0ms\n",
      "Speed: 2.0ms preprocess, 111.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001136_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00001163.png\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00001163.png: 416x640 1 person, 115.6ms\n",
      "Speed: 2.0ms preprocess, 115.6ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001163_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00001164.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00001164.jpg: 448x640 1 person, 116.2ms\n",
      "Speed: 4.0ms preprocess, 116.2ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001164_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\00001166(1).JPG\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00001166(1).JPG: 480x640 1 person, 124.3ms\n",
      "Speed: 4.0ms preprocess, 124.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001166(1)_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\00001166.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00001166.jpg: 448x640 2 persons, 118.5ms\n",
      "Speed: 2.0ms preprocess, 118.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001166_person_1_conf_0.95.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001166_person_2_conf_0.93.jpg\n",
      "Processing image: ./test\\Appropriate\\00001168.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00001168.jpg: 448x640 3 persons, 117.0ms\n",
      "Speed: 2.0ms preprocess, 117.0ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001168_person_1_conf_0.93.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001168_person_2_conf_0.92.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001168_person_3_conf_0.91.jpg\n",
      "Processing image: ./test\\Appropriate\\00001193.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\00001193.jpg: 480x640 1 person, 122.6ms\n",
      "Speed: 2.0ms preprocess, 122.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\00001193_person_1_conf_0.90.jpg\n",
      "Processing image: ./test\\Appropriate\\01 (1).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\01 (1).jpg: 640x384 1 person, 1 tie, 107.5ms\n",
      "Speed: 2.0ms preprocess, 107.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved cropped person: ./cropped\\Appropriate\\01 (1)_person_1_conf_0.91.jpg\n",
      "Processing image: ./test\\Appropriate\\02 (3).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\02 (3).jpg: 384x640 2 persons, 113.1ms\n",
      "Speed: 2.0ms preprocess, 113.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\02 (3)_person_1_conf_0.86.jpg\n",
      "Saved cropped person: ./cropped\\Appropriate\\02 (3)_person_2_conf_0.85.jpg\n",
      "Processing image: ./test\\Appropriate\\02 (8).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\02 (8).jpg: 640x224 1 person, 80.5ms\n",
      "Speed: 2.0ms preprocess, 80.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 224)\n",
      "Saved cropped person: ./cropped\\Appropriate\\02 (8)_person_1_conf_0.85.jpg\n",
      "Processing image: ./test\\Appropriate\\02 (9).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\02 (9).jpg: 640x288 1 person, 100.5ms\n",
      "Speed: 2.0ms preprocess, 100.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 288)\n",
      "Saved cropped person: ./cropped\\Appropriate\\02 (9)_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Appropriate\\05 (1).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\05 (1).jpg: 640x544 1 person, 149.5ms\n",
      "Speed: 5.0ms preprocess, 149.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Saved cropped person: ./cropped\\Appropriate\\05 (1)_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\05 (7).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\05 (7).jpg: 544x640 1 person, 1 tie, 146.9ms\n",
      "Speed: 3.0ms preprocess, 146.9ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\05 (7)_person_1_conf_0.97.jpg\n",
      "Processing image: ./test\\Appropriate\\05 (8).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\05 (8).jpg: 480x640 1 person, 123.1ms\n",
      "Speed: 4.0ms preprocess, 123.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\05 (8)_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\06 (2).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\06 (2).jpg: 416x640 1 person, 117.5ms\n",
      "Speed: 1.0ms preprocess, 117.5ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Saved cropped person: ./cropped\\Appropriate\\06 (2)_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Appropriate\\07 (8).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Appropriate\\07 (8).jpg: 640x608 1 person, 1 tie, 157.1ms\n",
      "Speed: 4.0ms preprocess, 157.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Saved cropped person: ./cropped\\Appropriate\\07 (8)_person_1_conf_0.84.jpg\n",
      "Processing image: ./test\\Inappropriate\\0000e2205e460318.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\0000e2205e460318.jpg: 448x640 5 persons, 118.5ms\n",
      "Speed: 3.0ms preprocess, 118.5ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0000e2205e460318_person_1_conf_0.93.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0000e2205e460318_person_2_conf_0.91.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0000e2205e460318_person_3_conf_0.91.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0000e2205e460318_person_4_conf_0.86.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0000e2205e460318_person_5_conf_0.72.jpg\n",
      "Processing image: ./test\\Inappropriate\\000379e29dd3603a.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\000379e29dd3603a.jpg: 640x512 2 persons, 115.5ms\n",
      "Speed: 4.0ms preprocess, 115.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\000379e29dd3603a_person_1_conf_0.89.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\000379e29dd3603a_person_2_conf_0.80.jpg\n",
      "Processing image: ./test\\Inappropriate\\000ec387d8a66dad.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\000ec387d8a66dad.jpg: 640x640 3 persons, 1 umbrella, 1 surfboard, 155.5ms\n",
      "Speed: 6.0ms preprocess, 155.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\000ec387d8a66dad_person_1_conf_0.93.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\000ec387d8a66dad_person_2_conf_0.85.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\000ec387d8a66dad_person_3_conf_0.77.jpg\n",
      "Processing image: ./test\\Inappropriate\\0074cc9df928b747.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\0074cc9df928b747.jpg: 448x640 3 persons, 120.7ms\n",
      "Speed: 2.0ms preprocess, 120.7ms inference, 4.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0074cc9df928b747_person_1_conf_0.95.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0074cc9df928b747_person_2_conf_0.86.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0074cc9df928b747_person_3_conf_0.84.jpg\n",
      "Processing image: ./test\\Inappropriate\\00de79a1cdd0019d.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\00de79a1cdd0019d.jpg: 640x480 1 person, 115.5ms\n",
      "Speed: 4.0ms preprocess, 115.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\00de79a1cdd0019d_person_1_conf_0.86.jpg\n",
      "Processing image: ./test\\Inappropriate\\01375c81e3f3627b.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\01375c81e3f3627b.jpg: 448x640 4 persons, 116.2ms\n",
      "Speed: 3.0ms preprocess, 116.2ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\01375c81e3f3627b_person_1_conf_0.94.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\01375c81e3f3627b_person_2_conf_0.93.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\01375c81e3f3627b_person_3_conf_0.72.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\01375c81e3f3627b_person_4_conf_0.62.jpg\n",
      "Processing image: ./test\\Inappropriate\\01b4c747bbbcb355.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\01b4c747bbbcb355.jpg: 448x640 1 person, 117.5ms\n",
      "Speed: 2.0ms preprocess, 117.5ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\01b4c747bbbcb355_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Inappropriate\\02920c8b7c3cbd7d.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\02920c8b7c3cbd7d.jpg: 640x352 1 person, 104.5ms\n",
      "Speed: 2.0ms preprocess, 104.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\02920c8b7c3cbd7d_person_1_conf_0.93.jpg\n",
      "Processing image: ./test\\Inappropriate\\0451d49133b25e7c.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\0451d49133b25e7c.jpg: 512x640 7 persons, 124.7ms\n",
      "Speed: 4.0ms preprocess, 124.7ms inference, 2.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0451d49133b25e7c_person_1_conf_0.92.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0451d49133b25e7c_person_2_conf_0.84.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0451d49133b25e7c_person_3_conf_0.82.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0451d49133b25e7c_person_4_conf_0.72.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0451d49133b25e7c_person_5_conf_0.69.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0451d49133b25e7c_person_6_conf_0.66.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0451d49133b25e7c_person_7_conf_0.65.jpg\n",
      "Processing image: ./test\\Inappropriate\\0480ce69618885d7.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\0480ce69618885d7.jpg: 640x448 1 person, 112.6ms\n",
      "Speed: 2.0ms preprocess, 112.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0480ce69618885d7_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Inappropriate\\05e187a56f91b1c9.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\05e187a56f91b1c9.jpg: 640x416 3 persons, 1 umbrella, 109.5ms\n",
      "Speed: 2.0ms preprocess, 109.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\05e187a56f91b1c9_person_1_conf_0.95.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\05e187a56f91b1c9_person_2_conf_0.91.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\05e187a56f91b1c9_person_3_conf_0.84.jpg\n",
      "Processing image: ./test\\Inappropriate\\05e679c074bcb395.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\05e679c074bcb395.jpg: 640x480 2 persons, 115.5ms\n",
      "Speed: 3.0ms preprocess, 115.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\05e679c074bcb395_person_1_conf_0.96.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\05e679c074bcb395_person_2_conf_0.89.jpg\n",
      "Processing image: ./test\\Inappropriate\\05fce26b65768cfe.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\05fce26b65768cfe.jpg: 608x640 1 person, 155.1ms\n",
      "Speed: 5.0ms preprocess, 155.1ms inference, 2.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\05fce26b65768cfe_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Inappropriate\\06ac58891617756e.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\06ac58891617756e.jpg: 448x640 1 person, 118.6ms\n",
      "Speed: 2.0ms preprocess, 118.6ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\06ac58891617756e_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Inappropriate\\08744a180d88946b.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\08744a180d88946b.jpg: 640x448 2 persons, 114.5ms\n",
      "Speed: 3.0ms preprocess, 114.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\08744a180d88946b_person_1_conf_0.92.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\08744a180d88946b_person_2_conf_0.87.jpg\n",
      "Processing image: ./test\\Inappropriate\\09380c803a4e77f9.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\09380c803a4e77f9.jpg: 640x384 1 person, 110.5ms\n",
      "Speed: 3.0ms preprocess, 110.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\09380c803a4e77f9_person_1_conf_0.93.jpg\n",
      "Processing image: ./test\\Inappropriate\\0ABCDAA1-99E8-4DC7-8281-ADFA15B5DE53.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\0ABCDAA1-99E8-4DC7-8281-ADFA15B5DE53.jpg: 608x640 1 person, 154.1ms\n",
      "Speed: 5.0ms preprocess, 154.1ms inference, 1.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0ABCDAA1-99E8-4DC7-8281-ADFA15B5DE53_person_1_conf_0.60.jpg\n",
      "Processing image: ./test\\Inappropriate\\0cef75de5f701811.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\0cef75de5f701811.jpg: 640x416 2 persons, 109.6ms\n",
      "Speed: 2.0ms preprocess, 109.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0cef75de5f701811_person_1_conf_0.94.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\0cef75de5f701811_person_2_conf_0.61.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (199).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (199).jpg: 448x640 1 person, 119.5ms\n",
      "Speed: 2.0ms preprocess, 119.5ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (199)_person_1_conf_0.82.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (223).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (223).jpg: 448x640 1 person, 117.5ms\n",
      "Speed: 2.0ms preprocess, 117.5ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (223)_person_1_conf_0.87.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (227).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (227).jpg: 384x640 1 person, 1 potted plant, 110.5ms\n",
      "Speed: 1.0ms preprocess, 110.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (227)_person_1_conf_0.88.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (234).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (234).jpg: 640x448 1 person, 113.1ms\n",
      "Speed: 2.0ms preprocess, 113.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (234)_person_1_conf_0.83.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (236).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (236).jpg: 448x640 1 person, 118.1ms\n",
      "Speed: 3.0ms preprocess, 118.1ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (236)_person_1_conf_0.86.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (240).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (240).jpg: 448x640 1 person, 116.6ms\n",
      "Speed: 4.0ms preprocess, 116.6ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (240)_person_1_conf_0.84.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (241).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (241).jpg: 416x640 2 persons, 1 couch, 114.4ms\n",
      "Speed: 2.0ms preprocess, 114.4ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (241)_person_1_conf_0.78.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (241)_person_2_conf_0.72.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (266).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (266).jpg: 640x448 1 person, 1 bed, 112.0ms\n",
      "Speed: 2.0ms preprocess, 112.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (266)_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (271).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (271).jpg: 384x640 3 persons, 111.5ms\n",
      "Speed: 2.0ms preprocess, 111.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (271)_person_1_conf_0.95.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (271)_person_2_conf_0.94.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (271)_person_3_conf_0.92.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (273).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (273).jpg: 448x640 1 person, 118.0ms\n",
      "Speed: 3.0ms preprocess, 118.0ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (273)_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (291).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (291).jpg: 320x640 2 persons, 102.6ms\n",
      "Speed: 2.0ms preprocess, 102.6ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (291)_person_1_conf_0.94.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (291)_person_2_conf_0.70.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (293).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (293).jpg: 640x544 2 persons, 149.5ms\n",
      "Speed: 3.0ms preprocess, 149.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (293)_person_1_conf_0.91.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (293)_person_2_conf_0.88.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (296).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (296).jpg: 480x640 1 person, 124.4ms\n",
      "Speed: 2.0ms preprocess, 124.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (296)_person_1_conf_0.90.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (302).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (302).jpg: 640x480 1 person, 115.5ms\n",
      "Speed: 2.0ms preprocess, 115.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (302)_person_1_conf_0.93.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (306).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (306).jpg: 640x480 1 person, 113.0ms\n",
      "Speed: 3.0ms preprocess, 113.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (306)_person_1_conf_0.97.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (309).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (309).jpg: 480x640 1 person, 122.3ms\n",
      "Speed: 1.0ms preprocess, 122.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (309)_person_1_conf_0.74.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (316).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (316).jpg: 480x640 2 persons, 121.5ms\n",
      "Speed: 2.0ms preprocess, 121.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (316)_person_1_conf_0.91.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (316)_person_2_conf_0.61.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (321).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (321).jpg: 448x640 1 person, 117.7ms\n",
      "Speed: 2.0ms preprocess, 117.7ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (321)_person_1_conf_0.80.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (338).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (338).jpg: 640x448 1 person, 112.3ms\n",
      "Speed: 4.0ms preprocess, 112.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (338)_person_1_conf_0.94.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (354).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (354).jpg: 640x448 1 person, 109.1ms\n",
      "Speed: 3.0ms preprocess, 109.1ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (354)_person_1_conf_0.87.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (363).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (363).jpg: 640x480 1 person, 113.7ms\n",
      "Speed: 2.0ms preprocess, 113.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (363)_person_1_conf_0.90.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (368).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (368).jpg: 640x416 1 person, 108.5ms\n",
      "Speed: 2.0ms preprocess, 108.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (368)_person_1_conf_0.94.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (371).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (371).jpg: 640x448 1 person, 111.6ms\n",
      "Speed: 1.0ms preprocess, 111.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (371)_person_1_conf_0.93.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (377).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (377).jpg: 640x448 1 person, 111.5ms\n",
      "Speed: 2.0ms preprocess, 111.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (377)_person_1_conf_0.90.jpg\n",
      "Processing image: ./test\\Inappropriate\\1 (382).jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1 (382).jpg: 640x448 1 person, 1 bed, 110.5ms\n",
      "Speed: 1.0ms preprocess, 110.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1 (382)_person_1_conf_0.90.jpg\n",
      "Processing image: ./test\\Inappropriate\\1073e7c99cef3839.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1073e7c99cef3839.jpg: 448x640 1 person, 117.6ms\n",
      "Speed: 4.0ms preprocess, 117.6ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1073e7c99cef3839_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Inappropriate\\10A569B7-3077-4628-AD1C-685829CFE240.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\10A569B7-3077-4628-AD1C-685829CFE240.jpg: 480x640 (no detections), 121.5ms\n",
      "Speed: 2.0ms preprocess, 121.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved image with no person detected: ./no_person\\10A569B7-3077-4628-AD1C-685829CFE240.jpg\n",
      "Processing image: ./test\\Inappropriate\\17527_05.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\17527_05.jpg: 640x480 1 person, 115.2ms\n",
      "Speed: 2.0ms preprocess, 115.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\17527_05_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Inappropriate\\1a002e1aab9cf6e5.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1a002e1aab9cf6e5.jpg: 640x448 1 person, 111.5ms\n",
      "Speed: 3.0ms preprocess, 111.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1a002e1aab9cf6e5_person_1_conf_0.93.jpg\n",
      "Processing image: ./test\\Inappropriate\\1e5e36da31cc1b8d.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1e5e36da31cc1b8d.jpg: 640x448 1 person, 110.5ms\n",
      "Speed: 2.0ms preprocess, 110.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1e5e36da31cc1b8d_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Inappropriate\\1F19E79E-EDCE-412F-B27C-7CBB739CE00F.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1F19E79E-EDCE-412F-B27C-7CBB739CE00F.jpg: 416x640 2 persons, 118.6ms\n",
      "Speed: 2.0ms preprocess, 118.6ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1F19E79E-EDCE-412F-B27C-7CBB739CE00F_person_1_conf_0.89.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1F19E79E-EDCE-412F-B27C-7CBB739CE00F_person_2_conf_0.68.jpg\n",
      "Processing image: ./test\\Inappropriate\\1f433aeec30ad9ae.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1f433aeec30ad9ae.jpg: 480x640 13 persons, 1 boat, 1 surfboard, 125.1ms\n",
      "Speed: 3.0ms preprocess, 125.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1f433aeec30ad9ae_person_1_conf_0.92.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1f433aeec30ad9ae_person_2_conf_0.90.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1f433aeec30ad9ae_person_3_conf_0.89.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1f433aeec30ad9ae_person_4_conf_0.88.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1f433aeec30ad9ae_person_5_conf_0.84.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1f433aeec30ad9ae_person_6_conf_0.84.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1f433aeec30ad9ae_person_7_conf_0.82.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1f433aeec30ad9ae_person_8_conf_0.81.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1f433aeec30ad9ae_person_9_conf_0.81.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1f433aeec30ad9ae_person_10_conf_0.74.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1f433aeec30ad9ae_person_11_conf_0.74.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1f433aeec30ad9ae_person_12_conf_0.71.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1f433aeec30ad9ae_person_13_conf_0.63.jpg\n",
      "Processing image: ./test\\Inappropriate\\1f8904ec432f43db.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\1f8904ec432f43db.jpg: 640x480 1 person, 115.4ms\n",
      "Speed: 3.0ms preprocess, 115.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\1f8904ec432f43db_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Inappropriate\\204E4BD0-7658-46D3-A2F6-25B6EDF35160.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\204E4BD0-7658-46D3-A2F6-25B6EDF35160.jpg: 640x448 1 person, 111.5ms\n",
      "Speed: 3.0ms preprocess, 111.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\204E4BD0-7658-46D3-A2F6-25B6EDF35160_person_1_conf_0.93.jpg\n",
      "Processing image: ./test\\Inappropriate\\2088edb092641108.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\2088edb092641108.jpg: 640x480 1 person, 114.5ms\n",
      "Speed: 2.0ms preprocess, 114.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\2088edb092641108_person_1_conf_0.97.jpg\n",
      "Processing image: ./test\\Inappropriate\\20c7ae35ce27d71c.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\20c7ae35ce27d71c.jpg: 640x448 1 person, 112.6ms\n",
      "Speed: 2.0ms preprocess, 112.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\20c7ae35ce27d71c_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Inappropriate\\21033_05.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\21033_05.jpg: 640x480 1 person, 114.3ms\n",
      "Speed: 2.0ms preprocess, 114.3ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\21033_05_person_1_conf_0.92.jpg\n",
      "Processing image: ./test\\Inappropriate\\22336a409fd33c04.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\22336a409fd33c04.jpg: 640x544 2 persons, 1 dog, 148.9ms\n",
      "Speed: 5.0ms preprocess, 148.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\22336a409fd33c04_person_1_conf_0.94.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\22336a409fd33c04_person_2_conf_0.81.jpg\n",
      "Processing image: ./test\\Inappropriate\\22f7a1d728cd8a04.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\22f7a1d728cd8a04.jpg: 640x480 2 persons, 115.6ms\n",
      "Speed: 3.0ms preprocess, 115.6ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\22f7a1d728cd8a04_person_1_conf_0.95.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\22f7a1d728cd8a04_person_2_conf_0.77.jpg\n",
      "Processing image: ./test\\Inappropriate\\237abe3bb1bd4cc7.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\237abe3bb1bd4cc7.jpg: 448x640 1 person, 120.0ms\n",
      "Speed: 3.0ms preprocess, 120.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\237abe3bb1bd4cc7_person_1_conf_0.94.jpg\n",
      "Processing image: ./test\\Inappropriate\\2425e8bc23aee976.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\2425e8bc23aee976.jpg: 448x640 1 person, 2 boats, 117.5ms\n",
      "Speed: 3.0ms preprocess, 117.5ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\2425e8bc23aee976_person_1_conf_0.88.jpg\n",
      "Processing image: ./test\\Inappropriate\\2556c1316e7c187b.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\2556c1316e7c187b.jpg: 448x640 2 persons, 1 motorcycle, 117.8ms\n",
      "Speed: 3.0ms preprocess, 117.8ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\2556c1316e7c187b_person_1_conf_0.93.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\2556c1316e7c187b_person_2_conf_0.87.jpg\n",
      "Processing image: ./test\\Inappropriate\\2a2f8ef1b2433b65.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\2a2f8ef1b2433b65.jpg: 448x640 (no detections), 118.2ms\n",
      "Speed: 3.0ms preprocess, 118.2ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved image with no person detected: ./no_person\\2a2f8ef1b2433b65.jpg\n",
      "Processing image: ./test\\Inappropriate\\2AA7D142-C3C9-4364-9930-4EC9DFA63BD9.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\2AA7D142-C3C9-4364-9930-4EC9DFA63BD9.jpg: 448x640 1 person, 115.5ms\n",
      "Speed: 2.0ms preprocess, 115.5ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\2AA7D142-C3C9-4364-9930-4EC9DFA63BD9_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Inappropriate\\2fc8cced974fccad.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\2fc8cced974fccad.jpg: 640x448 1 person, 112.6ms\n",
      "Speed: 3.0ms preprocess, 112.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\2fc8cced974fccad_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Inappropriate\\30C14D88-3F09-4CAB-A628-EC56C1E8E5BD.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\30C14D88-3F09-4CAB-A628-EC56C1E8E5BD.jpg: 640x448 (no detections), 109.6ms\n",
      "Speed: 3.0ms preprocess, 109.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved image with no person detected: ./no_person\\30C14D88-3F09-4CAB-A628-EC56C1E8E5BD.jpg\n",
      "Processing image: ./test\\Inappropriate\\31c41140fefbd8be.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\31c41140fefbd8be.jpg: 640x448 2 persons, 113.2ms\n",
      "Speed: 3.0ms preprocess, 113.2ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\31c41140fefbd8be_person_1_conf_0.94.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\31c41140fefbd8be_person_2_conf_0.77.jpg\n",
      "Processing image: ./test\\Inappropriate\\33358_06.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\33358_06.jpg: 640x480 1 person, 114.6ms\n",
      "Speed: 2.5ms preprocess, 114.6ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\33358_06_person_1_conf_0.74.jpg\n",
      "Processing image: ./test\\Inappropriate\\35e010df6b1b3dfe.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\35e010df6b1b3dfe.jpg: 640x512 1 person, 114.1ms\n",
      "Speed: 3.0ms preprocess, 114.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\35e010df6b1b3dfe_person_1_conf_0.93.jpg\n",
      "Processing image: ./test\\Inappropriate\\365a349e99d312ce.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\365a349e99d312ce.jpg: 480x640 1 person, 123.1ms\n",
      "Speed: 3.0ms preprocess, 123.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\365a349e99d312ce_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Inappropriate\\38A3D242-CAF9-4608-BCBF-1E5EDA0BD272.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\38A3D242-CAF9-4608-BCBF-1E5EDA0BD272.jpg: 480x640 (no detections), 124.5ms\n",
      "Speed: 2.0ms preprocess, 124.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved image with no person detected: ./no_person\\38A3D242-CAF9-4608-BCBF-1E5EDA0BD272.jpg\n",
      "Processing image: ./test\\Inappropriate\\38d27327f378db2e.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\38d27327f378db2e.jpg: 448x640 4 persons, 118.1ms\n",
      "Speed: 3.0ms preprocess, 118.1ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\38d27327f378db2e_person_1_conf_0.91.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\38d27327f378db2e_person_2_conf_0.89.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\38d27327f378db2e_person_3_conf_0.86.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\38d27327f378db2e_person_4_conf_0.80.jpg\n",
      "Processing image: ./test\\Inappropriate\\3a361f760c359c16.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\3a361f760c359c16.jpg: 480x640 4 persons, 120.5ms\n",
      "Speed: 3.0ms preprocess, 120.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\3a361f760c359c16_person_1_conf_0.92.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\3a361f760c359c16_person_2_conf_0.92.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\3a361f760c359c16_person_3_conf_0.78.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\3a361f760c359c16_person_4_conf_0.76.jpg\n",
      "Processing image: ./test\\Inappropriate\\3decdd736f4079b0.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\3decdd736f4079b0.jpg: 640x448 3 persons, 110.5ms\n",
      "Speed: 3.0ms preprocess, 110.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\3decdd736f4079b0_person_1_conf_0.87.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\3decdd736f4079b0_person_2_conf_0.85.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\3decdd736f4079b0_person_3_conf_0.73.jpg\n",
      "Processing image: ./test\\Inappropriate\\3EB8B7B9-B59E-4D2C-82BF-4B3D5BA66F81.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\3EB8B7B9-B59E-4D2C-82BF-4B3D5BA66F81.jpg: 640x544 (no detections), 150.6ms\n",
      "Speed: 4.0ms preprocess, 150.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Saved image with no person detected: ./no_person\\3EB8B7B9-B59E-4D2C-82BF-4B3D5BA66F81.jpg\n",
      "Processing image: ./test\\Inappropriate\\3F75B7A9-3F49-4C94-8904-935B4F3242C9.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\3F75B7A9-3F49-4C94-8904-935B4F3242C9.jpg: 448x640 2 persons, 115.3ms\n",
      "Speed: 2.0ms preprocess, 115.3ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\3F75B7A9-3F49-4C94-8904-935B4F3242C9_person_1_conf_0.92.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\3F75B7A9-3F49-4C94-8904-935B4F3242C9_person_2_conf_0.67.jpg\n",
      "Processing image: ./test\\Inappropriate\\4277dd007c66269d.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\4277dd007c66269d.jpg: 640x448 1 person, 1 surfboard, 108.6ms\n",
      "Speed: 2.0ms preprocess, 108.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\4277dd007c66269d_person_1_conf_0.92.jpg\n",
      "Processing image: ./test\\Inappropriate\\46EAC66F-09A0-4F24-B6B5-DE52216A26CC.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\46EAC66F-09A0-4F24-B6B5-DE52216A26CC.jpg: 512x640 (no detections), 123.0ms\n",
      "Speed: 1.0ms preprocess, 123.0ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Saved image with no person detected: ./no_person\\46EAC66F-09A0-4F24-B6B5-DE52216A26CC.jpg\n",
      "Processing image: ./test\\Inappropriate\\4a716d0819ab4396.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\4a716d0819ab4396.jpg: 640x448 1 person, 113.6ms\n",
      "Speed: 4.0ms preprocess, 113.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\4a716d0819ab4396_person_1_conf_0.94.jpg\n",
      "Processing image: ./test\\Inappropriate\\4B476E76-FB55-42C1-836D-4C98FBB9DFD8.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\4B476E76-FB55-42C1-836D-4C98FBB9DFD8.jpg: 640x448 1 person, 108.5ms\n",
      "Speed: 2.0ms preprocess, 108.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\4B476E76-FB55-42C1-836D-4C98FBB9DFD8_person_1_conf_0.94.jpg\n",
      "Processing image: ./test\\Inappropriate\\4BD74640-4E2B-41D9-A26C-6151CDDDE7B2.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\4BD74640-4E2B-41D9-A26C-6151CDDDE7B2.jpg: 640x480 1 person, 112.5ms\n",
      "Speed: 4.0ms preprocess, 112.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\4BD74640-4E2B-41D9-A26C-6151CDDDE7B2_person_1_conf_0.71.jpg\n",
      "Processing image: ./test\\Inappropriate\\4BE9DFAD-9AA0-4CD0-BE95-B0535DCA2197.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\4BE9DFAD-9AA0-4CD0-BE95-B0535DCA2197.jpg: 448x640 1 person, 117.3ms\n",
      "Speed: 1.0ms preprocess, 117.3ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\4BE9DFAD-9AA0-4CD0-BE95-B0535DCA2197_person_1_conf_0.90.jpg\n",
      "Processing image: ./test\\Inappropriate\\4c6c3853ecb81e9f.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\4c6c3853ecb81e9f.jpg: 448x640 2 persons, 1 chair, 116.5ms\n",
      "Speed: 2.0ms preprocess, 116.5ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\4c6c3853ecb81e9f_person_1_conf_0.89.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\4c6c3853ecb81e9f_person_2_conf_0.84.jpg\n",
      "Processing image: ./test\\Inappropriate\\4e04745cdd66fdb9.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\4e04745cdd66fdb9.jpg: 640x480 6 persons, 1 surfboard, 112.1ms\n",
      "Speed: 4.0ms preprocess, 112.1ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\4e04745cdd66fdb9_person_1_conf_0.94.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\4e04745cdd66fdb9_person_2_conf_0.93.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\4e04745cdd66fdb9_person_3_conf_0.74.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\4e04745cdd66fdb9_person_4_conf_0.71.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\4e04745cdd66fdb9_person_5_conf_0.70.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\4e04745cdd66fdb9_person_6_conf_0.61.jpg\n",
      "Processing image: ./test\\Inappropriate\\4e8efc5c0cd1b325.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\4e8efc5c0cd1b325.jpg: 640x640 1 person, 153.5ms\n",
      "Speed: 5.0ms preprocess, 153.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\4e8efc5c0cd1b325_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Inappropriate\\52F018A1-89FE-4FA8-A4F9-54AD2C34E50E.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\52F018A1-89FE-4FA8-A4F9-54AD2C34E50E.jpg: 640x448 1 person, 109.4ms\n",
      "Speed: 2.0ms preprocess, 109.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\52F018A1-89FE-4FA8-A4F9-54AD2C34E50E_person_1_conf_0.82.jpg\n",
      "Processing image: ./test\\Inappropriate\\55AED6BE-2EAE-46AC-B5A2-370189486BC4.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\55AED6BE-2EAE-46AC-B5A2-370189486BC4.jpg: 640x576 1 person, 147.4ms\n",
      "Speed: 3.0ms preprocess, 147.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\55AED6BE-2EAE-46AC-B5A2-370189486BC4_person_1_conf_0.89.jpg\n",
      "Processing image: ./test\\Inappropriate\\5a8a39a9bf556fdc.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\5a8a39a9bf556fdc.jpg: 480x640 1 person, 123.0ms\n",
      "Speed: 2.0ms preprocess, 123.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\5a8a39a9bf556fdc_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Inappropriate\\5c7780df9811d1d0.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\5c7780df9811d1d0.jpg: 416x640 2 persons, 113.5ms\n",
      "Speed: 2.0ms preprocess, 113.5ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\5c7780df9811d1d0_person_1_conf_0.94.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\5c7780df9811d1d0_person_2_conf_0.69.jpg\n",
      "Processing image: ./test\\Inappropriate\\5D84C1B7-A155-4320-9246-FF5E30A9ACAF.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\5D84C1B7-A155-4320-9246-FF5E30A9ACAF.jpg: 640x608 1 person, 150.4ms\n",
      "Speed: 3.0ms preprocess, 150.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\5D84C1B7-A155-4320-9246-FF5E30A9ACAF_person_1_conf_0.90.jpg\n",
      "Processing image: ./test\\Inappropriate\\5D8E8A76-6A8F-42A5-8A31-ABF53C34EED9.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\5D8E8A76-6A8F-42A5-8A31-ABF53C34EED9.jpg: 384x640 1 person, 1 bed, 106.5ms\n",
      "Speed: 2.0ms preprocess, 106.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\5D8E8A76-6A8F-42A5-8A31-ABF53C34EED9_person_1_conf_0.69.jpg\n",
      "Processing image: ./test\\Inappropriate\\6100f6f5c4dd28a7.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\6100f6f5c4dd28a7.jpg: 640x416 1 person, 106.6ms\n",
      "Speed: 2.0ms preprocess, 106.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\6100f6f5c4dd28a7_person_1_conf_0.95.jpg\n",
      "Processing image: ./test\\Inappropriate\\62D58462-0BB0-4577-81D6-1A1087B1EED4.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\62D58462-0BB0-4577-81D6-1A1087B1EED4.jpg: 640x448 (no detections), 108.1ms\n",
      "Speed: 1.0ms preprocess, 108.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved image with no person detected: ./no_person\\62D58462-0BB0-4577-81D6-1A1087B1EED4.jpg\n",
      "Processing image: ./test\\Inappropriate\\639bff9535aec5a7.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\639bff9535aec5a7.jpg: 640x448 2 persons, 106.5ms\n",
      "Speed: 3.0ms preprocess, 106.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\639bff9535aec5a7_person_1_conf_0.88.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\639bff9535aec5a7_person_2_conf_0.87.jpg\n",
      "Processing image: ./test\\Inappropriate\\64c6ede067e21ce6.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\64c6ede067e21ce6.jpg: 480x640 (no detections), 120.3ms\n",
      "Speed: 4.0ms preprocess, 120.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved image with no person detected: ./no_person\\64c6ede067e21ce6.jpg\n",
      "Processing image: ./test\\Inappropriate\\67f89c835588cb3d.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\67f89c835588cb3d.jpg: 448x640 3 persons, 1 car, 114.5ms\n",
      "Speed: 3.0ms preprocess, 114.5ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\67f89c835588cb3d_person_1_conf_0.94.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\67f89c835588cb3d_person_2_conf_0.92.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\67f89c835588cb3d_person_3_conf_0.73.jpg\n",
      "Processing image: ./test\\Inappropriate\\6bb7322e6be3bbbd.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\6bb7322e6be3bbbd.jpg: 640x512 (no detections), 113.0ms\n",
      "Speed: 3.0ms preprocess, 113.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Saved image with no person detected: ./no_person\\6bb7322e6be3bbbd.jpg\n",
      "Processing image: ./test\\Inappropriate\\6BE7EE95-156F-45AF-B706-3F32660A12D0.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\6BE7EE95-156F-45AF-B706-3F32660A12D0.jpg: 640x416 (no detections), 107.1ms\n",
      "Speed: 1.0ms preprocess, 107.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Saved image with no person detected: ./no_person\\6BE7EE95-156F-45AF-B706-3F32660A12D0.jpg\n",
      "Processing image: ./test\\Inappropriate\\6D22F848-7DE4-4A28-8FB1-C1B2ECBAB28C.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\6D22F848-7DE4-4A28-8FB1-C1B2ECBAB28C.jpg: 448x640 1 person, 120.2ms\n",
      "Speed: 2.0ms preprocess, 120.2ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\6D22F848-7DE4-4A28-8FB1-C1B2ECBAB28C_person_1_conf_0.85.jpg\n",
      "Processing image: ./test\\Inappropriate\\71a84c3181e70e58.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\71a84c3181e70e58.jpg: 448x640 13 persons, 114.4ms\n",
      "Speed: 2.0ms preprocess, 114.4ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\71a84c3181e70e58_person_1_conf_0.90.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\71a84c3181e70e58_person_2_conf_0.86.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\71a84c3181e70e58_person_3_conf_0.85.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\71a84c3181e70e58_person_4_conf_0.85.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\71a84c3181e70e58_person_5_conf_0.81.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\71a84c3181e70e58_person_6_conf_0.79.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\71a84c3181e70e58_person_7_conf_0.75.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\71a84c3181e70e58_person_8_conf_0.75.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\71a84c3181e70e58_person_9_conf_0.73.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\71a84c3181e70e58_person_10_conf_0.72.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\71a84c3181e70e58_person_11_conf_0.71.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\71a84c3181e70e58_person_12_conf_0.68.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\71a84c3181e70e58_person_13_conf_0.67.jpg\n",
      "Processing image: ./test\\Inappropriate\\749e735478d02f5e.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\749e735478d02f5e.jpg: 640x480 (no detections), 111.2ms\n",
      "Speed: 7.0ms preprocess, 111.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved image with no person detected: ./no_person\\749e735478d02f5e.jpg\n",
      "Processing image: ./test\\Inappropriate\\75a95457fb5f6d36.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\75a95457fb5f6d36.jpg: 640x480 (no detections), 111.1ms\n",
      "Speed: 2.0ms preprocess, 111.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved image with no person detected: ./no_person\\75a95457fb5f6d36.jpg\n",
      "Processing image: ./test\\Inappropriate\\7A7DC041-AC78-4DEC-94C0-4740A7B84429.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\7A7DC041-AC78-4DEC-94C0-4740A7B84429.jpg: 640x480 (no detections), 115.2ms\n",
      "Speed: 2.0ms preprocess, 115.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved image with no person detected: ./no_person\\7A7DC041-AC78-4DEC-94C0-4740A7B84429.jpg\n",
      "Processing image: ./test\\Inappropriate\\7A983B35-84F3-4C66-B839-5B4DBF803DE2.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\7A983B35-84F3-4C66-B839-5B4DBF803DE2.jpg: 480x640 (no detections), 121.3ms\n",
      "Speed: 3.0ms preprocess, 121.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved image with no person detected: ./no_person\\7A983B35-84F3-4C66-B839-5B4DBF803DE2.jpg\n",
      "Processing image: ./test\\Inappropriate\\7cd621bc5fa6ac07.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\7cd621bc5fa6ac07.jpg: 480x640 1 person, 119.5ms\n",
      "Speed: 2.0ms preprocess, 119.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\7cd621bc5fa6ac07_person_1_conf_0.92.jpg\n",
      "Processing image: ./test\\Inappropriate\\7D4FBD9E-4785-429E-BCA9-88D29A5952EB.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\7D4FBD9E-4785-429E-BCA9-88D29A5952EB.jpg: 640x384 (no detections), 105.0ms\n",
      "Speed: 2.0ms preprocess, 105.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved image with no person detected: ./no_person\\7D4FBD9E-4785-429E-BCA9-88D29A5952EB.jpg\n",
      "Processing image: ./test\\Inappropriate\\7fe7c6e71f3d239e.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\7fe7c6e71f3d239e.jpg: 640x416 1 person, 109.5ms\n",
      "Speed: 2.0ms preprocess, 109.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\7fe7c6e71f3d239e_person_1_conf_0.97.jpg\n",
      "Processing image: ./test\\Inappropriate\\83A835B2-FCE8-487F-94DB-9C05865A0385.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\83A835B2-FCE8-487F-94DB-9C05865A0385.jpg: 480x640 1 person, 121.5ms\n",
      "Speed: 2.0ms preprocess, 121.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\83A835B2-FCE8-487F-94DB-9C05865A0385_person_1_conf_0.85.jpg\n",
      "Processing image: ./test\\Inappropriate\\86E7D2A7-2899-46A3-A982-383F534849E2.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\86E7D2A7-2899-46A3-A982-383F534849E2.jpg: 640x512 1 person, 115.1ms\n",
      "Speed: 2.0ms preprocess, 115.1ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\86E7D2A7-2899-46A3-A982-383F534849E2_person_1_conf_0.89.jpg\n",
      "Processing image: ./test\\Inappropriate\\89F94391-348B-4153-9801-D259AE7275DE.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\89F94391-348B-4153-9801-D259AE7275DE.jpg: 640x640 1 person, 153.6ms\n",
      "Speed: 4.0ms preprocess, 153.6ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\89F94391-348B-4153-9801-D259AE7275DE_person_1_conf_0.91.jpg\n",
      "Processing image: ./test\\Inappropriate\\8A10BC23-0FE6-411B-A3E1-D7D1FCF35654.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\8A10BC23-0FE6-411B-A3E1-D7D1FCF35654.jpg: 640x480 1 person, 117.5ms\n",
      "Speed: 2.0ms preprocess, 117.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\8A10BC23-0FE6-411B-A3E1-D7D1FCF35654_person_1_conf_0.69.jpg\n",
      "Processing image: ./test\\Inappropriate\\8a52073053e98b25.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\8a52073053e98b25.jpg: 640x448 5 persons, 109.1ms\n",
      "Speed: 3.0ms preprocess, 109.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\8a52073053e98b25_person_1_conf_0.95.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\8a52073053e98b25_person_2_conf_0.71.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\8a52073053e98b25_person_3_conf_0.69.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\8a52073053e98b25_person_4_conf_0.63.jpg\n",
      "Saved cropped person: ./cropped\\Inappropriate\\8a52073053e98b25_person_5_conf_0.63.jpg\n",
      "Processing image: ./test\\Inappropriate\\8D2FBB3A-4CA5-4986-9898-643578BCE7CC.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\8D2FBB3A-4CA5-4986-9898-643578BCE7CC.jpg: 448x640 1 person, 116.4ms\n",
      "Speed: 2.0ms preprocess, 116.4ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\8D2FBB3A-4CA5-4986-9898-643578BCE7CC_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Inappropriate\\8F890AB7-56D5-4D36-A992-55ABBAF4C4DF.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\8F890AB7-56D5-4D36-A992-55ABBAF4C4DF.jpg: 640x448 1 person, 109.5ms\n",
      "Speed: 1.0ms preprocess, 109.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\8F890AB7-56D5-4D36-A992-55ABBAF4C4DF_person_1_conf_0.74.jpg\n",
      "Processing image: ./test\\Inappropriate\\9765cf9150e3d5f7.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\9765cf9150e3d5f7.jpg: 448x640 1 person, 115.4ms\n",
      "Speed: 3.0ms preprocess, 115.4ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\9765cf9150e3d5f7_person_1_conf_0.94.jpg\n",
      "Processing image: ./test\\Inappropriate\\98f538c5fef38418.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\98f538c5fef38418.jpg: 640x576 1 person, 148.1ms\n",
      "Speed: 5.0ms preprocess, 148.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\98f538c5fef38418_person_1_conf_0.96.jpg\n",
      "Processing image: ./test\\Inappropriate\\9E12C4A9-2311-4D3F-A189-15321EEAC4EA.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\9E12C4A9-2311-4D3F-A189-15321EEAC4EA.jpg: 640x384 (no detections), 104.7ms\n",
      "Speed: 1.0ms preprocess, 104.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Saved image with no person detected: ./no_person\\9E12C4A9-2311-4D3F-A189-15321EEAC4EA.jpg\n",
      "Processing image: ./test\\Inappropriate\\9E7872EC-20E2-4ED3-B585-A4505EAE417F.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\9E7872EC-20E2-4ED3-B585-A4505EAE417F.jpg: 640x448 1 person, 108.6ms\n",
      "Speed: 1.0ms preprocess, 108.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\9E7872EC-20E2-4ED3-B585-A4505EAE417F_person_1_conf_0.91.jpg\n",
      "Processing image: ./test\\Inappropriate\\9ED9B230-3379-47A9-953F-674FD7E3973E.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\9ED9B230-3379-47A9-953F-674FD7E3973E.jpg: 640x448 (no detections), 107.3ms\n",
      "Speed: 3.0ms preprocess, 107.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Saved image with no person detected: ./no_person\\9ED9B230-3379-47A9-953F-674FD7E3973E.jpg\n",
      "Processing image: ./test\\Inappropriate\\9FA50D05-8A85-4CB3-9B1C-91D0C2456B25.jpg\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\test\\Inappropriate\\9FA50D05-8A85-4CB3-9B1C-91D0C2456B25.jpg: 608x640 1 person, 151.1ms\n",
      "Speed: 3.0ms preprocess, 151.1ms inference, 1.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Saved cropped person: ./cropped\\Inappropriate\\9FA50D05-8A85-4CB3-9B1C-91D0C2456B25_person_1_conf_0.77.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def get_yolo_preds_and_save_crops(model, input_img_path, cropped_folder, no_person_folder, confidence_threshold=0.6):\n",
    "    # Ensure output directories exist\n",
    "    if not os.path.exists(cropped_folder):\n",
    "        os.makedirs(cropped_folder)\n",
    "\n",
    "    if not os.path.exists(no_person_folder):\n",
    "        os.makedirs(no_person_folder)\n",
    "\n",
    "\n",
    "    # Read the input image\n",
    "    image = cv2.imread(input_img_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image {input_img_path}\")\n",
    "        return\n",
    "\n",
    "    H, W = image.shape[:2]\n",
    "\n",
    "    # Run YOLOv3 inference\n",
    "    results = model.predict(input_img_path, device=\"cuda\", conf=confidence_threshold)\n",
    "\n",
    "    # Extract detected objects\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            class_id = int(box.cls[0].item())  # Get class ID\n",
    "            confidence = box.conf[0].item()  # Confidence score\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "\n",
    "            if confidence > confidence_threshold and class_id == 0:  # Class ID 0 is \"person\"\n",
    "                boxes.append([x1, y1, x2 - x1, y2 - y1])  # Convert to (x, y, w, h)\n",
    "                confidences.append(confidence)\n",
    "\n",
    "    # If no person detected, save original image\n",
    "    if not boxes:\n",
    "        no_person_filename = os.path.join(no_person_folder, os.path.basename(input_img_path))\n",
    "        cv2.imwrite(no_person_filename, image)\n",
    "        print(f\"Saved image with no person detected: {no_person_filename}\")\n",
    "        return  \n",
    "\n",
    "    # Save cropped persons\n",
    "    for i, (x, y, w, h) in enumerate(boxes):\n",
    "        cropped_person = image[y:y + h, x:x + w]\n",
    "\n",
    "        if cropped_person.size == 0:\n",
    "            print(f\"Warning: Empty crop for {input_img_path} at box {boxes[i]}\")\n",
    "            continue\n",
    "\n",
    "        original_filename = os.path.splitext(os.path.basename(input_img_path))[0]\n",
    "        cropped_filename = os.path.join(cropped_folder, f\"{original_filename}_person_{i + 1}_conf_{confidences[i]:.2f}.jpg\")\n",
    "        cv2.imwrite(cropped_filename, cropped_person)\n",
    "        print(f\"Saved cropped person: {cropped_filename}\")\n",
    "\n",
    "def process_images_in_folder(model, input_folder, output_folder, no_person_folder, confidence_threshold=0.5):\n",
    "    for label in ['Appropriate', 'Inappropriate']:  # Iterate over labeled folders\n",
    "        label_folder = os.path.join(input_folder, label)\n",
    "        output_label_folder = os.path.join(output_folder, label)\n",
    "\n",
    "        os.makedirs(output_label_folder, exist_ok=True)\n",
    "\n",
    "        # Process each image in the folder\n",
    "        for img_filename in os.listdir(label_folder):\n",
    "            img_path = os.path.join(label_folder, img_filename)\n",
    "\n",
    "            if os.path.isfile(img_path) and img_filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                print(f\"Processing image: {img_path}\")\n",
    "                get_yolo_preds_and_save_crops(model, img_path, output_label_folder, no_person_folder, confidence_threshold)\n",
    "\n",
    "# Load Ultralytics YOLOv3 model with CUDA\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = YOLO(\"yolov3u.pt\").to(device)\n",
    "\n",
    "print(f\"YOLO model is set to use: {device.upper()}\")\n",
    "\n",
    "# Example usage\n",
    "input_folder = \"./test\"\n",
    "output_folder = \"./cropped\"\n",
    "no_person_folder = \"./no_person\"\n",
    "\n",
    "process_images_in_folder(model, input_folder, output_folder, no_person_folder, confidence_threshold=0.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO + Resnet50&Inceptionv3 Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped Person 15 Saved: ./DARKNET/output/cropped/person_15.jpg with confidence 0.99\n",
      "The model is 96.61% confident the image does not belong to the class 'inappropriate'.\n",
      "Person 15 classified as: appropriate\n",
      "Skipping empty cropped person 13\n",
      "Cropped Person 40 Saved: ./DARKNET/output/cropped/person_40.jpg with confidence 0.99\n",
      "The model is 99.99% confident the image does not belong to the class 'inappropriate'.\n",
      "Person 40 classified as: appropriate\n",
      "Cropped Person 32 Saved: ./DARKNET/output/cropped/person_32.jpg with confidence 0.98\n",
      "The model is 76.33% confident the image does not belong to the class 'inappropriate'.\n",
      "Person 32 classified as: appropriate\n",
      "Cropped Person 5 Saved: ./DARKNET/output/cropped/person_5.jpg with confidence 0.97\n",
      "The model is 97.86% confident the image does not belong to the class 'inappropriate'.\n",
      "Person 5 classified as: appropriate\n",
      "Cropped Person 23 Saved: ./DARKNET/output/cropped/person_23.jpg with confidence 0.95\n",
      "The model is 97.97% confident the image does not belong to the class 'inappropriate'.\n",
      "Person 23 classified as: appropriate\n",
      "Cropped Person 20 Saved: ./DARKNET/output/cropped/person_20.jpg with confidence 0.85\n",
      "The model is 99.81% confident the image does not belong to the class 'inappropriate'.\n",
      "Person 20 classified as: appropriate\n",
      "Cropped Person 30 Saved: ./DARKNET/output/cropped/person_30.jpg with confidence 0.45\n",
      "The model is 99.94% confident the image does not belong to the class 'inappropriate'.\n",
      "Person 30 classified as: appropriate\n",
      "Cropped Person 12 Saved: ./DARKNET/output/cropped/person_12.jpg with confidence 0.23\n",
      "The model is 100.00% confident the image does not belong to the class 'inappropriate'.\n",
      "Person 12 classified as: appropriate\n",
      "Image with bounding boxes saved to ./DARKNET/output/whole/output1.jpg\n",
      "The entire image is classified as: appropriate\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "class_names = ['appropriate', 'inappropriate']\n",
    "\n",
    "# Suppress specific UserWarning for InceptionV3\n",
    "warnings.filterwarnings(\"ignore\", message=\"Scripted Inception3 always returns Inception3 Tuple\")\n",
    "\n",
    "# Load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# fussionmodel = torch.load('Saved models/resnet_inception_fullmodel_cropped.pth')\n",
    "# fussionmodel.to(device)\n",
    "# fussionmodel.eval() \n",
    "\n",
    "fuse = torch.jit.load(\"Saved models/resnet_inception_fullmodel_cropped.pt\")\n",
    "fuse.to(device)\n",
    "fuse.eval()\n",
    "\n",
    "# # Perform the forward pass during prediction\n",
    "# with torch.no_grad():  # Disable gradient tracking during inference for efficiency\n",
    "#     x_resnet = x_resnet.to(device)  # Make sure the input is on the correct device (CPU/GPU)\n",
    "#     x_inception = x_inception.to(device)\n",
    "#     output = model(x_resnet, x_inception)\n",
    "# # Image preprocessing functions equivalent to TensorFlow preprocessing\n",
    "\n",
    "resnet_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ResNet normalization\n",
    "])\n",
    "\n",
    "inception_transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Inception normalization\n",
    "])\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img_resnet = Image.open(img_path).convert('RGB')\n",
    "    img_inception = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    # Apply transformations for ResNet and Inception\n",
    "    img_tensor_resnet = resnet_transform(img_resnet).unsqueeze(0)  # Add batch dimension\n",
    "    img_tensor_inception = inception_transform(img_inception).unsqueeze(0)\n",
    "    \n",
    "    return img_tensor_resnet, img_tensor_inception\n",
    "\n",
    "def predict_image(img_path):\n",
    "    img_tensor_resnet, img_tensor_inception = preprocess_image(img_path)\n",
    "    img_tensor_resnet, img_tensor_inception = img_tensor_resnet.to(device), img_tensor_inception.to(device)\n",
    "\n",
    "    # Forward pass to get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = fuse(img_tensor_resnet, img_tensor_inception)\n",
    "        predicted_value = outputs.item()  # Extract scalar prediction\n",
    "\n",
    "    # Return class name based on threshold\n",
    "    predicted_class = class_names[int(predicted_value > 0.5)]\n",
    "    confidence = outputs[0].item()\n",
    "    interpretation = f\"The model is {(1 - confidence) * 100:.2f}% confident the image does not belong to the class '{class_names[1]}'.\"\n",
    "    # Return class name based on threshold (0.5)\n",
    "    print(interpretation)\n",
    "    return predicted_class\n",
    "\n",
    "\n",
    "def get_yolo_preds():\n",
    "    \"\"\"\n",
    "    Run YOLO predictions, save cropped persons, classify them, and label the entire image.\n",
    "    \"\"\"\n",
    "    labels_path = \"./DARKNET/coco.txt\"\n",
    "    yolo_cfg = \"./DARKNET/model_data/yolov3.cfg\"\n",
    "    yolo_weights = \"./DARKNET/model_data/yolov3.weights\"\n",
    "    input_img_path = \"./DARKNET/test1.jpg\"\n",
    "    output_img_path = \"./DARKNET/output/whole/output1.jpg\"\n",
    "    confidence_threshold = 0.4\n",
    "    overlapping_threshold = 0.5\n",
    "\n",
    "    # Load COCO labels\n",
    "    try:\n",
    "        with open(labels_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            labels = f.read().strip().split(\"\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {labels_path} not found. Please check the file path.\")\n",
    "        return\n",
    "\n",
    "    # Initialize YOLO model\n",
    "    net = cv2.dnn.readNetFromDarknet(yolo_cfg, yolo_weights)\n",
    "    if cv2.cuda.getCudaEnabledDeviceCount() > 0:\n",
    "        net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "        net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "    # Read input image\n",
    "    image = cv2.imread(input_img_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image {input_img_path}\")\n",
    "        return\n",
    "    (H, W) = image.shape[:2]\n",
    "\n",
    "    # Create a blob and perform inference\n",
    "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_outputs = net.forward([net.getLayerNames()[i - 1] for i in net.getUnconnectedOutLayers().flatten()])\n",
    "\n",
    "    # Initialize lists for detections\n",
    "    boxes, confidences, classIDs = [], [], []\n",
    "\n",
    "    for output in layer_outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            # Only keep detections for \"person\" (class ID 0 in COCO)\n",
    "            if confidence > confidence_threshold and classID == 0:\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                classIDs.append(classID)\n",
    "\n",
    "    # Non-maxima suppression\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, overlapping_threshold)\n",
    "\n",
    "    image_with_boxes = image.copy()\n",
    "    inappropriate_detected = False\n",
    "\n",
    "    # Collect cropped persons and classify them\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            (x, y, w, h) = boxes[i]\n",
    "\n",
    "            # Ensure the cropped region is within bounds\n",
    "            y_end = min(y + h, image.shape[0])\n",
    "            x_end = min(x + w, image.shape[1])\n",
    "\n",
    "            cropped_person = image[y:y_end, x:x_end]\n",
    "\n",
    "            # Check if the cropped person is valid (non-empty)\n",
    "            if cropped_person.size == 0:\n",
    "                print(f\"Skipping empty cropped person {i + 1}\")\n",
    "                continue\n",
    "\n",
    "            cropped_person_path = f\"./DARKNET/output/cropped/person_{i + 1}.jpg\"\n",
    "\n",
    "            # Save the cropped person if valid\n",
    "            if cv2.imwrite(cropped_person_path, cropped_person):\n",
    "                print(f\"Cropped Person {i + 1} Saved: {cropped_person_path} with confidence {confidences[i]:.2f}\")\n",
    "            else:\n",
    "                print(f\"Error saving cropped person {i + 1}\")\n",
    "\n",
    "            # Classify the cropped person\n",
    "            label = predict_image(cropped_person_path)\n",
    "            print(f\"Person {i + 1} classified as: {label}\")\n",
    "\n",
    "            # Check if the person is inappropriate\n",
    "            if label == 'inappropriate':\n",
    "                inappropriate_detected = True\n",
    "\n",
    "            # Draw bounding box on the image with classification\n",
    "            color = (0, 0, 255) if label == 'inappropriate' else (0, 255, 0)\n",
    "            text = f\"{label}\"\n",
    "            cv2.rectangle(image_with_boxes, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(image_with_boxes, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "            cv2.putText(image_with_boxes, f\"{confidences[i]:.2f}\", (x + 100, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "    # Save the output image with bounding boxes\n",
    "    if cv2.imwrite(output_img_path, image_with_boxes):\n",
    "        print(f\"Image with bounding boxes saved to {output_img_path}\")\n",
    "\n",
    "    # Final classification of the image\n",
    "    if inappropriate_detected:\n",
    "        print(\"The entire image is classified as: inappropriate\")\n",
    "    else:\n",
    "        print(\"The entire image is classified as: appropriate\")\n",
    "\n",
    "\n",
    "# Run the YOLO predictions\n",
    "if __name__ == '__main__':\n",
    "    get_yolo_preds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO + Resnet_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_20628\\1835371343.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  load_ex = torch.load('Saved models/svm/resnetextractor.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extractor loaded.\n",
      "SVM classifier loaded.\n",
      "Cropped Person 8 Saved: ./DARKNET/output/cropped/person_8.jpg\n",
      "Person 8 classified as: Appropriate\n",
      "Cropped Person 10 Saved: ./DARKNET/output/cropped/person_10.jpg\n",
      "Person 10 classified as: Appropriate\n",
      "Cropped Person 11 Saved: ./DARKNET/output/cropped/person_11.jpg\n",
      "Person 11 classified as: Appropriate\n",
      "Cropped Person 2 Saved: ./DARKNET/output/cropped/person_2.jpg\n",
      "Person 2 classified as: Appropriate\n",
      "Cropped Person 6 Saved: ./DARKNET/output/cropped/person_6.jpg\n",
      "Person 6 classified as: Appropriate\n",
      "Image with bounding boxes saved to ./DARKNET/output/whole/output.jpg\n",
      "The entire image is classified as: appropriate\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class_names = ['Appropriate', 'Inappropriate']\n",
    "\n",
    "# Load the ResNet feature extractor\n",
    "load_ex = torch.load('Saved models/svm/resnetextractor.pth')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_ex.to(device)\n",
    "print(\"Feature extractor loaded.\")\n",
    "\n",
    "# Load the SVM model\n",
    "svm_model_path = os.path.join('Saved models/svm', 'torch_svm.pkl')\n",
    "load_svm = joblib.load(svm_model_path)\n",
    "print(\"SVM classifier loaded.\")\n",
    "resnet_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ResNet normalization\n",
    "])\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img_resnet = Image.open(img_path).convert('RGB')\n",
    "    img_tensor_resnet = resnet_transform(img_resnet).unsqueeze(0).to(device)\n",
    "\n",
    " \n",
    "    \n",
    "    return img_tensor_resnet\n",
    "\n",
    "def predict_image(img_path):\n",
    "    \"\"\"\n",
    "    Predict the class of an image using the saved ResNet-based feature extractor and SVM classifier.\n",
    "\n",
    "    Args:\n",
    "        img_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        str: Predicted class name.\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = resnet_transform(img).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    load_ex.eval()  # Ensure the model is in evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        feature_map = load_ex(img_tensor)  \n",
    "        flattened_features = feature_map.squeeze().cpu().numpy().reshape(1, -1)  \n",
    "    prediction = load_svm.predict(flattened_features)  # Predict using the trained SVM classifier\n",
    "    return class_names[prediction[0]]\n",
    "\n",
    "\n",
    "\n",
    "def get_yolo_preds():\n",
    "    \"\"\"\n",
    "    Run YOLO predictions, save cropped persons, classify them, and label the entire image.\n",
    "    \"\"\"\n",
    "    labels_path = \"./DARKNET/coco.txt\"\n",
    "    yolo_cfg = \"./DARKNET/model_data/yolov3.cfg\"\n",
    "    yolo_weights = \"./DARKNET/model_data/yolov3.weights\"\n",
    "    input_img_path = \"./DARKNET/test.jpg\"\n",
    "    output_img_path = \"./DARKNET/output/whole/output.jpg\"\n",
    "    confidence_threshold = 0.5\n",
    "    overlapping_threshold = 0.5\n",
    "\n",
    "    # Load COCO labels\n",
    "    try:\n",
    "        with open(labels_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            labels = f.read().strip().split(\"\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {labels_path} not found. Please check the file path.\")\n",
    "        return\n",
    "\n",
    "    # Initialize YOLO model\n",
    "    net = cv2.dnn.readNetFromDarknet(yolo_cfg, yolo_weights)\n",
    "    if cv2.cuda.getCudaEnabledDeviceCount() > 0:\n",
    "        net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "        net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "    # Read input image\n",
    "    image = cv2.imread(input_img_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image {input_img_path}\")\n",
    "        return\n",
    "    (H, W) = image.shape[:2]\n",
    "\n",
    "    # Create a blob and perform inference\n",
    "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_outputs = net.forward([net.getLayerNames()[i - 1] for i in net.getUnconnectedOutLayers().flatten()])\n",
    "\n",
    "    # Initialize lists for detections\n",
    "    boxes, confidences, classIDs = [], [], []\n",
    "\n",
    "    for output in layer_outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            # Only keep detections for \"person\" (class ID 0 in COCO)\n",
    "            if confidence > confidence_threshold and classID == 0:\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                classIDs.append(classID)\n",
    "\n",
    "    # Non-maxima suppression\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, overlapping_threshold)\n",
    "\n",
    "    image_with_boxes = image.copy()\n",
    "    inappropriate_detected = False\n",
    "\n",
    "    # Collect cropped persons and classify them\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            (x, y, w, h) = boxes[i]\n",
    "\n",
    "            # Ensure the cropped region is within bounds\n",
    "            y_end = min(y + h, image.shape[0])\n",
    "            x_end = min(x + w, image.shape[1])\n",
    "\n",
    "            cropped_person = image[y:y_end, x:x_end]\n",
    "\n",
    "            # Check if the cropped person is valid (non-empty)\n",
    "            if cropped_person.size == 0:\n",
    "                print(f\"Skipping empty cropped person {i + 1}\")\n",
    "                continue\n",
    "\n",
    "            cropped_person_path = f\"./DARKNET/output/cropped/person_{i + 1}.jpg\"\n",
    "\n",
    "            # Save the cropped person if valid\n",
    "            if cv2.imwrite(cropped_person_path, cropped_person):\n",
    "                print(f\"Cropped Person {i + 1} Saved: {cropped_person_path} with confidence {confidences[i]:.2f}\")\n",
    "            else:\n",
    "                print(f\"Error saving cropped person {i + 1}\")\n",
    "\n",
    "            # Classify the cropped person\n",
    "            label = predict_image(cropped_person_path)\n",
    "            print(f\"Person {i + 1} classified as: {label}\")\n",
    "\n",
    "            # Check if the person is inappropriate\n",
    "            if label == 'inappropriate':\n",
    "                inappropriate_detected = True\n",
    "\n",
    "            # Draw bounding box on the image with classification\n",
    "            color = (0, 0, 255) if label == 'inappropriate' else (0, 255, 0)\n",
    "            cv2.rectangle(image_with_boxes, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(image_with_boxes, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # Save the output image with bounding boxes\n",
    "    if cv2.imwrite(output_img_path, image_with_boxes):\n",
    "        print(f\"Image with bounding boxes saved to {output_img_path}\")\n",
    "\n",
    "    # Final classification of the image\n",
    "    if inappropriate_detected:\n",
    "        print(\"The entire image is classified as: inappropriate\")\n",
    "    else:\n",
    "        print(\"The entire image is classified as: appropriate\")\n",
    "\n",
    "\n",
    "# Run the YOLO predictions\n",
    "if __name__ == '__main__':\n",
    "    get_yolo_preds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST PROPOSED WITH GPU YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\DARKNET\\test.jpg: 384x640 5 persons, 1 tv, 812.7ms\n",
      "Speed: 2.0ms preprocess, 812.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped Person 1 Saved: ./DARKNET/output/cropped/person_1.jpg with confidence 0.93\n",
      "The model is 92.57% confident the image does not belong to the class 'inappropriate'.\n",
      "Person 1 classified as: appropriate\n",
      "Cropped Person 2 Saved: ./DARKNET/output/cropped/person_2.jpg with confidence 0.92\n",
      "The model is 21.39% confident the image does not belong to the class 'inappropriate'.\n",
      "Person 2 classified as: inappropriate\n",
      "Cropped Person 3 Saved: ./DARKNET/output/cropped/person_3.jpg with confidence 0.92\n",
      "The model is 41.25% confident the image does not belong to the class 'inappropriate'.\n",
      "Person 3 classified as: inappropriate\n",
      "Cropped Person 4 Saved: ./DARKNET/output/cropped/person_4.jpg with confidence 0.91\n",
      "The model is 0.35% confident the image does not belong to the class 'inappropriate'.\n",
      "Person 4 classified as: inappropriate\n",
      "Cropped Person 5 Saved: ./DARKNET/output/cropped/person_5.jpg with confidence 0.87\n",
      "The model is 2.20% confident the image does not belong to the class 'inappropriate'.\n",
      "Person 5 classified as: inappropriate\n",
      "Image with bounding boxes saved to ./DARKNET/output/whole/output.jpg\n",
      "The entire image is classified as: inappropriate\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "class_names = ['appropriate', 'inappropriate']\n",
    "\n",
    "# Suppress specific UserWarning for InceptionV3\n",
    "warnings.filterwarnings(\"ignore\", message=\"Scripted Inception3 always returns Inception3 Tuple\")\n",
    "\n",
    "# Load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# fussionmodel = torch.load('Saved models/resnet_inception_fullmodel_cropped.pth')\n",
    "# fussionmodel.to(device)\n",
    "# fussionmodel.eval() \n",
    "\n",
    "fuse = torch.jit.load(\"Saved models/resnet_inception_fullmodel_cropped.pt\")\n",
    "fuse.to(device)\n",
    "fuse.eval()\n",
    "\n",
    "# # Perform the forward pass during prediction\n",
    "# with torch.no_grad():  # Disable gradient tracking during inference for efficiency\n",
    "#     x_resnet = x_resnet.to(device)  # Make sure the input is on the correct device (CPU/GPU)\n",
    "#     x_inception = x_inception.to(device)\n",
    "#     output = model(x_resnet, x_inception)\n",
    "# # Image preprocessing functions equivalent to TensorFlow preprocessing\n",
    "\n",
    "resnet_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ResNet normalization\n",
    "])\n",
    "\n",
    "inception_transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Inception normalization\n",
    "])\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img_resnet = Image.open(img_path).convert('RGB')\n",
    "    img_inception = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    # Apply transformations for ResNet and Inception\n",
    "    img_tensor_resnet = resnet_transform(img_resnet).unsqueeze(0)  # Add batch dimension\n",
    "    img_tensor_inception = inception_transform(img_inception).unsqueeze(0)\n",
    "    \n",
    "    return img_tensor_resnet, img_tensor_inception\n",
    "\n",
    "def predict_image(img_path):\n",
    "    img_tensor_resnet, img_tensor_inception = preprocess_image(img_path)\n",
    "    img_tensor_resnet, img_tensor_inception = img_tensor_resnet.to(device), img_tensor_inception.to(device)\n",
    "\n",
    "    # Forward pass to get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = fuse(img_tensor_resnet, img_tensor_inception)\n",
    "        predicted_value = outputs.item()  # Extract scalar prediction\n",
    "\n",
    "    # Return class name based on threshold\n",
    "    predicted_class = class_names[int(predicted_value > 0.5)]\n",
    "    confidence = outputs[0].item()\n",
    "    interpretation = f\"The model is {(1 - confidence) * 100:.2f}% confident the image does not belong to the class '{class_names[1]}'.\"\n",
    "    # Return class name based on threshold (0.5)\n",
    "    print(interpretation)\n",
    "    return predicted_class\n",
    "\n",
    "\n",
    "def get_yolo_preds():\n",
    "    \"\"\"\n",
    "    Run YOLOv3u predictions, save cropped persons, classify them, and label the entire image.\n",
    "    \"\"\"\n",
    "    model_path = \"yolov3u.pt\"  # Ensure this path is correct\n",
    "    input_img_path = \"./DARKNET/test.jpg\"\n",
    "    output_img_path = \"./DARKNET/output/whole/output.jpg\"\n",
    "    cropped_folder = \"./DARKNET/output/cropped/\"\n",
    "    confidence_threshold = 0.45\n",
    "\n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(cropped_folder, exist_ok=True)\n",
    "\n",
    "    # Load YOLOv3u model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Read input image\n",
    "    image = cv2.imread(input_img_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image {input_img_path}\")\n",
    "        return\n",
    "    H, W = image.shape[:2]\n",
    "    \n",
    "    # Run inference\n",
    "    results = model.predict(input_img_path, device=\"cuda\", conf=confidence_threshold)\n",
    "    \n",
    "    # Initialize lists for detections\n",
    "    boxes, confidences = [], []\n",
    "    image_with_boxes = image.copy()\n",
    "    inappropriate_detected = False\n",
    "    \n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            class_id = int(box.cls[0].item())  # Get class ID\n",
    "            confidence = box.conf[0].item()  # Confidence score\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "            \n",
    "            if confidence > confidence_threshold and class_id == 0:  # Class ID 0 is \"person\"\n",
    "                boxes.append([x1, y1, x2 - x1, y2 - y1])\n",
    "                confidences.append(confidence)\n",
    "    \n",
    "    # Process detected persons\n",
    "    if boxes:\n",
    "        for i, (x, y, w, h) in enumerate(boxes):\n",
    "            cropped_person = image[y:y + h, x:x + w]\n",
    "            \n",
    "            if cropped_person.size == 0:\n",
    "                print(f\"Skipping empty cropped person {i + 1}\")\n",
    "                continue\n",
    "            \n",
    "            cropped_person_path = os.path.join(cropped_folder, f\"person_{i + 1}.jpg\")\n",
    "            # Save the cropped person if valid\n",
    "            if cv2.imwrite(cropped_person_path, cropped_person):\n",
    "                print(f\"Cropped Person {i + 1} Saved: {cropped_person_path} with confidence {confidences[i]:.2f}\")\n",
    "            else:\n",
    "                print(f\"Error saving cropped person {i + 1}\")\n",
    "            \n",
    "            # Classify the cropped person\n",
    "            label = predict_image(cropped_person_path)\n",
    "            print(f\"Person {i + 1} classified as: {label}\")\n",
    "            \n",
    "            # Check if the person is inappropriate\n",
    "            if label == 'inappropriate':\n",
    "                inappropriate_detected = True\n",
    "            \n",
    "            # Draw bounding box on the image\n",
    "            color = (0, 0, 255) if label == 'inappropriate' else (0, 255, 0)\n",
    "            text = f\"{label}\"\n",
    "            cv2.rectangle(image_with_boxes, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(image_with_boxes, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "            cv2.putText(image_with_boxes, f\"{confidences[i]:.2f}\", (x + 100, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "    # Save the output image with bounding boxes\n",
    "    if cv2.imwrite(output_img_path, image_with_boxes):\n",
    "        print(f\"Image with bounding boxes saved to {output_img_path}\")\n",
    "\n",
    "    # Final classification of the image\n",
    "    if inappropriate_detected:\n",
    "        print(\"The entire image is classified as: inappropriate\")\n",
    "    else:\n",
    "        print(\"The entire image is classified as: appropriate\")\n",
    "\n",
    "\n",
    "# Run the YOLO predictions\n",
    "if __name__ == '__main__':\n",
    "    get_yolo_preds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST BASELINE WITH GPU YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extractor loaded.\n",
      "SVM classifier loaded.\n",
      "\n",
      "image 1/1 d:\\THESIS PROJECT FOLDER\\backend\\DARKNET\\test.jpg: 384x640 5 persons, 1 tv, 851.3ms\n",
      "Speed: 3.0ms preprocess, 851.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped Person 1 Saved: ./DARKNET/output/cropped/person_1.jpg with confidence 0.93\n",
      "Person 1 classified as: Appropriate\n",
      "Cropped Person 2 Saved: ./DARKNET/output/cropped/person_2.jpg with confidence 0.92\n",
      "Person 2 classified as: Appropriate\n",
      "Cropped Person 3 Saved: ./DARKNET/output/cropped/person_3.jpg with confidence 0.92\n",
      "Person 3 classified as: Appropriate\n",
      "Cropped Person 4 Saved: ./DARKNET/output/cropped/person_4.jpg with confidence 0.91\n",
      "Person 4 classified as: Appropriate\n",
      "Cropped Person 5 Saved: ./DARKNET/output/cropped/person_5.jpg with confidence 0.87\n",
      "Person 5 classified as: Appropriate\n",
      "Image with bounding boxes saved to ./DARKNET/output/whole/output.jpg\n",
      "The entire image is classified as: appropriate\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class_names = ['Appropriate', 'Inappropriate']\n",
    "\n",
    "# Load the ResNet feature extractor\n",
    "load_ex = torch.load('Saved models/svm/resnetextractor.pth')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_ex.to(device)\n",
    "print(\"Feature extractor loaded.\")\n",
    "\n",
    "# Load the SVM model\n",
    "svm_model_path = os.path.join('Saved models/svm', 'torch_svm.pkl')\n",
    "load_svm = joblib.load(svm_model_path)\n",
    "print(\"SVM classifier loaded.\")\n",
    "resnet_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ResNet normalization\n",
    "])\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img_resnet = Image.open(img_path).convert('RGB')\n",
    "    img_tensor_resnet = resnet_transform(img_resnet).unsqueeze(0).to(device)\n",
    "\n",
    " \n",
    "    \n",
    "    return img_tensor_resnet\n",
    "\n",
    "def predict_image(img_path):\n",
    "    \"\"\"\n",
    "    Predict the class of an image using the saved ResNet-based feature extractor and SVM classifier.\n",
    "\n",
    "    Args:\n",
    "        img_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        str: Predicted class name.\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = resnet_transform(img).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    load_ex.eval()  # Ensure the model is in evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        feature_map = load_ex(img_tensor)  \n",
    "        flattened_features = feature_map.squeeze().cpu().numpy().reshape(1, -1)  \n",
    "    prediction = load_svm.predict(flattened_features)  # Predict using the trained SVM classifier\n",
    "    return class_names[prediction[0]]\n",
    "\n",
    "\n",
    "\n",
    "def get_yolo_preds():\n",
    "    \"\"\"\n",
    "    Run YOLOv3u predictions, save cropped persons, classify them, and label the entire image.\n",
    "    \"\"\"\n",
    "    model_path = \"yolov3u.pt\"  # Ensure this path is correct\n",
    "    input_img_path = \"./DARKNET/test.jpg\"\n",
    "    output_img_path = \"./DARKNET/output/whole/output.jpg\"\n",
    "    cropped_folder = \"./DARKNET/output/cropped/\"\n",
    "    confidence_threshold = 0.45\n",
    "\n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(cropped_folder, exist_ok=True)\n",
    "\n",
    "    # Load YOLOv3u model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Read input image\n",
    "    image = cv2.imread(input_img_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image {input_img_path}\")\n",
    "        return\n",
    "    H, W = image.shape[:2]\n",
    "    \n",
    "    # Run inference\n",
    "    results = model.predict(input_img_path, device=\"cuda\", conf=confidence_threshold)\n",
    "    \n",
    "    # Initialize lists for detections\n",
    "    boxes, confidences = [], []\n",
    "    image_with_boxes = image.copy()\n",
    "    inappropriate_detected = False\n",
    "    \n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            class_id = int(box.cls[0].item())  # Get class ID\n",
    "            confidence = box.conf[0].item()  # Confidence score\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "            \n",
    "            if confidence > confidence_threshold and class_id == 0:  # Class ID 0 is \"person\"\n",
    "                boxes.append([x1, y1, x2 - x1, y2 - y1])\n",
    "                confidences.append(confidence)\n",
    "    \n",
    "    # Process detected persons\n",
    "    if boxes:\n",
    "        for i, (x, y, w, h) in enumerate(boxes):\n",
    "            cropped_person = image[y:y + h, x:x + w]\n",
    "            \n",
    "            if cropped_person.size == 0:\n",
    "                print(f\"Skipping empty cropped person {i + 1}\")\n",
    "                continue\n",
    "            \n",
    "            cropped_person_path = os.path.join(cropped_folder, f\"person_{i + 1}.jpg\")\n",
    "            # Save the cropped person if valid\n",
    "            if cv2.imwrite(cropped_person_path, cropped_person):\n",
    "                print(f\"Cropped Person {i + 1} Saved: {cropped_person_path} with confidence {confidences[i]:.2f}\")\n",
    "            else:\n",
    "                print(f\"Error saving cropped person {i + 1}\")\n",
    "            \n",
    "            # Classify the cropped person\n",
    "            label = predict_image(cropped_person_path)\n",
    "            print(f\"Person {i + 1} classified as: {label}\")\n",
    "            \n",
    "            # Check if the person is inappropriate\n",
    "            if label == 'inappropriate':\n",
    "                inappropriate_detected = True\n",
    "            \n",
    "            # Draw bounding box on the image\n",
    "            color = (0, 0, 255) if label == 'inappropriate' else (0, 255, 0)\n",
    "            text = f\"{label}\"\n",
    "            cv2.rectangle(image_with_boxes, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(image_with_boxes, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "            cv2.putText(image_with_boxes, f\"{confidences[i]:.2f}\", (x + 100, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "    # Save the output image with bounding boxes\n",
    "    if cv2.imwrite(output_img_path, image_with_boxes):\n",
    "        print(f\"Image with bounding boxes saved to {output_img_path}\")\n",
    "\n",
    "    # Final classification of the image\n",
    "    if inappropriate_detected:\n",
    "        print(\"The entire image is classified as: inappropriate\")\n",
    "    else:\n",
    "        print(\"The entire image is classified as: appropriate\")\n",
    "\n",
    "\n",
    "# Run the YOLO predictions\n",
    "if __name__ == '__main__':\n",
    "    get_yolo_preds()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
